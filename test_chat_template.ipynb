{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "import time \n",
    "import gc\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f69cfa-9e42-4826-9d07-a4899986e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 0.1%\n",
      "Total RAM: 503.68 GB\n",
      "Available RAM: 475.77 GB\n",
      "Used RAM: 14.14 GB\n",
      "RAM Usage Percentage: 5.5%\n",
      "['0', '1', '2', '3']\n",
      "\n",
      "-> gpu 0\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 1\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 2\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 3\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "cpu_percent = psutil.cpu_percent(interval=1)\n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# RAM usage\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {virtual_memory.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available RAM: {virtual_memory.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used RAM: {virtual_memory.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"RAM Usage Percentage: {virtual_memory.percent}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "    for gpu_index in  GPUS:\n",
    "        print(f\"\\n-> gpu {gpu_index}\")\n",
    "        gpu_index = int(gpu_index)\n",
    "        # gpu_index = 0  # Change this if you have multiple GPUs\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(gpu_index)\n",
    "        allocated_memory = torch.cuda.memory_allocated(gpu_index)\n",
    "        free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "        print(f\"Total GPU Memory: {total_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Allocated GPU Memory: {allocated_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Available GPU Memory: {free_memory / 1024 ** 3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ac76ea-afff-419b-bce5-d00d3171f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'Below is the graph of $y = a \\\\sin (bx + c) + d$ for some positive constants $a,$ $b,$ $c,$ and $d.$  Find the smallest possible value of $c.$\\n\\n[asy]import TrigMacros;\\n\\nsize(400);\\n\\nreal f(real x)\\n{\\n\\treturn 2*sin(3*x + pi) + 1;\\n}\\n\\ndraw(graph(f,-3*pi,3*pi,n=700,join=operator ..),red);\\ntrig_axes(-3*pi,3*pi,-4,4,pi/2,1);\\nlayer();\\nrm_trig_labels(-5,5, 2);\\n\\nlabel(\"$1$\", (0,1), E);\\nlabel(\"$2$\", (0,2), E);\\nlabel(\"$3$\", (0,3), E);\\nlabel(\"$-1$\", (0,-1), E);\\nlabel(\"$-2$\", (0,-2), E);\\nlabel(\"$-3$\", (0,-3), E);\\n[/asy]' \n",
    "prompt =  'If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.'\n",
    "max_new_tokens = 1024 \n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e455f81a-c8ec-4c6e-83ca-1ba34f6bd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference(model, tokenizer, prompt, max_new_tokens, num_runs, use_vllm=False):\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        if use_vllm:\n",
    "            # vLLM generates text directly from the prompt\n",
    "            sampling_params = SamplingParams(temperature=0.8, max_tokens=max_new_tokens, top_p=1.0, n=1, seed=123)\n",
    "            output = model.generate(prompt, sampling_params)\n",
    "            generated_text = output[0].outputs[0].text\n",
    "            generated_ids = output[0].outputs[0].token_ids\n",
    "            num_tokens = len(generated_ids)\n",
    "        else:\n",
    "            # Transformers requires tokenization\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            num_tokens = len(tokenizer.encode(generated_text))\n",
    "            \n",
    "        end_time = time.time()\n",
    "        total_time += (end_time - start_time)\n",
    "        # total_tokens += len(tokenizer.encode(generated_text)) if not use_vllm else max_new_tokens\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "    latency = total_time / num_runs\n",
    "    throughput = total_tokens / total_time\n",
    "    avg_tokens = total_tokens / num_runs\n",
    "    return latency, throughput, avg_tokens, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b58c7e34-cc55-4937-bdce-7a2e13ee7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0079345703125\n"
     ]
    }
   ],
   "source": [
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39e0fc69-6fe1-4acc-95c4-f6b35ca11f79",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(llm_tokenizer_path)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model_regular = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     llm_tokenizer_path, device_map='cuda:0')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model_regular \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# quantized_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     load_in_4bit=True, bnb_4bit_use_double_quant=True, \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\u001b[39;00m\n\u001b[1;32m     21\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect();torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache();\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:487\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_path)\n",
    "\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_tokenizer_path, device_map='cuda:0')\n",
    "\n",
    "model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_path, device_map='cuda:0')\n",
    "\n",
    "# quantized_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, \n",
    "#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_tokenizer_path, quantization_config=quantized_config, device_map='cuda:0')\n",
    "\n",
    "# model_id = \"QuantFactory/Llama-3.2-1B-Instruct-GGUF\"\n",
    "# filename = \"Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id, gguf_file=filename, device_map='cuda:0')\n",
    "\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "935cd525-3b63-46b4-96a4-cf750b42d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers (Regular) - Latency: 6.6855s, Throughput: 79.87 tokens/s\n"
     ]
    }
   ],
   "source": [
    "latency_regular, throughput_regular, avg_tokens, generated_text = measure_inference(model_regular, tokenizer, prompt, max_new_tokens, num_runs)\n",
    "print(f\"Transformers (Regular) - Latency: {latency_regular:.4f}s, Throughput: {throughput_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355aaf7-440b-43cc-a0f8-dcbef6928a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c9b1b4-26c1-445b-937a-3d438a96a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0079345703125\n"
     ]
    }
   ],
   "source": [
    "del(tokenizer)\n",
    "del(model_regular)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 11:43:07 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-19 11:43:07 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-19 11:43:15 config.py:549] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-19 11:43:15 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-19 11:43:16 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-19 11:43:16 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-19 11:43:17 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b03d70eff94236915d1f27b442429f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 11:43:19 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 03-19 11:43:19 worker.py:267] Memory profiling takes 0.45 seconds\n",
      "INFO 03-19 11:43:19 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.20) = 6.35GiB\n",
      "INFO 03-19 11:43:19 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 2.75GiB.\n",
      "INFO 03-19 11:43:19 executor_base.py:111] # cuda blocks: 5631, # CPU blocks: 8192\n",
      "INFO 03-19 11:43:19 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 18.02x\n",
      "INFO 03-19 11:43:21 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 11:43:34 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.13 GiB\n",
      "INFO 03-19 11:43:34 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 15.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.084694862365723\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "llm_regular = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b56f64b-3c90-46e0-b2e4-0c15727be573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vllm.engine.llm_engine.LLMEngine object at 0x7f01ceb13250>\n"
     ]
    }
   ],
   "source": [
    "print(llm_regular.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12833482-d304-4da8-9caf-42da552b3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it, est. speed input: 16.58 toks/s, output: 212.79 toks/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LLM.encode() is only supported for pooling models. Your model supports the 'pooling' runner, but is currently initialized for the 'generate' runner. Please initialize vLLM using `--task embed`, `--task classify`, `--task score` etc.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latency_vllm_regular, throughput_vllm_regular, avg_tokens, generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_regular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_vllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM (Regular) - Latency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatency_vllm_regular\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Throughput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput_vllm_regular\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens/s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mmeasure_inference\u001b[0;34m(model, tokenizer, prompt, max_new_tokens, num_runs, use_vllm)\u001b[0m\n\u001b[1;32m     11\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtoken_ids\n\u001b[1;32m     13\u001b[0m pooling_params \u001b[38;5;241m=\u001b[39m PoolingParams()\n\u001b[0;32m---> 14\u001b[0m generated_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m generated_last_embedding \u001b[38;5;241m=\u001b[39m generated_encoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_last_embedding)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/utils.py:1057\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1052\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1053\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1054\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m         )\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:896\u001b[0m, in \u001b[0;36mLLM.encode\u001b[0;34m(self, prompts, pooling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m supported_runner_types:\n\u001b[1;32m    890\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour model supports the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m runner, but is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    892\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently initialized for the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunner_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m runner. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease initialize vLLM using `--task embed`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--task classify`, `--task score` etc.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(messages))\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     parsed_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_v1_inputs(\n\u001b[1;32m    900\u001b[0m         prompts\u001b[38;5;241m=\u001b[39mcast(Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]], prompts),\n\u001b[1;32m    901\u001b[0m         prompt_token_ids\u001b[38;5;241m=\u001b[39mprompt_token_ids,\n\u001b[1;32m    902\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: LLM.encode() is only supported for pooling models. Your model supports the 'pooling' runner, but is currently initialized for the 'generate' runner. Please initialize vLLM using `--task embed`, `--task classify`, `--task score` etc."
     ]
    }
   ],
   "source": [
    "latency_vllm_regular, throughput_vllm_regular, avg_tokens, generated_text = measure_inference(llm_regular, None, prompt, max_new_tokens, num_runs, use_vllm=True)\n",
    "print(f\"vLLM (Regular) - Latency: {latency_vllm_regular:.4f}s, Throughput: {throughput_vllm_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "411741be-9225-4d44-a1bc-7028d66f219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0\n",
      " \n",
      "\n",
      "## Step 1: Plug in the value of x in the function f(x) to find f(-2)\n",
      "To find the value of $f(-2)$, we plug in $x = -2$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-2) = \\frac{3(-2)-2}{-2-2} = \\frac{-6-2}{-4} = \\frac{-8}{-4} = 2$.\n",
      "\n",
      "## Step 2: Plug in the value of x in the function f(x) to find f(-1)\n",
      "To find the value of $f(-1)$, we plug in $x = -1$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-1) = \\frac{3(-1)-2}{-1-2} = \\frac{-3-2}{-3} = \\frac{-5}{-3} = \\frac{5}{3}$.\n",
      "\n",
      "## Step 3: Plug in the value of x in the function f(x) to find f(0)\n",
      "To find the value of $f(0)$, we plug in $x = 0$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(0) = \\frac{3(0)-2}{0-2} = \\frac{-2}{-2} = 1$.\n",
      "\n",
      "## Step 4: Add up the values of f(-2), f(-1), and f(0)\n",
      "Now that we have found the values of $f(-2)$, $f(-1)$, and $f(0)$, which are 2, $\\frac{5}{3}$, and 1 respectively, we can add them up. $f(-2) + f(-1) + f(0) = 2 + \\frac{5}{3} + 1$.\n",
      "\n",
      "## Step 5: Calculate the sum\n",
      "To find the sum, we can first find a common denominator. Since 2 and 1 can both be written as $\\frac{6}{6}$, we can rewrite the sum as $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6}$. Now we can add the fractions together. $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6} = \\frac{6}{6} + \\frac{10}{6} + \\frac{6}{6} = \\frac{22}{6}$. Finally, we can simplify the fraction by dividing both the numerator and denominator by their greatest common divisor, which is 2. This gives us $\\frac{22}{6} = \\frac{11}{3}$.\n",
      "\n",
      "The final answer is: $\\boxed{\\frac{11}{3}}$\n"
     ]
    }
   ],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff66373-39b6-48be-82cc-353eb69dc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0394287109375\n"
     ]
    }
   ],
   "source": [
    "# del(llm_regular)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00291de5-4c0f-42aa-986f-d666e082f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-17 17:42:41 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-17 17:42:41 config.py:549] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-17 17:42:41 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-17 17:42:42 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb93b0e7086d4be699d57fccddc2a12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 17:42:44 model_runner.py:1115] Loading model weights took 2.3029 GB\n",
      "INFO 03-17 17:42:44 worker.py:267] Memory profiling takes 0.38 seconds\n",
      "INFO 03-17 17:42:44 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 03-17 17:42:44 worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 18.73GiB.\n",
      "INFO 03-17 17:42:44 executor_base.py:111] # cuda blocks: 38353, # CPU blocks: 8192\n",
      "INFO 03-17 17:42:44 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.73x\n",
      "INFO 03-17 17:42:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 17:43:00 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.04 GiB\n",
      "INFO 03-17 17:43:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 16.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# increase gpu_memory_utilization=0.5\n",
    "llm_regular = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f47c83-57e6-40bc-8a2c-0ac681272ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it, est. speed input: 16.64 toks/s, output: 213.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.72 toks/s, output: 214.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.72 toks/s, output: 214.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.71 toks/s, output: 214.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.71 toks/s, output: 214.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Regular) - Latency: 2.9366s, Throughput: 214.19 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latency_vllm_regular, throughput_vllm_regular, avg_tokens, generated_text = measure_inference(llm_regular, None, prompt, max_new_tokens, num_runs, use_vllm=True)\n",
    "print(f\"vLLM (Regular) - Latency: {latency_vllm_regular:.4f}s, Throughput: {throughput_vllm_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85e7222e-3c18-4df9-9295-47f196458f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0\n",
      " \n",
      "\n",
      "## Step 1: Plug in the value of x in the function f(x) to find f(-2)\n",
      "To find the value of $f(-2)$, we plug in $x = -2$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-2) = \\frac{3(-2)-2}{-2-2} = \\frac{-6-2}{-4} = \\frac{-8}{-4} = 2$.\n",
      "\n",
      "## Step 2: Plug in the value of x in the function f(x) to find f(-1)\n",
      "To find the value of $f(-1)$, we plug in $x = -1$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-1) = \\frac{3(-1)-2}{-1-2} = \\frac{-3-2}{-3} = \\frac{-5}{-3} = \\frac{5}{3}$.\n",
      "\n",
      "## Step 3: Plug in the value of x in the function f(x) to find f(0)\n",
      "To find the value of $f(0)$, we plug in $x = 0$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(0) = \\frac{3(0)-2}{0-2} = \\frac{-2}{-2} = 1$.\n",
      "\n",
      "## Step 4: Add up the values of f(-2), f(-1), and f(0)\n",
      "Now that we have found the values of $f(-2)$, $f(-1)$, and $f(0)$, which are 2, $\\frac{5}{3}$, and 1 respectively, we can add them up. $f(-2) + f(-1) + f(0) = 2 + \\frac{5}{3} + 1$.\n",
      "\n",
      "## Step 5: Calculate the sum\n",
      "To find the sum, we can first find a common denominator. Since 2 and 1 can both be written as $\\frac{6}{6}$, we can rewrite the sum as $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6}$. Now we can add the fractions together. $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6} = \\frac{6}{6} + \\frac{10}{6} + \\frac{6}{6} = \\frac{22}{6}$. Finally, we can simplify the fraction by dividing both the numerator and denominator by their greatest common divisor, which is 2. This gives us $\\frac{22}{6} = \\frac{11}{3}$.\n",
      "\n",
      "The final answer is: $\\boxed{\\frac{11}{3}}$\n"
     ]
    }
   ],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06adc753-b349-4c62-80f6-5b5212aca561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it, est. speed input: 16.52 toks/s, output: 212.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=max_new_tokens, top_p=1.0, n=1)\n",
    "output = llm_regular.generate(prompt, sampling_params)\n",
    "generated_text = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5463dfae-4c58-462f-8e3e-21a5e17c2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "print(len(output[0].outputs[0].token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
