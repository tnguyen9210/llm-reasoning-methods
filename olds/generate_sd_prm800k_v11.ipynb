{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import build_conv, generate_k_steps, last\n",
    "\n",
    "from core import select_diverse\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 16:39:24 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-07 16:39:24 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-07 16:39:32 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-07 16:39:32 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-07 16:39:34 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-07 16:39:34 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-07 16:39:35 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1474dcf35b9c49f98a5322354a475a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 16:39:37 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-07 16:39:37 worker.py:267] Memory profiling takes 0.67 seconds\n",
      "INFO 04-07 16:39:37 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-07 16:39:37 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 04-07 16:39:38 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 04-07 16:39:38 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 04-07 16:39:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 16:39:56 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 04-07 16:39:56 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590d4828-9a59-4431-8054-d1859ec213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    q_idx: int\n",
    "    question: str\n",
    "    templated_conv: str\n",
    "    current_text: str | None\n",
    "    next_texts: list[str] | None\n",
    "    lookahead_texts: list[str] | None\n",
    "    stop_reasons: list[str | None] | None\n",
    "    best_scores: list[float]  # the PRM scores\n",
    "    all_scores: list[list[float]]  # all PRM scores\n",
    "    previous_text: str | None\n",
    "    pruned: False\n",
    "    history: list[str]\n",
    "    completed: bool = False\n",
    "    completion_tokens: int = 0\n",
    "\n",
    "\n",
    "def _select_diverse(X_embeds, X_lprobs, X_ppl, K, V):\n",
    "    num_arms = len(X_embeds)\n",
    "    _V = copy.deepcopy(V)\n",
    "    A_idxes = []\n",
    "    A_embeds = []\n",
    "    tol = 0.0001\n",
    "    for it in range(K):\n",
    "        _V_inv = np.linalg.inv(_V)\n",
    "        arm_vals = np.einsum('ij,jk,ik->i', X_embeds, _V_inv, X_embeds)\n",
    "        max_val = np.max([val for idx, val in enumerate(arm_vals) if idx not in A_idxes])\n",
    "        # candidate_idxes = np.where(np.abs(arm_vals-max_val) < tol)[0]\n",
    "        candidate_idxes = [\n",
    "            arm_idx for arm_idx, arm_val in enumerate(arm_vals)\n",
    "            if (np.abs(max_val - arm_val) <= tol) and (arm_idx not in A_idxes)\n",
    "        ]\n",
    "\n",
    "        best_idx = max(candidate_idxes, key=lambda i: X_ppl[i])\n",
    "        # print(arm_vals)\n",
    "        # print(X_lprobs)\n",
    "        # print(candidate_idxes)\n",
    "        # print(best_idx)\n",
    "\n",
    "        best_embeds = X_embeds[best_idx]\n",
    "        # print(best_embeds.shape)\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(best_embeds, best_embeds.T)\n",
    "\n",
    "        # update A\n",
    "        A_idxes.append(best_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(A_idxes)\n",
    "\n",
    "    return A_idxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8582e6a-b80c-47e7-ba9b-f8c1296b816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def _select_diverse_search(batch_of_questions, config: Config, llm_vllm: LLM, llm_tf, llm_tokenizer) -> list[Beam]:\n",
    "    \n",
    "\n",
    "    # beams: list[Beam] = []\n",
    "    # for q_idx, question in enumerate(batch_of_questions):\n",
    "    #     beams.append(\n",
    "    #         Beam(\n",
    "    #             q_idx=q_idx,\n",
    "    #             question=question,\n",
    "    #             templated_conv=\"\",\n",
    "    #             current_text=\"\",\n",
    "    #             next_texts=None,\n",
    "    #             lookahead_texts=None,\n",
    "    #             pruned=False,\n",
    "    #             completed=False,  # New flag to track completion\n",
    "    #             stop_reasons=None,\n",
    "    #             history=[],\n",
    "    #             best_scores=[],\n",
    "    #             all_scores=[],\n",
    "    #             previous_text=None,\n",
    "    #             completion_tokens=0,\n",
    "    #         )\n",
    "    #     )          \n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "    beams: list[Beam] = []\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        for _ in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    q_idx=q_idx,\n",
    "                    question=question,\n",
    "                    templated_conv=\"\",\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    \n",
    "    # completed_answer = [[] for _ in range(len(batch_of_questions))]\n",
    "    # completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "    active_beams = [b for b in beams if not b.pruned]\n",
    "    \n",
    "    convs = [\n",
    "        build_conv(b.question, b.current_text, config.system_prompt)\n",
    "        for b in active_beams\n",
    "    ]\n",
    "    # continue_final_message = i > 0\n",
    "    # add_generation_prompt = i == 0\n",
    "\n",
    "    tokenizer = llm_vllm.get_tokenizer()\n",
    "\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=False,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    # # templated_convs = [c for conv in templated_convs for c in [conv]*config.n]\n",
    "    # for t_idx, conv in enumerate(templated_convs):\n",
    "    #     print(f\"t_idx = {t_idx}\")\n",
    "    #     print(conv)\n",
    "    # stop\n",
    "    # # print(templated_convs)\n",
    "    # stop\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    lookahead = 0\n",
    "    gen_results = generate_k_steps(\n",
    "        templated_convs, lookahead, llm_vllm, sampling_params, 1\n",
    "    )\n",
    "\n",
    "    # prompts, completions = [], []\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    _templated_convs = [[] for _ in range(len(batch_of_questions))]\n",
    "    # completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "    r_idx = 0\n",
    "    for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "        idx = r_idx // config.n\n",
    "        beam.next_texts = gen_result.next_texts\n",
    "        beam.stop_reasons = gen_result.stop_reasons\n",
    "        beam.lookahead_texts = gen_result.lookahead_texts\n",
    "        beam.completion_tokens += gen_result.completion_tokens\n",
    "        beam.current_text += gen_result.next_texts[0]\n",
    "        # beam.history.append(beam.next_texts[0])\n",
    "        beam.templated_prompt = gen_result.prompt\n",
    "        # pprint.pprint(gen_result)\n",
    "        # print(f\"beam.next_texts = {beam.next_texts}\")\n",
    "        # print(f\"beam.stop_reasons = {beam.stop_reasons}\")\n",
    "        # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "        # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "        # stop\n",
    "        \n",
    "        if (\n",
    "            beam.stop_reasons[0] == \"EOS\"\n",
    "            or beam.stop_reasons[0] == \"length\"\n",
    "            or beam.next_texts[0] == \"\"\n",
    "        ):\n",
    "            beam.completed = True\n",
    "            completed_beams.append(beam)\n",
    "            # continue\n",
    "\n",
    "        # prompts.append(beam.prompt)\n",
    "        completions[idx].append([beam.current_text])\n",
    "        # _templated_convs[idx]\n",
    "        \n",
    "    \n",
    "        r_idx += 1\n",
    "\n",
    "    active_beams = [b for b in active_beams if not b.completed]\n",
    "    \n",
    "    # pprint.pprint(completions)\n",
    "    # # get completion's embeddings\n",
    "    batch_completions_embeds = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_completions_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_completions_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_beams = [[] for _ in range(len(batch_of_questions))]\n",
    "    # completions_log_probs = np.zeros(len(active_beams))\n",
    "    # completions_ppl = np.zeros(len(active_beams))\n",
    "\n",
    "    # get completion's embeddings \n",
    "    for b_idx, beam in enumerate(active_beams):\n",
    "        with torch.no_grad():\n",
    "            # get beam.current_text which include previous all steps upto now\n",
    "            gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "            # print(gen_prompt)\n",
    "            # stop\n",
    "            inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "            outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "\n",
    "            # Get last_token_embeds\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "            # print(last_token_embeds.shape)\n",
    "\n",
    "            # Compute otuput_log_prob\n",
    "            # Prepare labels: shift input_ids to the right by one\n",
    "            # print(inputs)\n",
    "            labels = inputs['input_ids'][:, 1:]   \n",
    "            shifted_logits = outputs.logits[:, :-1, :]\n",
    "            loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "            completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "            completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "            # print(sent_ppl)\n",
    "            # print(loss)\n",
    "\n",
    "            # normalize the embeds\n",
    "            if config.normalize_embeds:\n",
    "                norm = np.linalg.norm(last_token_embeds)\n",
    "                last_token_embeds /= norm\n",
    "                # print(np.linalg.norm(last_token_embeds))\n",
    "\n",
    "            batch_completions_embeds[beam.q_idx].append(last_token_embeds)\n",
    "            batch_completions_log_probs[beam.q_idx].append(completion_log_prob)\n",
    "            batch_completions_ppl[beam.q_idx].append(completion_ppl)\n",
    "            batch_beams[beam.q_idx].append(beam)\n",
    "\n",
    "    pprint.pprint(len(batch_completions_embeds))\n",
    "    pprint.pprint(len(batch_completions_log_probs))\n",
    "    pprint.pprint(len(batch_completions_ppl))\n",
    "\n",
    "    # selected_idxes = [[] for _ in range(len(batch_of_questions))]\n",
    "    for q_idx in range(len(batch_of_questions)):\n",
    "        V = config.lam*np.eye(2048)\n",
    "        K = int(config.n / config.beam_width)\n",
    "        if len(active_beams) <= K:\n",
    "            continue \n",
    "\n",
    "        selected_idxes = _select_diverse(\n",
    "            batch_completions_embeds[q_idx], batch_completions_log_probs[q_idx], batch_completions_ppl[q_idx], K, V)\n",
    "\n",
    "        print(selected_idxes)\n",
    "        \n",
    "        for idx, beam in enumerate(batch_beams[q_idx]):\n",
    "            if idx not in selected_idxes:\n",
    "                beam.pruned = True \n",
    "\n",
    "    # pprint.pprint(beams)\n",
    "    active_beams = [b for b in beams if not b.pruned]\n",
    "    # pprint.pprint(active_beams)\n",
    "\n",
    "    extended_beams = []\n",
    "    for beam in active_beams:\n",
    "        for j in range(config.beam_width):\n",
    "            extended_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "    active_beams = extended_beams\n",
    "                                  \n",
    "    convs = [\n",
    "        build_conv(b.question, b.current_text, config.system_prompt)\n",
    "        for b in active_beams\n",
    "    ]\n",
    "    # continue_final_message = i > 0\n",
    "    # add_generation_prompt = i == 0\n",
    "\n",
    "    tokenizer = llm_vllm.get_tokenizer()\n",
    "\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs,\n",
    "        add_generation_prompt=False,\n",
    "        continue_final_message=True, # setting this False will add token <|eot_id|> end of turn \n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    # for conv in templated_convs:\n",
    "    #     print(conv)\n",
    "\n",
    "    # Last iteration, generate to EOS\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    lookahead = 0\n",
    "    gen_results = generate_k_steps(\n",
    "        templated_convs, lookahead, llm_vllm, sampling_params, 1)\n",
    "\n",
    "    for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "        beam.next_texts = gen_result.next_texts\n",
    "        beam.stop_reasons = gen_result.stop_reasons\n",
    "        beam.lookahead_texts = gen_result.lookahead_texts\n",
    "        beam.completion_tokens += gen_result.completion_tokens\n",
    "        beam.current_text += beam.next_texts[0]\n",
    "\n",
    "        beam.templated_prompt = gen_result.prompt\n",
    "\n",
    "        if (\n",
    "            beam.stop_reasons[0] == \"EOS\"\n",
    "            or beam.stop_reasons[0] == \"length\"\n",
    "            or beam.next_texts[0] == \"\"\n",
    "        ):\n",
    "            beam.completed = True\n",
    "            completed_beams.append(beam)\n",
    "\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        # print(active_beams)\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            print(\"break\")\n",
    "            break\n",
    "\n",
    "    return completed_beams\n",
    "\n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 1\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "level = '4'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "beam_results = _select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 2\n",
      "num_trials = 1\n",
      "\n",
      "-> 0\n",
      "8\n",
      "8\n",
      "2\n",
      "4\n",
      "\n",
      "-> 1\n",
      "8\n",
      "0\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "def _select_diverse_search(batch_of_questions, config: Config, llm_vllm: LLM, llm_tf, llm_tokenizer) -> list[Beam]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "    \n",
    "    completed_beams: list[Beam] = []\n",
    "    beams: list[Beam] = []\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        for _ in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    q_idx=q_idx,\n",
    "                    question=question,\n",
    "                    templated_conv=\"\",\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            ) \n",
    "\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for it in range(config.num_iterations):\n",
    "        print(f\"\\n-> {it}\")\n",
    "        if it == 0:\n",
    "            active_beams = beams\n",
    "        else:\n",
    "            # active_beams = [b for b in active_beams if not b.pruned]\n",
    "            extended_beams = []\n",
    "            for beam in active_beams:\n",
    "                if beam.pruned:\n",
    "                    continue \n",
    "                    \n",
    "                for j in range(config.beam_width):\n",
    "                    extended_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "            active_beams = extended_beams\n",
    "        \n",
    "        print(len(active_beams))\n",
    "        convs = [\n",
    "            build_conv(b.question, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "\n",
    "        add_generation_prompt = it == 0\n",
    "        continue_final_message = it > 0\n",
    "    \n",
    "        tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "            \n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        # Last iteration, generate to EOS\n",
    "        if it == config.num_iterations - 1:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        lookahead = 0 if it == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm_vllm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        # Collecct gen_results into beams\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += gen_result.next_texts[0]\n",
    "            # beam.history.append(beam.next_texts[0])\n",
    "            beam.templated_prompt = gen_result.prompt\n",
    "            # pprint.pprint(gen_result)\n",
    "            # print(f\"beam.next_texts = {beam.next_texts}\")\n",
    "            # print(f\"beam.stop_reasons = {beam.stop_reasons}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # stop\n",
    "            \n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "                # continue\n",
    "        \n",
    "        # Filter out comleted beams \n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        print(len(active_beams))\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            print(\"break\")\n",
    "            break\n",
    "        \n",
    "        # Extract completion's embeddings and other info\n",
    "        batch_embeds = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_beams = [[] for _ in range(len(batch_of_questions))]\n",
    "    \n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "                # print(gen_prompt)\n",
    "                # stop\n",
    "                inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "    \n",
    "                # Get last_token_embeds\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_token_embeds.shape)\n",
    "    \n",
    "                # Compute otuput_log_prob\n",
    "                # Prepare labels: shift input_ids to the right by one\n",
    "                labels = inputs['input_ids'][:, 1:]   \n",
    "                shifted_logits = outputs.logits[:, :-1, :]\n",
    "                loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "                completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "                completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "                # print(sent_ppl)\n",
    "                # print(loss)\n",
    "    \n",
    "                # normalize the embeds\n",
    "                if config.normalize_embeds:\n",
    "                    norm = np.linalg.norm(last_token_embeds)\n",
    "                    last_token_embeds /= norm\n",
    "                    # print(np.linalg.norm(last_token_embeds))\n",
    "    \n",
    "                batch_embeds[beam.q_idx].append(last_token_embeds)\n",
    "                batch_log_probs[beam.q_idx].append(completion_log_prob)\n",
    "                batch_ppl[beam.q_idx].append(completion_ppl)\n",
    "                batch_beams[beam.q_idx].append(beam)\n",
    "    \n",
    "        # pprint.pprint(len(batch_completions_embeds))\n",
    "        # pprint.pprint(len(batch_completions_log_probs))\n",
    "        # pprint.pprint(len(batch_completions_ppl))\n",
    "        print(len(batch_beams))\n",
    "        print(len(batch_beams[0]))\n",
    "\n",
    "        # Use _select_diverse to diversify embeddings \n",
    "        for q_idx in range(len(batch_of_questions)):\n",
    "            V = config.lam*np.eye(2048)\n",
    "            K = int(config.n / config.beam_width)\n",
    "            if len(batch_beams[q_idx]) <= K:\n",
    "                continue \n",
    "    \n",
    "            selected_idxes = _select_diverse(\n",
    "                batch_embeds[q_idx], batch_log_probs[q_idx], batch_ppl[q_idx], K, V)\n",
    "    \n",
    "            # print(selected_idxes)\n",
    "            \n",
    "            for idx, beam in enumerate(batch_beams[q_idx]):\n",
    "                if idx not in selected_idxes:\n",
    "                    beam.pruned = True \n",
    "\n",
    "        # tasks = [(q_idx, batch_embeds[q_idx],\n",
    "        #           batch_log_probs[q_idx], batch_ppl[q_idx], config) for q_idx in range(len(batch_of_questions))]\n",
    "        # tasks = [(q_idx, config) for q_idx in range(len(batch_of_questions))]\n",
    "\n",
    "        # with mp.Pool() as pool:\n",
    "        #     pool.starmap(process_select_diverse, tasks)\n",
    "                \n",
    "\n",
    "    # Collect the completions from beams\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "\n",
    "    # for r_idx, r in enumerate(responses):\n",
    "    #     # print(r.request_id)\n",
    "    #     if len(r.outputs) != config.n:\n",
    "    #         raise ValueError(f\"Generated {len(r.outputs)} completions instead of {config.n}\")\n",
    "            \n",
    "    #     for output in r.outputs:\n",
    "    #         # print(output.text)\n",
    "    #         # print(output.stop_reason)\n",
    "    #         completions[r_idx].append(output.text)\n",
    "    #         completion_ntokens[r_idx].append(len(output.token_ids))\n",
    "    for beam in completed_beams:\n",
    "        completions[beam.q_idx].append(beam.current_text)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    # results[\"completion_ntokens\"] = completion_ntokens\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# def process_select_diverse(q_idx, config):\n",
    "#     V = config.lam*np.eye(2048)\n",
    "#     K = int(config.n/config.beam_width)\n",
    "\n",
    "#     # if len(q_active_beams) <= K:\n",
    "#     #     return (q_idx, None)\n",
    "\n",
    "#     # selected_idxes  = _select_diverse(q_embeds, q_log_probs, q_ppl, K, V)\n",
    "\n",
    "#     # for idx, beam in enumerate(q_active_beams):\n",
    "#     #     if idx not in selected_idxes:\n",
    "#     #         beam.pruned = True \n",
    "            \n",
    "#     return  \n",
    "#     # return (q_idx, selected_idxes) \n",
    "        \n",
    "\n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "level = '4'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "results = _select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(beam_results))\n",
    "pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
