{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "import logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.utils.score import aggregate_scores\n",
    "# from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from core.reward_models import RLHFFlow\n",
    "from core import select_diverse\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3']\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 14:07:57 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-20 14:07:57 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-20 14:08:04 config.py:549] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-20 14:08:04 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-20 14:08:06 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-20 14:08:06 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-20 14:08:06 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d872fff93a5749a1a19621e586124c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 14:08:08 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-20 14:08:09 worker.py:267] Memory profiling takes 0.50 seconds\n",
      "INFO 04-20 14:08:09 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-20 14:08:09 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 04-20 14:08:09 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 04-20 14:08:09 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 31.73 GiB of which 192.00 KiB is free. Process 480043 has 23.18 GiB memory in use. Including non-PyTorch memory, this process has 8.54 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 9.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# baseline: gpu_memory_utilization=0.2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# use the standard model \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm_vllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm_tokenizer_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Utilize 50% of GPU memory\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# enable_chunked_prefill=False, # and enable_chunked_prefill\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# # use the gguf quantized model \u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# llm_regular = LLM(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#     model = llm_dir,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#     dtype = \"float16\",\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#     seed = 123)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect();torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache();\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:276\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:434\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    436\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit engine (profile, create kv cache, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m), elapsed)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/executor/executor_base.py:122\u001b[0m, in \u001b[0;36mExecutorBase.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitialize_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/worker/worker.py:306\u001b[0m, in \u001b[0;36mWorker.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    304\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warm_up_model()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/worker/worker.py:311\u001b[0m, in \u001b[0;36mWorker._init_cache_engine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline_parallel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/worker/worker.py:312\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    315\u001b[0m     ]\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/worker/cache_engine.py:69\u001b[0m, in \u001b[0;36mCacheEngine.__init__\u001b[0;34m(self, cache_config, model_config, parallel_config, device_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m get_attn_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size,\n\u001b[1;32m     62\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     63\u001b[0m                                      cache_config\u001b[38;5;241m.\u001b[39mcache_dtype,\n\u001b[1;32m     64\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m     65\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mis_attention_free,\n\u001b[1;32m     66\u001b[0m                                      use_mla\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39muse_mla)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allocate_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allocate_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cpu_blocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/worker/cache_engine.py:103\u001b[0m, in \u001b[0;36mCacheEngine._allocate_kv_cache\u001b[0;34m(self, num_blocks, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m     alloc_shape \u001b[38;5;241m=\u001b[39m kv_cache_shape\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_layers):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# null block in CpuGpuBlockAllocator requires at least that\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# block to be zeroed-out.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# We zero-out everything for simplicity.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     layer_kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43malloc_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# If we allocated with padding for alignment reasons truncate the\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# shape while preserving the aligned stride\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign_cache:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 31.73 GiB of which 192.00 KiB is free. Process 480043 has 23.18 GiB memory in use. Including non-PyTorch memory, this process has 8.54 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 9.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acd21de6ec84b09bc3cd3a3b27bef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 21.061745643615723\n",
      "#--- memory: 14.95752763748169\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:3')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beam_search(batch_of_questions, config, llm, prm):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    beams: list[Beam] = []\n",
    "    for prompt in batch_of_questions:\n",
    "        for i in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    prompt=prompt,\n",
    "                    index=i,\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for i in range(config.num_iterations):\n",
    "        # print(f\"\\n-> i = {i}\")\n",
    "        if i == 0:\n",
    "            active_beams = [b for b in beams if not b.pruned]\n",
    "        else:\n",
    "            active_beams = [b for b in active_beams if not b.pruned]\n",
    "        # print(\"n-> pass 1\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "\n",
    "        # Duplicate active beams to ensure that we have config.n beams per iteration\n",
    "        if len(active_beams) != config.n:\n",
    "            repeats = (config.n // len(active_beams)) + 1\n",
    "            logger.debug(\n",
    "                f\"Extending active_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "            )\n",
    "            extended_active_beams = [\n",
    "                copy.deepcopy(b) for b in (active_beams * repeats)[: config.n]\n",
    "            ]\n",
    "            active_beams = extended_active_beams\n",
    "            if len(active_beams) != config.n:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {config.n} active beams, but got {len(active_beams)}\"\n",
    "                )\n",
    "        \n",
    "        # print(\"n-> pass 2\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "            \n",
    "        if i == config.num_iterations - 1:\n",
    "            # Last iteration, generate to EOS\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        convs = [\n",
    "            build_conv(b.prompt, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "        continue_final_message = i > 0\n",
    "        add_generation_prompt = i == 0\n",
    "\n",
    "        tokenizer = llm.get_tokenizer()\n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        lookahead = 0 if i == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        prompts, completions = [], []\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += beam.next_texts[0]\n",
    "            beam.history.append(beam.next_texts[0])\n",
    "\n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "            prompts.append(beam.prompt)\n",
    "            completions.append([beam.current_text])\n",
    "\n",
    "        scores = prm.score(prompts, completions)\n",
    "        \n",
    "\n",
    "        agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "            for score in scores\n",
    "        ]\n",
    "        # print(f\"scores = {scores}\")\n",
    "        # print(f\"agg_scores = {agg_scores}\")\n",
    "\n",
    "        for beam, score in zip(active_beams, scores, strict=True):\n",
    "            beam.all_scores = score[0]\n",
    "\n",
    "        # Now filter active_beams and agg_scores for beams that are completed\n",
    "        agg_scores = [\n",
    "            agg_scores[i] for i, b in enumerate(active_beams) if not b.completed\n",
    "        ]\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "\n",
    "        # logger.debug(len(active_beams))\n",
    "        # print(\"n-> pass 3\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "        #     print(beam.completed)\n",
    "        #     print(beam.pruned)\n",
    "        \n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "        # Filter duplicate active beams\n",
    "        if config.filter_duplicates:\n",
    "            # Create a dictionary to filter duplicates and retain order\n",
    "            unique_beam_dict = {}\n",
    "            for i, b in enumerate(active_beams):\n",
    "                if b.current_text not in unique_beam_dict:\n",
    "                    unique_beam_dict[b.current_text] = (\n",
    "                        i  # Map the unique text to its index\n",
    "                    )\n",
    "            active_beams = [active_beams[i] for i in unique_beam_dict.values()]\n",
    "            agg_scores = [agg_scores[i] for i in unique_beam_dict.values()]\n",
    "\n",
    "        # Get indices for top (config.n / config.beam_width) completions\n",
    "        top_indices = np.argsort(np.array(agg_scores).flatten())[\n",
    "            -(config.n // config.beam_width) :\n",
    "        ]\n",
    "\n",
    "        for idx, beam in enumerate(active_beams):\n",
    "            if idx not in top_indices:\n",
    "                beam.pruned = True\n",
    "\n",
    "    # Filter completed beams for those with top config.n scores\n",
    "    if config.sort_completed:\n",
    "        completed_beams = sorted(\n",
    "            completed_beams,\n",
    "            key=lambda b: aggregate_scores(b.all_scores, config.agg_strategy),\n",
    "            reverse=True,\n",
    "        )[: config.n]\n",
    "    else:\n",
    "        completed_beams = completed_beams[: config.n]\n",
    "\n",
    "    if len(completed_beams) != config.n:\n",
    "        # If we don't have enough completed_beams, duplicate until we reach config.n\n",
    "        repeats = (config.n // len(completed_beams)) + 1\n",
    "        logger.debug(\n",
    "            f\"Extending completed_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "        )\n",
    "        extended_completed_beams = [\n",
    "            copy.deepcopy(b) for b in (completed_beams * repeats)[: config.n]\n",
    "        ]\n",
    "        completed_beams = extended_completed_beams\n",
    "\n",
    "    return completed_beams\n",
    "\n",
    "\n",
    "def beam_search(batch_of_questions, config, llm, prm):\n",
    "    # Collect the completions from responses\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        # print(f\"question {q_idx}\")\n",
    "        beam_results = _beam_search([question], config, llm, prm)\n",
    "        for b_idx, beam in enumerate(beam_results):\n",
    "            # print(beam.current_text)\n",
    "            completions[q_idx].append(beam.current_text)\n",
    "            completion_ntokens[q_idx].append(beam.completion_tokens)\n",
    "\n",
    "    # print(completions)\n",
    "    # print(completion_ntokens)\n",
    "    # stop\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    results[\"completion_ntokens\"] = completion_ntokens\n",
    "\n",
    "    return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4dc454b-dc6c-49a2-a88c-4c771118bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 2\n",
      "num_trials = 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trial_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m results \u001b[38;5;241m=\u001b[39m beam_search(batch_of_questions, config, llm_vllm, prm)\n\u001b[1;32m     21\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m---> 22\u001b[0m time_per_trial \u001b[38;5;241m=\u001b[39m total_time\u001b[38;5;241m/\u001b[39m(\u001b[43mtrial_idx\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m time_per_question \u001b[38;5;241m=\u001b[39m time_per_trial\u001b[38;5;241m/\u001b[39mnum_questions\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trial_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "config.sort_completed = False \n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "start_time = time.time()\n",
    "results = beam_search(batch_of_questions, config, llm_vllm, prm)\n",
    "total_time = time.time() - start_time\n",
    "trial_idx = 0\n",
    "time_per_trial = total_time/(trial_idx+1)\n",
    "time_per_question = time_per_trial/num_questions\n",
    "print(f\"trial {trial_idx}\")\n",
    "print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "print(f\"it takes {time_per_trial:0.4f}s per trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c67adb1-9a1a-4610-9d90-4aecfa879ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "it takes 2.7317s per question\n",
      "it takes 5.4635s per trial\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "defaultdict(<class 'list'>,\n",
      "            {'completion_ntokens': [[0, 0, 0, 0], [0, 0, 0, 0]],\n",
      "             'completions': [['## Step 1:  The problem involves finding the '\n",
      "                              'length of $DE$ in the given right-angled '\n",
      "                              'triangle $DEF$.\\n'\n",
      "                              '## Step 2:  We can start by applying the '\n",
      "                              'Pythagorean theorem to the entire triangle '\n",
      "                              '$DEF$, which states that for any right-angled '\n",
      "                              'triangle, the square of the length of the '\n",
      "                              'hypotenuse (the side opposite the right angle) '\n",
      "                              'is equal to the sum of the squares of the '\n",
      "                              'lengths of the other two sides.\\n'\n",
      "                              '## Step 3:  Therefore, we have $DE^2 + DF^2 = '\n",
      "                              'EF^2$.\\n'\n",
      "                              '## Step 4:  Since $\\\\sin D = 0.7$, we know that '\n",
      "                              '$\\\\sin D = \\\\frac{DE}{EF}$.\\n'\n",
      "                              '## Step 5:  Given that $\\\\sin D = '\n",
      "                              '\\\\frac{DE}{EF}$, we can set up the equation '\n",
      "                              '$\\\\frac{DE}{EF} = 0.7$.\\n'\n",
      "                              \"## Step 6:  Additionally, we're given the \"\n",
      "                              'length of $DF$ as $7$, which allows us to use '\n",
      "                              'the Pythagorean theorem to find the length of '\n",
      "                              '$EF$.\\n'\n",
      "                              '## Step 7:  Therefore, we have $EF^2 = DF^2 + '\n",
      "                              'DE^2$, and since $\\\\sin D = \\\\frac{DE}{EF} = '\n",
      "                              '0.7$, we can express $EF = 7$.\\n'\n",
      "                              '## Step 8:  Substituting the value of $EF$ into '\n",
      "                              'the equation $EF^2 = DF^2 + DE^2$ gives us $7^2 '\n",
      "                              '= DF^2 + DE^2$.\\n'\n",
      "                              '## Step 9:  To find the value of $DE$, we need '\n",
      "                              'to solve the equation $49 = DF^2 + DE^2$ for '\n",
      "                              '$DE$.\\n'\n",
      "                              '## Step 10:  By rearranging the equation, we '\n",
      "                              'get $DE^2 = 49 - DF^2$.\\n'\n",
      "                              '## Step 11:  We can substitute $DF = 7$ into '\n",
      "                              'the equation $DE^2 = 49 - DF^2$ to solve for '\n",
      "                              '$DE$.\\n'\n",
      "                              '## Step 12:  Therefore, $DE^2 = 49 - 7^2 = 49 - '\n",
      "                              '49 = 0$.\\n'\n",
      "                              '## Step 13:  As $DE^2 = 0$, we can take the '\n",
      "                              'square root of both sides to find $DE$.\\n'\n",
      "                              '## Step 14:  This gives us $DE = \\\\sqrt{0} = '\n",
      "                              '0$.\\n'\n",
      "                              '## Step 15:  So, the length of $DE$ is '\n",
      "                              '$\\\\boxed{0}$.',\n",
      "                              '## Step 1:  To solve for the length of side '\n",
      "                              '$DE$, we first need to understand the given '\n",
      "                              'diagram and the relationship between the '\n",
      "                              'different elements.\\n'\n",
      "                              \"## Step 2:  We're given that $\\\\sin D = 0.7$. \"\n",
      "                              'This implies that the ratio of the length of '\n",
      "                              'side $DF$ to the length of side $DE$ is $0.7$.\\n'\n",
      "                              '## Step 3:  Looking at the diagram, we see that '\n",
      "                              '$DF=7$. We can use this information to find the '\n",
      "                              'length of $DE$.\\n'\n",
      "                              '## Step 4:  Since we have the ratio '\n",
      "                              '$\\\\frac{DF}{DE} = 0.7$ and we know that $DF = '\n",
      "                              '7$, we can set up the equation $\\\\frac{7}{DE} = '\n",
      "                              '0.7$.\\n'\n",
      "                              '## Step 5:  To solve for $DE$, we can multiply '\n",
      "                              'both sides of the equation by $DE$, giving us '\n",
      "                              '$7 = 0.7 \\\\times DE$.\\n'\n",
      "                              '## Step 6:  We then divide both sides by $0.7$ '\n",
      "                              'to isolate $DE$, giving us $DE = '\n",
      "                              '\\\\frac{7}{0.7}$.\\n'\n",
      "                              '## Step 7:  This simplifies to $DE = 10$, so '\n",
      "                              'the length of side $DE$ is $10$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{10}$',\n",
      "                              '## Step 1:  We are given that $\\\\sin D = 0.7$, '\n",
      "                              'and we need to find the length of $DE$ in the '\n",
      "                              'given diagram.\\n'\n",
      "                              '## Step 2:  Since $\\\\sin D = \\\\frac{DE}{DF}$, '\n",
      "                              'we can substitute the given value of $\\\\sin D$ '\n",
      "                              'to get $0.7 = \\\\frac{DE}{7}$.\\n'\n",
      "                              '## Step 3:  To solve for $DE$, we need to '\n",
      "                              'isolate $DE$ in the equation. This can be done '\n",
      "                              'by multiplying both sides of the equation by '\n",
      "                              '$7$, resulting in $DE = 0.7 \\\\times 7$.\\n'\n",
      "                              '## Step 4:  Multiplying $0.7$ by $7$ gives us '\n",
      "                              '$DE = 4.9$.\\n'\n",
      "                              '## Step 5:  Therefore, the length of $DE$ is '\n",
      "                              '$4.9$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{4.9}$',\n",
      "                              '## Step 1:  To solve for the length of side '\n",
      "                              '$DE$, we first need to understand the given '\n",
      "                              'diagram and the relationship between the '\n",
      "                              'different elements.\\n'\n",
      "                              \"## Step 2:  We're given that $\\\\sin D = 0.7$. \"\n",
      "                              'This implies that the ratio of the length of '\n",
      "                              'side $DF$ to the length of side $DE$ is $0.7$.\\n'\n",
      "                              '## Step 3:  Looking at the diagram, we see that '\n",
      "                              '$DF=7$. We can use this information to find the '\n",
      "                              'length of $DE$.\\n'\n",
      "                              '## Step 4:  Since we have the ratio '\n",
      "                              '$\\\\frac{DF}{DE} = 0.7$ and we know that $DF = '\n",
      "                              '7$, we can set up the equation $\\\\frac{7}{DE} = '\n",
      "                              '0.7$.\\n'\n",
      "                              '## Step 5:  To solve for $DE$, we can multiply '\n",
      "                              'both sides of the equation by $DE$, giving us '\n",
      "                              '$7 = 0.7 \\\\times DE$.\\n'\n",
      "                              '## Step 6:  We then divide both sides by $0.7$ '\n",
      "                              'to isolate $DE$, giving us $DE = '\n",
      "                              '\\\\frac{7}{0.7}$.\\n'\n",
      "                              '## Step 7:  This simplifies to $DE = 10$, so '\n",
      "                              'the length of side $DE$ is $10$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{10}$'],\n",
      "                             ['## Step 1: Group the terms\\n'\n",
      "                              'We can group the terms in the series such that '\n",
      "                              'each pair of consecutive terms adds up to -1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Find the number of pairs\\n'\n",
      "                              'There are a total of 50 pairs, with the first '\n",
      "                              'term being 1 and the last term being 100.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of the pairs\\n'\n",
      "                              'Each pair adds up to -1, so the sum of all the '\n",
      "                              'pairs is -1 * 50 = -50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Calculate the sum of the remaining '\n",
      "                              'terms\\n'\n",
      "                              'There are 2 terms left, 99 and 100. Adding '\n",
      "                              'these two terms to the sum of the pairs gives '\n",
      "                              'us -50 + 1 = -49.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 5: Conclude\\n'\n",
      "                              'The final answer is: $\\\\boxed{-49}$',\n",
      "                              '## Step 1: Group terms in pairs from the left\\n'\n",
      "                              'We start by pairing up terms: (1 - 2), (3 - 4), '\n",
      "                              '(5 - 6), ..., (99 - 100).\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Simplify each pair\\n'\n",
      "                              'Simplifying each pair, we get -1, -1, -1, ..., '\n",
      "                              '-1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Count the number of pairs\\n'\n",
      "                              'There are 50 pairs in total.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Multiply the number of pairs by -1\\n'\n",
      "                              'Multiplying the number of pairs by -1, we get '\n",
      "                              '-50.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{-50}$',\n",
      "                              '## Step 1: Group the terms\\n'\n",
      "                              'We can group the terms in the series such that '\n",
      "                              'each pair of consecutive terms adds up to -1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Calculate the number of pairs\\n'\n",
      "                              'There are 50 terms in the series (1-2, 3-4, '\n",
      "                              '..., 99-100). We can calculate the number of '\n",
      "                              'pairs as 50/2 = 25.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of the series\\n'\n",
      "                              'Since each pair adds up to -1, the sum of the '\n",
      "                              'series is -1 * 25 = -25.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{-25}$',\n",
      "                              '## Step 1: Group terms in pairs from the left\\n'\n",
      "                              'We start by pairing up terms: (1 - 2), (3 - 4), '\n",
      "                              '(5 - 6), ..., (99 - 100).\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Observe the pattern in each pair\\n'\n",
      "                              'Each pair has a difference that is 1: -1, -1, '\n",
      "                              '-1, ..., -1. Since there are 50 such pairs, the '\n",
      "                              'sum of these pairs will be -1 * 50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of all pairs\\n'\n",
      "                              '- The sum of all pairs is -1 * 50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Add the last unpaired term\\n'\n",
      "                              'The last unpaired term is 100. Therefore, we '\n",
      "                              'add 100 to the sum of all pairs.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 5: Compute the final sum\\n'\n",
      "                              'The sum is -1 * 50 + 100.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 6: Simplify the sum\\n'\n",
      "                              '-50 + 100 is 50.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{50}$']]})\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
