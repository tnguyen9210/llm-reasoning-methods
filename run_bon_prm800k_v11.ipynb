{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60830ff9-7b7c-4cbc-a609-b19600063f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d622e741-3535-466f-8581-9ea57167f873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:57:23 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6875041-970b-4322-be56-a78ba6315520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import aggregate_scores\n",
    "\n",
    "from reward_models import RLHFFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a563f2-4f96-40c0-bacd-39c57d0854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff59d566-fb30-4b4c-af7a-9a278c61332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-22 21:57:34 [config.py:2599] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-22 21:57:43 [config.py:583] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-22 21:57:43 [arg_utils.py:1765] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-22 21:57:43 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-22 21:57:45 [cuda.py:234] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-22 21:57:45 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-22 21:57:46 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-22 21:57:46 [model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bf64e9e82a463c8c9e6159a979bc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:57:48 [loader.py:429] Loading weights took 1.32 seconds\n",
      "INFO 03-22 21:57:48 [model_runner.py:1146] Model loading took 2.3185 GB and 1.605521 seconds\n",
      "INFO 03-22 21:57:49 [worker.py:267] Memory profiling takes 0.76 seconds\n",
      "INFO 03-22 21:57:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.50) = 15.87GiB\n",
      "INFO 03-22 21:57:49 [worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 12.25GiB.\n",
      "INFO 03-22 21:57:49 [executor_base.py:111] # cuda blocks: 25088, # CPU blocks: 8192\n",
      "INFO 03-22 21:57:49 [executor_base.py:116] Maximum concurrency for 10000 tokens per request: 40.14x\n",
      "INFO 03-22 21:57:51 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:58:07 [model_runner.py:1570] Graph capturing finished in 16 secs, took 0.13 GiB\n",
      "INFO 03-22 21:58:07 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 18.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.584884643554688\n"
     ]
    }
   ],
   "source": [
    "# gpu_memory_utilization=0.2\n",
    "llm_vllm = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.5,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 10000,\n",
    "    # enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "# use the gguf quantized model \n",
    "# llm_vllm = LLM(\n",
    "#     model = llm_path,\n",
    "#     tokenizer = llm_tokenizer_path,\n",
    "#     tensor_parallel_size=1,\n",
    "#     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "#     max_model_len = 10000,\n",
    "#     dtype = \"float16\",\n",
    "#     seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06e00bb-67ea-49b5-977e-c7f6921c6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.584884643554688\n"
     ]
    }
   ],
   "source": [
    "# del(llm_vllm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0569159-cd06-4dbe-a1ac-ca482d121c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade0fca46ca349a89af1eae7e154e25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.95752763748169\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_path, device_map='cuda:1')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd1258f-4d8e-4712-bb88-3162ceca3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "data_by_levels = defaultdict(list)\n",
    "with open(f\"{dataset_path}/test.jsonl\", 'r', encoding='utf-8') as filein:\n",
    "    for line in filein:\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            # print(data['level'])\n",
    "            data_by_levels[f\"{data['level']}\"].append(data)\n",
    "\n",
    "    # data =  [json.loads(line) for line in filein if line.strip()]\n",
    "    # pprint.pprint(data, compact=True)\n",
    "\n",
    "for key in range(1,6):\n",
    "    key = str(key)\n",
    "    print(f\"{key}: {len(data_by_levels[key])}\")\n",
    "    # pprint.pprint(data_by_levels[key][:2], compact=True)\n",
    "# print(data_by_levels.keys())\n",
    "# pprint.pprint(data_by_levels['2'], compact=True)\n",
    "\n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8dceb2d-5df7-4c27-abe1-b43e2cbfb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n(batch_of_questions, config: Config, llm_vllm: LLM, prm: PRM, random_seed):\n",
    "\n",
    "    convs = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        for prompt in batch_of_questions\n",
    "    ]\n",
    "    \n",
    "    tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "    # TODO: set the augmented template from a file\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs, add_generation_prompt=True, tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Duplicate convs to generate config.n completions per prompt so we can do continous batching\n",
    "    # This makes [p1, p2, p3, p4] become [p1, p1, p2, p2, p3, p3, p4, p4] for e.g. config.n=2\n",
    "    # templated_convs = [c for conv in templated_convs for c in [conv] * config.n]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        # temperature=0,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=config.n,  # generate n outputs\n",
    "        best_of=config.n,\n",
    "        # stop=[\n",
    "        #     \"\\n\\n\"\n",
    "        # ],  # we consider that a step in the problem is indicated by a double newline\n",
    "        # include_stop_str_in_output=True,\n",
    "        seed=random_seed,\n",
    "    )        \n",
    "\n",
    "    # Generate responses \n",
    "    responses = llm_vllm.generate(\n",
    "        templated_convs,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Re-generate responses if we get more responses than expected\n",
    "    if len(responses) != len(batch_of_questions):\n",
    "        responses = llm_vllm.generate(\n",
    "            templated_convs,\n",
    "            sampling_params=sampling_params,\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "        assert len(responses) == len(batch_of_questions), \\\n",
    "            f\"Generated {len(responses)} responses instead of {len(batch_of_questions)}\"\n",
    "    \n",
    "    # Collect the completions from responses\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "\n",
    "    for r_idx, r in enumerate(responses):\n",
    "        # print(r.request_id)\n",
    "        if len(r.outputs) != config.n:\n",
    "            raise ValueError(f\"Generated {len(r.outputs)} completions instead of {config.n}\")\n",
    "            \n",
    "        for output in r.outputs[:config.n]:\n",
    "            # print(output.text)\n",
    "            # print(output.stop_reason)\n",
    "            completions[r_idx].append(output.text)\n",
    "            completion_ntokens[r_idx].append(len(output.token_ids))\n",
    "\n",
    "    # Compute the scores of completions\n",
    "    scores = prm.score(batch_of_questions, completions)\n",
    "    agg_scores = [\n",
    "        [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores\n",
    "    ]\n",
    "    # print(agg_scores)\n",
    "    # print(len(completions))\n",
    "\n",
    "    # results = {\"completions\": [], \"best_completions\": [], \"completion_tokens\": [], \"agg_scores\": [], \"best_agg_scores\": []}\n",
    "    # results = {\"questions\": [], \"gt_answers\": [], \"completions\": [], \"completion_ntokens\": [], \"agg_scores\": []}\n",
    "    results = {\"completions\": [], \"completion_ntokens\": [], \"agg_scores\": []}\n",
    "    # results[\"questions\"] = batch_of_questions\n",
    "    results[\"completions\"] = completions\n",
    "    results[\"completion_ntokens\"] = completion_ntokens\n",
    "    results[\"agg_scores\"] = agg_scores\n",
    "    \n",
    "    # for pidx in range(len(batch_of_questions)):\n",
    "    #     best_idx = np.argmax(agg_scores[pidx])\n",
    "    #     results[\"best_scores\"].append(agg_scores[pidx][best_idx])\n",
    "    #     results[\"best_completions\"].append(completions[pidx][best_idx])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0f6608-962d-4d39-8241-8eba153f3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 4                  # num of generations in BoN \n",
    "\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 10\n",
    "config.sort_completed = False\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19e68d-a061-4bbd-8e04-35c719befe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trials = 50\n",
      "num_questions = 43\n"
     ]
    }
   ],
   "source": [
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 10\n",
    "num_trials = 50\n",
    "config.n = 128\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "# batch_of_answers = [data_by_levels[level][q_idx]['answer'] for q_idx in range(num_questions)]\n",
    "\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "for trial_idx in range(num_trials):\n",
    "    results = best_of_n(batch_of_questions, config, llm_vllm, prm, 10000+trial_idx)\n",
    "    # results[\"gt_answers\"] = batch_of_answers\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # compute the time\n",
    "    total_time = time.time() - start_time\n",
    "    time_per_trial = total_time/(trial_idx+1)\n",
    "    time_per_question = time_per_trial/num_questions\n",
    "    if trial_idx % 5 == 0:\n",
    "        print(f\"trial {trial_idx}\")\n",
    "        print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "        print(f\"it takes {time_per_trial:0.4f}s for this trial\")\n",
    "\n",
    "print(f\"it takes {total_time:0.4f}s in total\")\n",
    "\n",
    "result_filename = f\"results/run_best_of_n_prm800k_level{level}_v21.json\"\n",
    "with open(result_filename, 'w+', encoding = 'utf-8') as fout:\n",
    "    json.dump(all_results, fout, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523223aa-30c5-4b27-87e2-df871f81933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_score = 0\n",
    "# correct_idxes = []\n",
    "# for q_idx in range(num_questions):\n",
    "#     print(f\"question {q_idx}\")\n",
    "#     # print(f\"question: {data_by_levels['4'][q_idx]['problem']}\")\n",
    "#     best_completion = results['best_completions'][q_idx]\n",
    "#     print(f\"best completion: {best_completion}\")\n",
    "#     pred_answer = extract_last_boxed_answer(best_completion)\n",
    "#     gt_answer = data_by_levels['4'][q_idx]['answer']\n",
    "#     is_correct = grader.grade_answer(pred_answer, gt_answer)\n",
    "#     print(f\"pred answer: {pred_answer}\")\n",
    "#     print(f\"gt answer: {gt_answer}\")\n",
    "#     print(f\"is correct: {is_correct}\")\n",
    "#     print(f\"all scores = {results['all_scores'][q_idx]}\")\n",
    "#     print(f\"best score = {results['best_scores'][q_idx]}\")\n",
    "#     if is_correct:\n",
    "#         correct_idxes.append(q_idx)\n",
    "\n",
    "# num_corrects = len(correct_idxes)\n",
    "# acc = num_corrects/num_questions\n",
    "# print(f\"num correct answers = {num_corrects}\")\n",
    "# print(f\"acc = {acc:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72d242-8e67-4987-97b3-398765beb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_last_boxed_answer(text):\n",
    "#     \"\"\"\n",
    "#     Extracts the content inside the last \\\\boxed{...} in the given text, \n",
    "#     handling nested braces properly.\n",
    "#     \"\"\"\n",
    "#     # Find the starting index of the last '\\\\boxed{'\n",
    "#     boxed_start = text.rfind('\\\\boxed{')\n",
    "#     if boxed_start == -1:\n",
    "#         return None  # No \\\\boxed{ found\n",
    "    \n",
    "#     # Start after the opening '{'\n",
    "#     start_index = boxed_start + len('\\\\boxed{')\n",
    "#     brace_count = 1  # We've seen the opening '{'\n",
    "#     content = ''\n",
    "    \n",
    "#     # Iterate through the text to find the matching closing brace\n",
    "#     for i in range(start_index, len(text)):\n",
    "#         char = text[i]\n",
    "#         if char == '{':\n",
    "#             brace_count += 1\n",
    "#         elif char == '}':\n",
    "#             brace_count -= 1\n",
    "#             if brace_count == 0:\n",
    "#                 return content.strip()  # Return content when braces balance\n",
    "#         content += char\n",
    "    \n",
    "#     return None  # No matching closing brace found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
