{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bf0993-2810-464e-a789-ae0684aca94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "import pprint\n",
    "import json\n",
    "import os, psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e76822-d253-4c26-b94d-39535e2f442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import aggregate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f69cfa-9e42-4826-9d07-a4899986e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 0.1%\n",
      "Total RAM: 503.68 GB\n",
      "Available RAM: 469.96 GB\n",
      "Used RAM: 15.77 GB\n",
      "RAM Usage Percentage: 6.7%\n",
      "['0', '1', '2', '3']\n",
      "\n",
      "-> gpu 0: Tesla V100S-PCIE-32GB\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 1: Tesla V100S-PCIE-32GB\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 2: Tesla V100S-PCIE-32GB\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 3: Tesla V100S-PCIE-32GB\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "cpu_percent = psutil.cpu_percent(interval=1)\n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# RAM usage\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {virtual_memory.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available RAM: {virtual_memory.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used RAM: {virtual_memory.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"RAM Usage Percentage: {virtual_memory.percent}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "    for gpu_index in  GPUS:\n",
    "        gpu_index = int(gpu_index)\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "        print(f\"\\n-> gpu {gpu_index}: {gpu_name}\")\n",
    "        # gpu_index = 0  # Change this if you have multiple GPUs\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(gpu_index)\n",
    "        allocated_memory = torch.cuda.memory_allocated(gpu_index)\n",
    "        free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "        print(f\"Total GPU Memory: {total_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Allocated GPU Memory: {allocated_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Available GPU Memory: {free_memory / 1024 ** 3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6d6d64f-f992-4279-8726-40ddf791326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLHFFlow(PRM):\n",
    "#     def load_model_and_tokenizer(\n",
    "#         self, **model_kwargs\n",
    "#     ) -> tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "#         prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "#         print(prm_tokenizer_path)\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             prm_tokenizer_path,\n",
    "#         )\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             prm_tokenizer_path,\n",
    "#             device_map=\"auto\",\n",
    "#             max_memory={1: \"16GB\", 2: \"16GB\", 3: \"16GB\"},\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             **model_kwargs,\n",
    "#         ).eval()\n",
    "#         tokenizer.padding_side = \"right\"\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#         model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "#         plus_tag_id = tokenizer.encode(\"+\")[-1]\n",
    "#         minus_tag_id = tokenizer.encode(\"-\")[-1]\n",
    "#         self.candidate_tokens = [plus_tag_id, minus_tag_id]\n",
    "\n",
    "#         return model, tokenizer\n",
    "\n",
    "#     def score(\n",
    "#         self,\n",
    "#         questions: list[str],\n",
    "#         outputs: list[list[str]],\n",
    "#         batched: bool = True,\n",
    "#         batch_size=8,\n",
    "#     ) -> list[list[float]]:\n",
    "#         if batched is True:\n",
    "#             print(f\"batched\")\n",
    "#             return self._score_batched(questions, outputs, batch_size=batch_size)\n",
    "#         else:\n",
    "#             print(f\"single\")\n",
    "#             return self._score_single(questions, outputs)\n",
    "\n",
    "#     def _score_single(self, questions: list[str], outputs: list[list[str]]):\n",
    "#         # reference code: https://github.com/RLHFlow/RLHF-Reward-Modeling/blob/main/math-rm/prm_evaluate.py\n",
    "#         all_scores = []\n",
    "#         for question, answers in zip(questions, outputs, strict=True):\n",
    "#             all_step_scores = []\n",
    "#             for ans in answers:\n",
    "#                 single_step_score = []\n",
    "#                 conversation = []\n",
    "#                 ans_list = ans.split(\"\\n\\n\")\n",
    "#                 # print(ans_list)\n",
    "#                 for k in range(len(ans_list)):\n",
    "#                     if k == 0:\n",
    "#                         # TODO: add the system prompt like we did for math shepard?\n",
    "#                         text = question + \" \" + ans_list[0]\n",
    "#                     else:\n",
    "#                         text = ans_list[k]\n",
    "#                     conversation.append({\"content\": text, \"role\": \"user\"})\n",
    "#                     conversation.append({\"content\": \"+\", \"role\": \"assistant\"})\n",
    "#                     # print(conversation)\n",
    "#                     input_ids = self.tokenizer.apply_chat_template(\n",
    "#                         conversation, return_tensors=\"pt\"\n",
    "#                     ).to(self.model.device)\n",
    "#                     # print(input_ids)\n",
    "#                     with torch.no_grad():\n",
    "#                         # model_outputs = self.model(input_ids)\n",
    "#                         # print(f\"model_outputs\")\n",
    "#                         # print(model_outputs)\n",
    "#                         # print(f\"model_outputs.logits\")\n",
    "#                         # print(model_outputs.logits)\n",
    "#                         # logits = model_outputs.logits[:,-3,self.candidate_tokens]\n",
    "#                         logits = self.model(input_ids).logits[\n",
    "#                             :, -3, self.candidate_tokens\n",
    "#                         ]  # simple version, the +/- is predicted by the '-3' position\n",
    "#                         # print(logits)\n",
    "#                         step_scores = logits.softmax(dim=-1)[\n",
    "#                             :, 0\n",
    "#                         ]  # 0 means the prob of + (1 mean -)\n",
    "#                         # print(step_scores)\n",
    "#                         single_step_score.append(\n",
    "#                             step_scores[0]\n",
    "#                             .detach()\n",
    "#                             .to(\"cpu\", dtype=torch.float32)\n",
    "#                             .item()\n",
    "#                         )\n",
    "#                 # stop\n",
    "#                 all_step_scores.append(single_step_score)\n",
    "#             all_scores.append(all_step_scores)\n",
    "#         return all_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cdad628-57a2-4aa5-8f64-bc59027b536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLHFFlow(PRM):\n",
    "#     def load_model_and_tokenizer(\n",
    "#         self, **model_kwargs\n",
    "#     ) -> tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             \"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "#         )\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\",\n",
    "#             device_map=\"auto\",\n",
    "#             max_memory={1: \"16GB\", 2: \"16GB\", 3: \"16GB\"},\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             **model_kwargs,\n",
    "#         ).eval()\n",
    "#         tokenizer.padding_side = \"right\"\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#         model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "#         plus_tag_id = tokenizer.encode(\"+\")[-1]\n",
    "#         minus_tag_id = tokenizer.encode(\"-\")[-1]\n",
    "#         self.candidate_tokens = [plus_tag_id, minus_tag_id]\n",
    "\n",
    "#         return model, tokenizer\n",
    "\n",
    "#     def score(\n",
    "#         self,\n",
    "#         questions: list[str],\n",
    "#         outputs: list[list[str]],\n",
    "#         batched: bool = True,\n",
    "#         batch_size=8,\n",
    "#     ) -> list[list[float]]:\n",
    "#         if batched is True:\n",
    "#             return self._score_batched(questions, outputs, batch_size=batch_size)\n",
    "#         else:\n",
    "#             return self._score_single(questions, outputs)\n",
    "\n",
    "#     def _score_single(self, questions: list[str], outputs: list[list[str]]):\n",
    "#         # reference code: https://github.com/RLHFlow/RLHF-Reward-Modeling/blob/main/math-rm/prm_evaluate.py\n",
    "#         all_scores = []\n",
    "#         for question, answers in zip(questions, outputs, strict=True):\n",
    "#             all_step_scores = []\n",
    "#             for ans in answers:\n",
    "#                 single_step_score = []\n",
    "#                 conversation = []\n",
    "#                 ans_list = ans.split(\"\\n\\n\")\n",
    "#                 for k in range(len(ans_list)):\n",
    "#                     if k == 0:\n",
    "#                         # TODO: add the system prompt like we did for math shepard?\n",
    "#                         text = question + \" \" + ans_list[0]\n",
    "#                     else:\n",
    "#                         text = ans_list[k]\n",
    "#                     conversation.append({\"content\": text, \"role\": \"user\"})\n",
    "#                     conversation.append({\"content\": \"+\", \"role\": \"assistant\"})\n",
    "#                     input_ids = self.tokenizer.apply_chat_template(\n",
    "#                         conversation, return_tensors=\"pt\"\n",
    "#                     ).to(self.model.device)\n",
    "#                     with torch.no_grad():\n",
    "#                         logits = self.model(input_ids).logits[\n",
    "#                             :, -3, self.candidate_tokens\n",
    "#                         ]  # simple version, the +/- is predicted by the '-3' position\n",
    "#                         step_scores = logits.softmax(dim=-1)[\n",
    "#                             :, 0\n",
    "#                         ]  # 0 means the prob of + (1 mean -)\n",
    "#                         # print(scores)\n",
    "#                         single_step_score.append(\n",
    "#                             step_scores[0]\n",
    "#                             .detach()\n",
    "#                             .to(\"cpu\", dtype=torch.float32)\n",
    "#                             .item()\n",
    "#                         )\n",
    "\n",
    "#                 all_step_scores.append(single_step_score)\n",
    "#             all_scores.append(all_step_scores)\n",
    "#         return all_scores\n",
    "\n",
    "#     def _score_batched(\n",
    "#         self, questions: list[str], outputs: list[list[str]], batch_size: int = 2\n",
    "#     ):\n",
    "#         # The RLHFlow models are trained to predict the \"+\" or \"-\" tokens in a dialogue, but since these are not unique\n",
    "#         # we need to introduce a dummy special token here for masking.\n",
    "\n",
    "#         special_tok_id = self.tokenizer(\"ки\", return_tensors=\"pt\").input_ids[0, 1]\n",
    "#         # We construct two parallel dialogues, one with a \"+\" token per assistant turn, the other with the dummy token \"ки\" for masking\n",
    "#         conversations = []\n",
    "#         conversations2 = []\n",
    "#         for question, answers in zip(questions, outputs, strict=True):\n",
    "#             for ans in answers:\n",
    "#                 conversation = []\n",
    "#                 conversation2 = []\n",
    "#                 ans_list = ans.split(\"\\n\\n\")\n",
    "#                 for k in range(len(ans_list)):\n",
    "#                     if k == 0:\n",
    "#                         text = question + \" \" + ans_list[0]\n",
    "#                     else:\n",
    "#                         text = ans_list[k]\n",
    "#                     conversation.append({\"content\": text, \"role\": \"user\"})\n",
    "#                     conversation.append({\"content\": \"+\", \"role\": \"assistant\"})\n",
    "\n",
    "#                     # we track to location of the special token with ки in order to extract the scores\n",
    "#                     conversation2.append({\"content\": text, \"role\": \"user\"})\n",
    "#                     conversation2.append({\"content\": \"ки\", \"role\": \"assistant\"})\n",
    "\n",
    "#                 conversations.append(conversation)\n",
    "#                 conversations2.append(conversation2)\n",
    "\n",
    "#         output_scores = []\n",
    "#         for i in range(0, len(conversations), batch_size):\n",
    "#             convs_batch = conversations[i : i + batch_size]\n",
    "#             convs2_batch = conversations2[i : i + batch_size]\n",
    "#             inputs_batch = self.tokenizer.apply_chat_template(\n",
    "#                 convs_batch, padding=True, return_tensors=\"pt\"\n",
    "#             ).to(self.model.device)\n",
    "#             inputs2_batch = self.tokenizer.apply_chat_template(\n",
    "#                 convs2_batch, padding=True, return_tensors=\"pt\"\n",
    "#             ).to(self.model.device)\n",
    "#             assert inputs_batch.shape == inputs2_batch.shape\n",
    "#             with torch.no_grad():\n",
    "#                 logits = self.model(inputs_batch).logits[:, :, self.candidate_tokens]\n",
    "#                 scores = logits.softmax(dim=-1)[\n",
    "#                     :, :, 0\n",
    "#                 ]  # 0 means the prob of + (1 mean -)\n",
    "\n",
    "#                 for i in range(len(convs_batch)):\n",
    "#                     # We slice on the N-1 token since the model is trained to predict the Nth one (\"+\" in this case)\n",
    "#                     step_scores_flat = scores[i, :-1][\n",
    "#                         inputs2_batch[i, 1:] == special_tok_id\n",
    "#                     ].tolist()\n",
    "#                     output_scores.append(step_scores_flat)\n",
    "\n",
    "#         # reshape the output scores to match the input\n",
    "#         reshaped_output_scores = []\n",
    "#         counter = 0\n",
    "\n",
    "#         for question, answers in zip(questions, outputs):\n",
    "#             scores = []\n",
    "#             for answer in answers:\n",
    "#                 scores.append(output_scores[counter])\n",
    "#                 counter += 1\n",
    "#             reshaped_output_scores.append(scores)\n",
    "\n",
    "#         return reshaped_output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e14a6f3-7db6-4168-8a20-3034f5981bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cad95611a04419b3a23b8cf786adee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# del(prm)\n",
    "prm = RLHFFlow(prm_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d6d43f-86f1-4991-bdd1-b68285965bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(prm.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb34bfe-2e29-41c2-a8f1-3690143ecf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 3.0098419189453125\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79ac76ea-afff-419b-bce5-d00d3171f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'Below is the graph of $y = a \\\\sin (bx + c) + d$ for some positive constants $a,$ $b,$ $c,$ and $d.$  Find the smallest possible value of $c.$\\n\\n[asy]import TrigMacros;\\n\\nsize(400);\\n\\nreal f(real x)\\n{\\n\\treturn 2*sin(3*x + pi) + 1;\\n}\\n\\ndraw(graph(f,-3*pi,3*pi,n=700,join=operator ..),red);\\ntrig_axes(-3*pi,3*pi,-4,4,pi/2,1);\\nlayer();\\nrm_trig_labels(-5,5, 2);\\n\\nlabel(\"$1$\", (0,1), E);\\nlabel(\"$2$\", (0,2), E);\\nlabel(\"$3$\", (0,3), E);\\nlabel(\"$-1$\", (0,-1), E);\\nlabel(\"$-2$\", (0,-2), E);\\nlabel(\"$-3$\", (0,-3), E);\\n[/asy]' \n",
    "prompt =  'If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.'\n",
    "max_new_tokens = 1024 \n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00291de5-4c0f-42aa-986f-d666e082f660",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gpu_memory_utilization=0.2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m(\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m llm_tokenizer_path,\n\u001b[1;32m      4\u001b[0m     gpu_memory_utilization \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# Utilize 50% of GPU memory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     max_model_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      6\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      9\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect();torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache();\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#--- memory:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLM' is not defined"
     ]
    }
   ],
   "source": [
    "# gpu_memory_utilization=0.2\n",
    "llm = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c429f1b7-a62d-417e-9a0d-86c9e51ae1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "data_by_levels = defaultdict(list)\n",
    "with open(f\"{dataset_path}/test.jsonl\", 'r', encoding='utf-8') as filein:\n",
    "    for line in filein:\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            # print(data['level'])\n",
    "            data_by_levels[f\"{data['level']}\"].append(data)\n",
    "\n",
    "    # data =  [json.loads(line) for line in filein if line.strip()]\n",
    "    # pprint.pprint(data, compact=True)\n",
    "\n",
    "for key in range(1,6):\n",
    "    key = str(key)\n",
    "    print(f\"{key}: {len(data_by_levels[key])}\")\n",
    "    # pprint.pprint(data_by_levels[key][:2], compact=True)\n",
    "# print(data_by_levels.keys())\n",
    "# pprint.pprint(data_by_levels['2'], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea813ac-02e0-4eb6-aead-4688d13d12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maxk_sequence(batch_of_prompts, config: Config):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    convs = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        for prompt in batch_of_prompts\n",
    "    ]\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "\n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # duplicate convs to generate config.n completions per prompt\n",
    "    # so we can do continous batching\n",
    "    # this makes [p1, p2, p3, p4] become [p1, p1, p2, p2, p3, p3, p4, p4] for e.g. config.n=2\n",
    "    # templated_convs = [c for conv in templated_convs for c in [conv] * config.n]\n",
    "\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=1,  # Since we've already duplicated the prompt_token_ids, we only need to generate 1 completion per prompt\n",
    "    )\n",
    "\n",
    "    # initialize empty lists for completions and completion tokens\n",
    "    completions = [[] for _ in range(len(batch_of_prompts))]\n",
    "    completion_tokens = [[] for _ in range(len(batch_of_prompts))]\n",
    "\n",
    "    num_gens = config.n\n",
    "    for i in range(len(completions)):\n",
    "        for _ in range(config.n // num_gens):\n",
    "            responses = llm.generate(\n",
    "                [c for conv in templated_convs for c in [conv] * num_gens],\n",
    "                sampling_params=sampling_params,\n",
    "                use_tqdm=False,\n",
    "            )\n",
    "            print(len(responses))\n",
    "\n",
    "            for r in responses[:num_gens]:\n",
    "                for output in r.outputs:\n",
    "                    completions[i].append(output.text)\n",
    "                    completion_tokens[i].append(len(output.token_ids))\n",
    "\n",
    "\n",
    "    print(f\"completion token counts: {completion_tokens}\")\n",
    "\n",
    "    # Check we generated the correct number of completions for each prompt\n",
    "    for c in completions:\n",
    "        if len(c) != config.n:\n",
    "            raise ValueError(f\"Generated {len(c)} completions instead of {config.n}\")\n",
    "\n",
    "    scores = prm.score(batch_of_prompts, completions)\n",
    "    print(scores)\n",
    "    agg_scores = [\n",
    "        [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores\n",
    "    ]\n",
    "    # print(scores)\n",
    "    print(agg_scores)\n",
    "    # print(len(agg_scores[0]))\n",
    "\n",
    "    max_score = 0\n",
    "    max_score_arr = np.zeros(config.n)\n",
    "    for i, score in enumerate(agg_scores[0]):\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "        max_score_arr[i] = max_score\n",
    "\n",
    "    # del(completions)\n",
    "    # del(responses)\n",
    "    # print(max_score_arr)\n",
    "    # stop\n",
    "    return max_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b9d2285-cffe-42e1-8b3a-f2ce1d3e3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_maxk_score(num_trials, batch_of_prompts, config: Config):\n",
    "\n",
    "    trial_max_score_arr = np.zeros((num_trials, config.n))\n",
    "    # for i in tqdm(range(num_trials)):\n",
    "    for p_idx, prompt in enumerate(batch_of_prompts):\n",
    "        print(f\"{p_idx}: {prompt}\")\n",
    "        for i in range(num_trials):\n",
    "            # print(f\"i_trial = {i}\")\n",
    "            trial_max_score_arr[i] = _maxk_sequence([prompt], config)\n",
    "\n",
    "        xrange = np.arange(1,config.n+1)\n",
    "        score_mean = np.mean(trial_max_score_arr, axis=0)\n",
    "        score_std = np.std(trial_max_score_arr, axis=0, ddof=1)\n",
    "        score_std /= np.sqrt(num_trials)\n",
    "        plt.plot(xrange, score_mean)\n",
    "        plt.fill_between(xrange, score_mean-score_std, score_mean+score_std, alpha=0.3)\n",
    "        # for i in range(num_trials):\n",
    "        #     plt.plot(xrange, trial_max_score_arr[i], alpha=0.3, color='b')\n",
    "    plt.xticks(xrange)\n",
    "    plt.show()\n",
    "\n",
    "    return trial_max_score_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76d0bee-bb88-4e87-97e7-d9827e707231",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.n = 5\n",
    "num_trials = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d7c9bba-fd3e-4335-9d97-121623b0f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The set of points $(x,y,z)$ that satisfy\n",
      "\\[2x = 3y = -z\\]is a line.\n",
      "\n",
      "The set of points $(x,y,z)$ that satisfy\n",
      "\\[6x = -y = -4z\\]is another line.\n",
      "\n",
      "Find the angle between these lines, in degrees.\n",
      "5\n",
      "completion token counts: [[570, 510, 669, 619, 499]]\n",
      "[[[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]]]\n",
      "[[0.5, 0.5, 0.5, 0.5, 0.5]]\n",
      "5\n",
      "completion token counts: [[751, 590, 656, 705, 788]]\n",
      "[[[0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]]]\n",
      "[[0.5, 0.5, 0.5, 0.5, 0.5]]\n",
      "5\n",
      "completion token counts: [[780, 648, 714, 495, 923]]\n",
      "[[[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]]]\n",
      "[[0.5, 0.5, 0.5, 0.5, 0.5]]\n",
      "5\n",
      "completion token counts: [[628, 567, 781, 727, 627]]\n",
      "[[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]]\n",
      "[[nan, nan, nan, nan, nan]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 69.53726148605347\n"
     ]
    }
   ],
   "source": [
    "# level-4 questions\n",
    "batch_of_prompts = [data_by_levels['4'][0]['problem']]\n",
    "# print(batch_of_prompts)\n",
    "start_time = time.time()\n",
    "exp_maxk_score(num_trials, batch_of_prompts, config)\n",
    "print(f\"time taken: {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
