{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60830ff9-7b7c-4cbc-a609-b19600063f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pprint\n",
    "import json\n",
    "import os, psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d622e741-3535-466f-8581-9ea57167f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6875041-970b-4322-be56-a78ba6315520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.config import Config\n",
    "# from sal.models.reward_models import PRM\n",
    "from sal.utils.score import aggregate_scores\n",
    "\n",
    "from reward_models import RLHFFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a563f2-4f96-40c0-bacd-39c57d0854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff59d566-fb30-4b4c-af7a-9a278c61332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 14:09:22 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-21 14:09:22 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-21 14:09:33 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-21 14:09:33 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-21 14:09:34 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-21 14:09:34 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-21 14:09:36 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6949c2a4e334e3d8b2a961174811650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 14:09:40 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 03-21 14:09:41 worker.py:267] Memory profiling takes 0.79 seconds\n",
      "INFO 03-21 14:09:41 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.20) = 6.35GiB\n",
      "INFO 03-21 14:09:41 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 2.73GiB.\n",
      "INFO 03-21 14:09:41 executor_base.py:111] # cuda blocks: 5592, # CPU blocks: 8192\n",
      "INFO 03-21 14:09:41 executor_base.py:116] Maximum concurrency for 10000 tokens per request: 8.95x\n",
      "INFO 03-21 14:09:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 14:09:58 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.13 GiB\n",
      "INFO 03-21 14:09:58 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 18.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.0653533935546875\n"
     ]
    }
   ],
   "source": [
    "# gpu_memory_utilization=0.2\n",
    "llm = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 10000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06e00bb-67ea-49b5-977e-c7f6921c6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.0653533935546875\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0569159-cd06-4dbe-a1ac-ca482d121c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3256065a7104f1185dcea6d9d46dcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.95752763748169\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_path, device_map='cuda:1')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd1258f-4d8e-4712-bb88-3162ceca3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "data_by_levels = defaultdict(list)\n",
    "with open(f\"{dataset_path}/test.jsonl\", 'r', encoding='utf-8') as filein:\n",
    "    for line in filein:\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            # print(data['level'])\n",
    "            data_by_levels[f\"{data['level']}\"].append(data)\n",
    "\n",
    "    # data =  [json.loads(line) for line in filein if line.strip()]\n",
    "    # pprint.pprint(data, compact=True)\n",
    "\n",
    "for key in range(1,6):\n",
    "    key = str(key)\n",
    "    print(f\"{key}: {len(data_by_levels[key])}\")\n",
    "    # pprint.pprint(data_by_levels[key][:2], compact=True)\n",
    "# print(data_by_levels.keys())\n",
    "# pprint.pprint(data_by_levels['2'], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dceb2d-5df7-4c27-abe1-b43e2cbfb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n(batch_of_prompts, config: Config, llm: LLM, prm):\n",
    "\n",
    "    convs = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        for prompt in batch_of_prompts\n",
    "    ]\n",
    "    \n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    # TODO: set the augmented template from a file\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs, add_generation_prompt=True, tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Duplicate convs to generate config.n completions per prompt so we can do continous batching\n",
    "    # This makes [p1, p2, p3, p4] become [p1, p1, p2, p2, p3, p3, p4, p4] for e.g. config.n=2\n",
    "    templated_convs = [c for conv in templated_convs for c in [conv] * config.n]\n",
    "\n",
    "    # Initialize empty lists for completions and completion tokens\n",
    "    completions = [[] for _ in range(len(batch_of_prompts))]\n",
    "    completion_tokens = [[] for _ in range(len(batch_of_prompts))]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=1,  # Since we've already duplicated the prompt_token_ids, we only need to generate 1 completion per prompt\n",
    "    )\n",
    "\n",
    "    responses = llm.generate(\n",
    "        templated_convs,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    if len(responses) != len(batch_of_prompts) * config.n:\n",
    "        raise ValueError(\n",
    "            f\"Generated {len(responses)} responses instead of {len(batch_of_prompts * config.n)}\"\n",
    "        )\n",
    "\n",
    "    for i in range(len(completions)):\n",
    "        completions[i] = [\n",
    "            output.text\n",
    "            for r in responses[i * config.n : (i + 1) * config.n]\n",
    "            for output in r.outputs\n",
    "        ]\n",
    "        completion_tokens[i] = [\n",
    "            len(output.token_ids)\n",
    "            for r in responses[i * config.n : (i + 1) * config.n]\n",
    "            for output in r.outputs\n",
    "        ]\n",
    "\n",
    "    # Check we generated the correct number of completions for each prompt\n",
    "    for c in completions:\n",
    "        if len(c) != config.n:\n",
    "            raise ValueError(f\"Generated {len(c)} completions instead of {config.n}\")\n",
    "\n",
    "    scores = prm.score(batch_of_prompts, completions)\n",
    "    agg_scores = [\n",
    "        [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores\n",
    "    ]\n",
    "    # print(agg_scores)\n",
    "    # print(len(completions))\n",
    "\n",
    "    results = {\"completions\": [], \"pred\": [], \"completion_tokens\": [], \"best_scores\": []}\n",
    "    results[\"completions\"] = completions\n",
    "    results[\"completion_tokens\"] = completion_tokens\n",
    "    \n",
    "    for pidx in range(len(batch_of_prompts)):\n",
    "        best_idx = np.argmax(agg_scores[pidx])\n",
    "        results[\"best_scores\"].append(agg_scores[pidx][best_idx])\n",
    "        results[\"pred\"].append(completions[pidx][best_idx])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0f6608-962d-4d39-8241-8eba153f3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 4                  # num of generations in BoN \n",
    "config.lookahead = 0\n",
    "config.num_iterations = 10\n",
    "config.sort_completed = False\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e3cdea-f692-4696-8f26-05b5ea4124ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "question 0\n",
      "question 1\n",
      "question 2\n",
      "question 3\n",
      "question 4\n",
      "question 5\n",
      "question 6\n",
      "question 7\n",
      "question 8\n",
      "question 9\n",
      "avg_scores: 0.8376220703125\n",
      "it takes 42.2492s\n"
     ]
    }
   ],
   "source": [
    "num_questions = len(data_by_levels['4'])\n",
    "num_questions = 10\n",
    "print(num_questions)\n",
    "all_scores =  []\n",
    "start_time = time.time()\n",
    "for q_idx in range(num_questions):\n",
    "    print(f\"question {q_idx}\")\n",
    "    batch_of_prompts = [data_by_levels['4'][q_idx]['problem']]\n",
    "    results = best_of_n(batch_of_prompts, config, llm, prm)\n",
    "    all_scores.append(results['best_scores'])\n",
    "    \n",
    "total_time = time.time() - start_time\n",
    "avg_scores = np.mean(all_scores)\n",
    "print(f\"avg_scores: {avg_scores}\")\n",
    "print(f\"it takes {total_time:0.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523223aa-30c5-4b27-87e2-df871f81933d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
