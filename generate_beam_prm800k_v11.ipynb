{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "import logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.utils.score import aggregate_scores\n",
    "# from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from core.reward_models import RLHFFlow\n",
    "from core import select_diverse\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-07 19:23:50 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-07 19:23:50 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-07 19:23:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-07 19:23:51 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0582c9ce078b4251bb3da8757102202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 19:23:52 model_runner.py:1115] Loading model weights took 2.3029 GB\n",
      "INFO 04-07 19:23:53 worker.py:267] Memory profiling takes 0.41 seconds\n",
      "INFO 04-07 19:23:53 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-07 19:23:53 worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 18.73GiB.\n",
      "INFO 04-07 19:23:53 executor_base.py:111] # cuda blocks: 38353, # CPU blocks: 8192\n",
      "INFO 04-07 19:23:53 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.73x\n",
      "INFO 04-07 19:23:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 19:24:12 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 04-07 19:24:12 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 21.061745643615723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acd21de6ec84b09bc3cd3a3b27bef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 21.061745643615723\n",
      "#--- memory: 14.95752763748169\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:1')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beam_search(batch_of_questions, config, llm, prm):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    beams: list[Beam] = []\n",
    "    for prompt in batch_of_questions:\n",
    "        for i in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    prompt=prompt,\n",
    "                    index=i,\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for i in range(config.num_iterations):\n",
    "        # print(f\"\\n-> i = {i}\")\n",
    "        if i == 0:\n",
    "            active_beams = [b for b in beams if not b.pruned]\n",
    "        else:\n",
    "            active_beams = [b for b in active_beams if not b.pruned]\n",
    "        # print(\"n-> pass 1\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "\n",
    "        # Duplicate active beams to ensure that we have config.n beams per iteration\n",
    "        if len(active_beams) != config.n:\n",
    "            repeats = (config.n // len(active_beams)) + 1\n",
    "            logger.debug(\n",
    "                f\"Extending active_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "            )\n",
    "            extended_active_beams = [\n",
    "                copy.deepcopy(b) for b in (active_beams * repeats)[: config.n]\n",
    "            ]\n",
    "            active_beams = extended_active_beams\n",
    "            if len(active_beams) != config.n:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {config.n} active beams, but got {len(active_beams)}\"\n",
    "                )\n",
    "        \n",
    "        # print(\"n-> pass 2\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "            \n",
    "        if i == config.num_iterations - 1:\n",
    "            # Last iteration, generate to EOS\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        convs = [\n",
    "            build_conv(b.prompt, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "        continue_final_message = i > 0\n",
    "        add_generation_prompt = i == 0\n",
    "\n",
    "        tokenizer = llm.get_tokenizer()\n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        lookahead = 0 if i == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        prompts, completions = [], []\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += beam.next_texts[0]\n",
    "            beam.history.append(beam.next_texts[0])\n",
    "\n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "            prompts.append(beam.prompt)\n",
    "            completions.append([beam.current_text])\n",
    "\n",
    "        scores = prm.score(prompts, completions)\n",
    "        \n",
    "\n",
    "        agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "            for score in scores\n",
    "        ]\n",
    "        # print(f\"scores = {scores}\")\n",
    "        # print(f\"agg_scores = {agg_scores}\")\n",
    "\n",
    "        for beam, score in zip(active_beams, scores, strict=True):\n",
    "            beam.all_scores = score[0]\n",
    "\n",
    "        # Now filter active_beams and agg_scores for beams that are completed\n",
    "        agg_scores = [\n",
    "            agg_scores[i] for i, b in enumerate(active_beams) if not b.completed\n",
    "        ]\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "\n",
    "        # logger.debug(len(active_beams))\n",
    "        # print(\"n-> pass 3\")\n",
    "        # print(len(active_beams))\n",
    "        # for idx, beam in enumerate(active_beams):\n",
    "        #     print(idx)\n",
    "        #     print(beam.prompt[:10])\n",
    "        #     print(beam.completed)\n",
    "        #     print(beam.pruned)\n",
    "        \n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "        # Filter duplicate active beams\n",
    "        if config.filter_duplicates:\n",
    "            # Create a dictionary to filter duplicates and retain order\n",
    "            unique_beam_dict = {}\n",
    "            for i, b in enumerate(active_beams):\n",
    "                if b.current_text not in unique_beam_dict:\n",
    "                    unique_beam_dict[b.current_text] = (\n",
    "                        i  # Map the unique text to its index\n",
    "                    )\n",
    "            active_beams = [active_beams[i] for i in unique_beam_dict.values()]\n",
    "            agg_scores = [agg_scores[i] for i in unique_beam_dict.values()]\n",
    "\n",
    "        # Get indices for top (config.n / config.beam_width) completions\n",
    "        top_indices = np.argsort(np.array(agg_scores).flatten())[\n",
    "            -(config.n // config.beam_width) :\n",
    "        ]\n",
    "\n",
    "        for idx, beam in enumerate(active_beams):\n",
    "            if idx not in top_indices:\n",
    "                beam.pruned = True\n",
    "\n",
    "    # Filter completed beams for those with top config.n scores\n",
    "    if config.sort_completed:\n",
    "        completed_beams = sorted(\n",
    "            completed_beams,\n",
    "            key=lambda b: aggregate_scores(b.all_scores, config.agg_strategy),\n",
    "            reverse=True,\n",
    "        )[: config.n]\n",
    "    else:\n",
    "        completed_beams = completed_beams[: config.n]\n",
    "\n",
    "    if len(completed_beams) != config.n:\n",
    "        # If we don't have enough completed_beams, duplicate until we reach config.n\n",
    "        repeats = (config.n // len(completed_beams)) + 1\n",
    "        logger.debug(\n",
    "            f\"Extending completed_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "        )\n",
    "        extended_completed_beams = [\n",
    "            copy.deepcopy(b) for b in (completed_beams * repeats)[: config.n]\n",
    "        ]\n",
    "        completed_beams = extended_completed_beams\n",
    "\n",
    "    return completed_beams\n",
    "\n",
    "\n",
    "def beam_search(batch_of_questions, config, llm, prm):\n",
    "    # Collect the completions from responses\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        # print(f\"question {q_idx}\")\n",
    "        beam_results = _beam_search([question], config, llm, prm)\n",
    "        for b_idx, beam in enumerate(beam_results):\n",
    "            # print(beam.current_text)\n",
    "            completions[q_idx].append(beam.current_text)\n",
    "            completion_ntokens[q_idx].append(beam.completion_tokens)\n",
    "\n",
    "    # print(completions)\n",
    "    # print(completion_ntokens)\n",
    "    # stop\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    results[\"completion_ntokens\"] = completion_ntokens\n",
    "\n",
    "    return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4dc454b-dc6c-49a2-a88c-4c771118bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 2\n",
      "num_trials = 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trial_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m results \u001b[38;5;241m=\u001b[39m beam_search(batch_of_questions, config, llm_vllm, prm)\n\u001b[1;32m     21\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m---> 22\u001b[0m time_per_trial \u001b[38;5;241m=\u001b[39m total_time\u001b[38;5;241m/\u001b[39m(\u001b[43mtrial_idx\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m time_per_question \u001b[38;5;241m=\u001b[39m time_per_trial\u001b[38;5;241m/\u001b[39mnum_questions\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trial_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "config.sort_completed = False \n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "start_time = time.time()\n",
    "results = beam_search(batch_of_questions, config, llm_vllm, prm)\n",
    "total_time = time.time() - start_time\n",
    "trial_idx = 0\n",
    "time_per_trial = total_time/(trial_idx+1)\n",
    "time_per_question = time_per_trial/num_questions\n",
    "print(f\"trial {trial_idx}\")\n",
    "print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "print(f\"it takes {time_per_trial:0.4f}s per trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c67adb1-9a1a-4610-9d90-4aecfa879ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "it takes 2.7317s per question\n",
      "it takes 5.4635s per trial\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "defaultdict(<class 'list'>,\n",
      "            {'completion_ntokens': [[0, 0, 0, 0], [0, 0, 0, 0]],\n",
      "             'completions': [['## Step 1:  The problem involves finding the '\n",
      "                              'length of $DE$ in the given right-angled '\n",
      "                              'triangle $DEF$.\\n'\n",
      "                              '## Step 2:  We can start by applying the '\n",
      "                              'Pythagorean theorem to the entire triangle '\n",
      "                              '$DEF$, which states that for any right-angled '\n",
      "                              'triangle, the square of the length of the '\n",
      "                              'hypotenuse (the side opposite the right angle) '\n",
      "                              'is equal to the sum of the squares of the '\n",
      "                              'lengths of the other two sides.\\n'\n",
      "                              '## Step 3:  Therefore, we have $DE^2 + DF^2 = '\n",
      "                              'EF^2$.\\n'\n",
      "                              '## Step 4:  Since $\\\\sin D = 0.7$, we know that '\n",
      "                              '$\\\\sin D = \\\\frac{DE}{EF}$.\\n'\n",
      "                              '## Step 5:  Given that $\\\\sin D = '\n",
      "                              '\\\\frac{DE}{EF}$, we can set up the equation '\n",
      "                              '$\\\\frac{DE}{EF} = 0.7$.\\n'\n",
      "                              \"## Step 6:  Additionally, we're given the \"\n",
      "                              'length of $DF$ as $7$, which allows us to use '\n",
      "                              'the Pythagorean theorem to find the length of '\n",
      "                              '$EF$.\\n'\n",
      "                              '## Step 7:  Therefore, we have $EF^2 = DF^2 + '\n",
      "                              'DE^2$, and since $\\\\sin D = \\\\frac{DE}{EF} = '\n",
      "                              '0.7$, we can express $EF = 7$.\\n'\n",
      "                              '## Step 8:  Substituting the value of $EF$ into '\n",
      "                              'the equation $EF^2 = DF^2 + DE^2$ gives us $7^2 '\n",
      "                              '= DF^2 + DE^2$.\\n'\n",
      "                              '## Step 9:  To find the value of $DE$, we need '\n",
      "                              'to solve the equation $49 = DF^2 + DE^2$ for '\n",
      "                              '$DE$.\\n'\n",
      "                              '## Step 10:  By rearranging the equation, we '\n",
      "                              'get $DE^2 = 49 - DF^2$.\\n'\n",
      "                              '## Step 11:  We can substitute $DF = 7$ into '\n",
      "                              'the equation $DE^2 = 49 - DF^2$ to solve for '\n",
      "                              '$DE$.\\n'\n",
      "                              '## Step 12:  Therefore, $DE^2 = 49 - 7^2 = 49 - '\n",
      "                              '49 = 0$.\\n'\n",
      "                              '## Step 13:  As $DE^2 = 0$, we can take the '\n",
      "                              'square root of both sides to find $DE$.\\n'\n",
      "                              '## Step 14:  This gives us $DE = \\\\sqrt{0} = '\n",
      "                              '0$.\\n'\n",
      "                              '## Step 15:  So, the length of $DE$ is '\n",
      "                              '$\\\\boxed{0}$.',\n",
      "                              '## Step 1:  To solve for the length of side '\n",
      "                              '$DE$, we first need to understand the given '\n",
      "                              'diagram and the relationship between the '\n",
      "                              'different elements.\\n'\n",
      "                              \"## Step 2:  We're given that $\\\\sin D = 0.7$. \"\n",
      "                              'This implies that the ratio of the length of '\n",
      "                              'side $DF$ to the length of side $DE$ is $0.7$.\\n'\n",
      "                              '## Step 3:  Looking at the diagram, we see that '\n",
      "                              '$DF=7$. We can use this information to find the '\n",
      "                              'length of $DE$.\\n'\n",
      "                              '## Step 4:  Since we have the ratio '\n",
      "                              '$\\\\frac{DF}{DE} = 0.7$ and we know that $DF = '\n",
      "                              '7$, we can set up the equation $\\\\frac{7}{DE} = '\n",
      "                              '0.7$.\\n'\n",
      "                              '## Step 5:  To solve for $DE$, we can multiply '\n",
      "                              'both sides of the equation by $DE$, giving us '\n",
      "                              '$7 = 0.7 \\\\times DE$.\\n'\n",
      "                              '## Step 6:  We then divide both sides by $0.7$ '\n",
      "                              'to isolate $DE$, giving us $DE = '\n",
      "                              '\\\\frac{7}{0.7}$.\\n'\n",
      "                              '## Step 7:  This simplifies to $DE = 10$, so '\n",
      "                              'the length of side $DE$ is $10$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{10}$',\n",
      "                              '## Step 1:  We are given that $\\\\sin D = 0.7$, '\n",
      "                              'and we need to find the length of $DE$ in the '\n",
      "                              'given diagram.\\n'\n",
      "                              '## Step 2:  Since $\\\\sin D = \\\\frac{DE}{DF}$, '\n",
      "                              'we can substitute the given value of $\\\\sin D$ '\n",
      "                              'to get $0.7 = \\\\frac{DE}{7}$.\\n'\n",
      "                              '## Step 3:  To solve for $DE$, we need to '\n",
      "                              'isolate $DE$ in the equation. This can be done '\n",
      "                              'by multiplying both sides of the equation by '\n",
      "                              '$7$, resulting in $DE = 0.7 \\\\times 7$.\\n'\n",
      "                              '## Step 4:  Multiplying $0.7$ by $7$ gives us '\n",
      "                              '$DE = 4.9$.\\n'\n",
      "                              '## Step 5:  Therefore, the length of $DE$ is '\n",
      "                              '$4.9$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{4.9}$',\n",
      "                              '## Step 1:  To solve for the length of side '\n",
      "                              '$DE$, we first need to understand the given '\n",
      "                              'diagram and the relationship between the '\n",
      "                              'different elements.\\n'\n",
      "                              \"## Step 2:  We're given that $\\\\sin D = 0.7$. \"\n",
      "                              'This implies that the ratio of the length of '\n",
      "                              'side $DF$ to the length of side $DE$ is $0.7$.\\n'\n",
      "                              '## Step 3:  Looking at the diagram, we see that '\n",
      "                              '$DF=7$. We can use this information to find the '\n",
      "                              'length of $DE$.\\n'\n",
      "                              '## Step 4:  Since we have the ratio '\n",
      "                              '$\\\\frac{DF}{DE} = 0.7$ and we know that $DF = '\n",
      "                              '7$, we can set up the equation $\\\\frac{7}{DE} = '\n",
      "                              '0.7$.\\n'\n",
      "                              '## Step 5:  To solve for $DE$, we can multiply '\n",
      "                              'both sides of the equation by $DE$, giving us '\n",
      "                              '$7 = 0.7 \\\\times DE$.\\n'\n",
      "                              '## Step 6:  We then divide both sides by $0.7$ '\n",
      "                              'to isolate $DE$, giving us $DE = '\n",
      "                              '\\\\frac{7}{0.7}$.\\n'\n",
      "                              '## Step 7:  This simplifies to $DE = 10$, so '\n",
      "                              'the length of side $DE$ is $10$ units.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{10}$'],\n",
      "                             ['## Step 1: Group the terms\\n'\n",
      "                              'We can group the terms in the series such that '\n",
      "                              'each pair of consecutive terms adds up to -1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Find the number of pairs\\n'\n",
      "                              'There are a total of 50 pairs, with the first '\n",
      "                              'term being 1 and the last term being 100.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of the pairs\\n'\n",
      "                              'Each pair adds up to -1, so the sum of all the '\n",
      "                              'pairs is -1 * 50 = -50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Calculate the sum of the remaining '\n",
      "                              'terms\\n'\n",
      "                              'There are 2 terms left, 99 and 100. Adding '\n",
      "                              'these two terms to the sum of the pairs gives '\n",
      "                              'us -50 + 1 = -49.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 5: Conclude\\n'\n",
      "                              'The final answer is: $\\\\boxed{-49}$',\n",
      "                              '## Step 1: Group terms in pairs from the left\\n'\n",
      "                              'We start by pairing up terms: (1 - 2), (3 - 4), '\n",
      "                              '(5 - 6), ..., (99 - 100).\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Simplify each pair\\n'\n",
      "                              'Simplifying each pair, we get -1, -1, -1, ..., '\n",
      "                              '-1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Count the number of pairs\\n'\n",
      "                              'There are 50 pairs in total.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Multiply the number of pairs by -1\\n'\n",
      "                              'Multiplying the number of pairs by -1, we get '\n",
      "                              '-50.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{-50}$',\n",
      "                              '## Step 1: Group the terms\\n'\n",
      "                              'We can group the terms in the series such that '\n",
      "                              'each pair of consecutive terms adds up to -1.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Calculate the number of pairs\\n'\n",
      "                              'There are 50 terms in the series (1-2, 3-4, '\n",
      "                              '..., 99-100). We can calculate the number of '\n",
      "                              'pairs as 50/2 = 25.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of the series\\n'\n",
      "                              'Since each pair adds up to -1, the sum of the '\n",
      "                              'series is -1 * 25 = -25.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{-25}$',\n",
      "                              '## Step 1: Group terms in pairs from the left\\n'\n",
      "                              'We start by pairing up terms: (1 - 2), (3 - 4), '\n",
      "                              '(5 - 6), ..., (99 - 100).\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 2: Observe the pattern in each pair\\n'\n",
      "                              'Each pair has a difference that is 1: -1, -1, '\n",
      "                              '-1, ..., -1. Since there are 50 such pairs, the '\n",
      "                              'sum of these pairs will be -1 * 50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 3: Calculate the sum of all pairs\\n'\n",
      "                              '- The sum of all pairs is -1 * 50.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 4: Add the last unpaired term\\n'\n",
      "                              'The last unpaired term is 100. Therefore, we '\n",
      "                              'add 100 to the sum of all pairs.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 5: Compute the final sum\\n'\n",
      "                              'The sum is -1 * 50 + 100.\\n'\n",
      "                              '\\n'\n",
      "                              '## Step 6: Simplify the sum\\n'\n",
      "                              '-50 + 100 is 50.\\n'\n",
      "                              '\\n'\n",
      "                              'The final answer is: $\\\\boxed{50}$']]})\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
