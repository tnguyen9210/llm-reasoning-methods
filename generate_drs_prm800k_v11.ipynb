{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 08:57:37 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import build_conv, generate_k_steps, last\n",
    "from sal.utils.score import aggregate_scores\n",
    "\n",
    "# from core import select_diverse_v31\n",
    "from core.reward_models import RLHFFlow\n",
    "\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 08:57:54 [config.py:823] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 06-18 08:57:54 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 06-18 08:57:54 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 06-18 08:57:54 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 06-18 08:57:56 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-18 08:57:56 [cuda.py:324] Using XFormers backend.\n",
      "INFO 06-18 08:57:57 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-18 08:57:57 [model_runner.py:1171] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b82fa9510294973b32d4a51ab5a2036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 08:57:58 [default_loader.py:272] Loading weights took 1.32 seconds\n",
      "INFO 06-18 08:57:59 [model_runner.py:1203] Model loading took 2.3185 GiB and 1.569181 seconds\n",
      "INFO 06-18 08:58:00 [worker.py:294] Memory profiling takes 0.66 seconds\n",
      "INFO 06-18 08:58:00 [worker.py:294] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 06-18 08:58:00 [worker.py:294] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 06-18 08:58:00 [executor_base.py:113] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 06-18 08:58:00 [executor_base.py:118] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 06-18 08:58:02 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1df6486e90d4c3ca558b930e7304a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 08:58:19 [model_runner.py:1671] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 06-18 08:58:19 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 19.95 seconds\n",
      "#--- memory: 20.959717750549316\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 0)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959717750549316\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82257a1e-d2d6-4ac4-9e2f-22dff8b23faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f540df2dd44348b187d210faac3eb973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prm \u001b[38;5;241m=\u001b[39m \u001b[43mRLHFFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprm_tokenizer_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:29\u001b[0m, in \u001b[0;36mPRM.__init__\u001b[0;34m(self, model_path, device_map, **model_kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_map \u001b[38;5;241m=\u001b[39m device_map\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:53\u001b[0m, in \u001b[0;36mRLHFFlow.load_model_and_tokenizer\u001b[0;34m(self, **model_kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_and_tokenizer\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[PreTrainedModel, PreTrainedTokenizer]:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=gguf_file)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=gguf_file).eval()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 53\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     59\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4565\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4567\u001b[0m     (\n\u001b[1;32m   4568\u001b[0m         model,\n\u001b[1;32m   4569\u001b[0m         missing_keys,\n\u001b[1;32m   4570\u001b[0m         unexpected_keys,\n\u001b[1;32m   4571\u001b[0m         mismatched_keys,\n\u001b[1;32m   4572\u001b[0m         offload_index,\n\u001b[1;32m   4573\u001b[0m         error_msgs,\n\u001b[0;32m-> 4574\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/modeling_utils.py:4991\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hqq_or_quark:\n\u001b[1;32m   4990\u001b[0m     expanded_device_map \u001b[38;5;241m=\u001b[39m expand_device_map(device_map, expected_keys)\n\u001b[0;32m-> 4991\u001b[0m     \u001b[43mcaching_allocator_warmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_device_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4993\u001b[0m error_msgs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   4994\u001b[0m \u001b[38;5;66;03m# Iterate on all the shards to load the weights\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/modeling_utils.py:6051\u001b[0m, in \u001b[0;36mcaching_allocator_warmup\u001b[0;34m(model, expanded_device_map, hf_quantizer)\u001b[0m\n\u001b[1;32m   6049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   6050\u001b[0m     index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\n\u001b[0;32m-> 6051\u001b[0m     device_memory \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6052\u001b[0m     \u001b[38;5;66;03m# Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\u001b[39;00m\n\u001b[1;32m   6053\u001b[0m     \u001b[38;5;66;03m# than that amount might sometimes lead to unnecessary cuda OOM, if the last parameter to be loaded on the device is large,\u001b[39;00m\n\u001b[1;32m   6054\u001b[0m     \u001b[38;5;66;03m# and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6057\u001b[0m     \u001b[38;5;66;03m# Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\u001b[39;00m\n\u001b[1;32m   6058\u001b[0m     \u001b[38;5;66;03m# if using e.g. 90% of device size, while a 140GiB device would allocate too little\u001b[39;00m\n\u001b[1;32m   6059\u001b[0m     byte_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(byte_count, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(device_memory \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)))\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/cuda/memory.py:836\u001b[0m, in \u001b[0;36mmem_get_info\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# optional=True allows `device = torch.device('cuda')` for which device.index is None\u001b[39;00m\n\u001b[1;32m    835\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudaMemGetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d4828-9a59-4431-8054-d1859ec213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    q_idx: int\n",
    "    question: str\n",
    "    templated_conv: str\n",
    "    current_text: str | None\n",
    "    next_texts: list[str] | None\n",
    "    lookahead_texts: list[str] | None\n",
    "    stop_reasons: list[str | None] | None\n",
    "    best_scores: list[float]  # the PRM scores\n",
    "    all_scores: list[list[float]]  # all PRM scores\n",
    "    previous_text: str | None\n",
    "    pruned: False\n",
    "    history: list[str]\n",
    "    completed: bool = False\n",
    "    completion_tokens: int = 0\n",
    "\n",
    "\n",
    "def _diverse_select(K, V, q_embeds, q_lprobs, q_ppl, q_scores, ds_alpha, ds_beta=0):\n",
    "    num_arms = len(q_embeds)\n",
    "    _V = copy.deepcopy(V)\n",
    "    selected_idxes = []\n",
    "    tol = 0.0001\n",
    "    for it in range(K):\n",
    "        _V_inv = np.linalg.inv(_V)\n",
    "        q_diversity = np.einsum('ij,jk,ik->i', q_embeds, _V_inv, q_embeds)\n",
    "        # print(q_scores.shape)\n",
    "        # print(q_diversity.shape)\n",
    "        q_vals = ds_beta*q_scores + ds_alpha*q_diversity\n",
    "        # print(q_vals.shape)\n",
    "        max_val = np.max([val for idx, val in enumerate(q_vals) if idx not in selected_idxes])\n",
    "        # candidate_idxes = np.where(np.abs(q_vals-max_val) < tol)[0]\n",
    "        candidate_idxes = [\n",
    "            arm_idx for arm_idx, arm_val in enumerate(q_vals)\n",
    "            if (np.abs(max_val - arm_val) <= tol) and (arm_idx not in selected_idxes)\n",
    "        ]\n",
    "\n",
    "        best_idx = max(candidate_idxes, key=lambda i: q_lprobs[i])\n",
    "        # print(q_vals)\n",
    "        # print(q_lprobs)\n",
    "        # print(candidate_idxes)\n",
    "        # print(best_idx)\n",
    "        \n",
    "        best_embeds = q_embeds[best_idx]\n",
    "        best_embeds = best_embeds.reshape(-1, 1)\n",
    "        # print(best_embeds.shape)\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(best_embeds, best_embeds.T)\n",
    "\n",
    "        # update selected_idxes\n",
    "        selected_idxes.append(best_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(selected_idxes)\n",
    "\n",
    "    return selected_idxes\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, llm_tokenizer, prm):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    ndim = 2048\n",
    "    V = config.lam*np.eye(ndim)\n",
    "    K = int(config.n / config.beam_width)\n",
    "    \n",
    "    completed_beams: list[Beam] = []\n",
    "    beams: list[Beam] = []\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        for _ in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    q_idx=q_idx,\n",
    "                    question=question,\n",
    "                    templated_conv=\"\",\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    active_beams = beams\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    start_time = time.time()\n",
    "    for it in range(config.num_iterations):\n",
    "        # print(f\"\\n-> {it}\")\n",
    "        # if it != 0:\n",
    "        #     # active_beams = [b for b in active_beams if not b.pruned]\n",
    "        #     extended_beams = []\n",
    "        #     for beam in active_beams:\n",
    "        #         if beam.pruned:\n",
    "        #             continue \n",
    "                    \n",
    "        #         for j in range(config.beam_width):\n",
    "        #             extended_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "        #     active_beams = extended_beams\n",
    "        \n",
    "        # print(len(active_beams))\n",
    "        convs = [\n",
    "            build_conv(b.question, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "\n",
    "        add_generation_prompt = it == 0\n",
    "        continue_final_message = it > 0\n",
    "    \n",
    "        tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "            \n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        # Last iteration, generate to EOS\n",
    "        if it == config.num_iterations - 1:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        lookahead = 0 if it == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm_vllm, sampling_params, 1\n",
    "        )\n",
    "        # total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "        # Collecct gen_results into beams\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += gen_result.next_texts[0]\n",
    "            # beam.history.append(beam.next_texts[0])\n",
    "            beam.templated_prompt = gen_result.prompt\n",
    "            # pprint.pprint(gen_result)\n",
    "            # print(f\"beam.next_texts = {beam.next_texts}\")\n",
    "            # print(f\"beam.stop_reasons = {beam.stop_reasons}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # stop\n",
    "            \n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "        # Filter out comleted beams \n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        # print(len(active_beams))\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            print(\"break\")\n",
    "            break\n",
    "\n",
    "        # Compute prm scores\n",
    "        all_prompts = []\n",
    "        all_completions = []\n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            all_prompts.append(beam.question)\n",
    "            all_completions.append([beam.current_text])\n",
    "\n",
    "        # print(all_completions)\n",
    "        all_scores = prm.score(all_prompts, all_completions)\n",
    "        # print(all_scores)\n",
    "        # for score in all_scores:\n",
    "        #     print(score)\n",
    "        #     for s in score:\n",
    "        #         print(s)\n",
    "        all_agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "            for score in all_scores\n",
    "        ]\n",
    "        # print(all_agg_scores)\n",
    "        \n",
    "        # Extract completion's embeddings and other info\n",
    "        batch_prms = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_embeds = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_beams = [[] for _ in range(len(batch_of_questions))]\n",
    "        \n",
    "        batch_scores = [[] for _ in range(len(batch_of_questions))]\n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            batch_scores[beam.q_idx].append(all_agg_scores[b_idx][0])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "                # print(gen_prompt)\n",
    "                # stop\n",
    "                inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "    \n",
    "                # Get last_token_embeds\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_token_embeds.shape)\n",
    "    \n",
    "                # Compute otuput_log_prob\n",
    "                # Prepare labels: shift input_ids to the right by one\n",
    "                labels = inputs['input_ids'][:, 1:]   \n",
    "                shifted_logits = outputs.logits[:, :-1, :]\n",
    "                loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "                completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "                completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "                # print(sent_ppl)\n",
    "                # print(loss)\n",
    "    \n",
    "                # normalize the embeds\n",
    "                if config.normalize_embeds:\n",
    "                    norm = np.linalg.norm(last_token_embeds)\n",
    "                    last_token_embeds /= norm\n",
    "                    # print(np.linalg.norm(last_token_embeds))\n",
    "    \n",
    "                batch_embeds[beam.q_idx].append(last_token_embeds)\n",
    "                batch_log_probs[beam.q_idx].append(completion_log_prob)\n",
    "                batch_ppl[beam.q_idx].append(completion_ppl)\n",
    "                batch_beams[beam.q_idx].append(beam)\n",
    "    \n",
    "        # pprint.pprint(len(batch_completions_embeds))\n",
    "        # pprint.pprint(len(batch_completions_log_probs))\n",
    "        # pprint.pprint(len(batch_completions_ppl))\n",
    "        # print(len(batch_beams))\n",
    "        # print(len(batch_beams[0]))\n",
    "        # total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "        \n",
    "        # Use _select_diverse to diversify embeddings \n",
    "        for q_idx in range(len(batch_of_questions)):\n",
    "            \n",
    "            if len(batch_beams[q_idx]) <= K:\n",
    "                continue \n",
    "            \n",
    "            selected_idxes = _select_diverse_prm(\n",
    "                K, V, batch_embeds[q_idx], batch_log_probs[q_idx], batch_ppl[q_idx], \n",
    "                batch_scores[q_idx], config.ds_alpha)\n",
    "                \n",
    "            for idx, beam in enumerate(batch_beams[q_idx]):\n",
    "                if idx not in selected_idxes:\n",
    "                    beam.pruned = True \n",
    "\n",
    "        # create next active beams\n",
    "        next_active_beams = []\n",
    "        for beam in active_beams:\n",
    "            if beam.pruned:\n",
    "                continue \n",
    "                \n",
    "            for j in range(config.beam_width):\n",
    "                next_active_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "        active_beams = next_active_beams\n",
    "\n",
    "                        \n",
    "        total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "    stop\n",
    "    # Filter duplicate active beams\n",
    "    if config.filter_duplicates:\n",
    "        # Create a dictionary to filter duplicates and retain order\n",
    "        unique_beam_dict = {}\n",
    "        for i, b in enumerate(completed_beams):\n",
    "            if b.current_text not in unique_beam_dict:\n",
    "                unique_beam_dict[b.current_text] = (\n",
    "                    i  # Map the unique text to its index\n",
    "                )\n",
    "        completed_beams = [completed_beams[i] for i in unique_beam_dict.values()]\n",
    "            \n",
    "    # Collect the completions from beams\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    # completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "    \n",
    "    for beam in completed_beams:\n",
    "        completions[beam.q_idx].append(beam.current_text)\n",
    "        # completion_ntokens[beam.q_idx].append(beam.current_text)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    # results[\"completion_ntokens\"] = completion_ntokens\n",
    "    \n",
    "    return results\n",
    "\n",
    "        \n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 8\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "config.ds_alpha = 1.0\n",
    "\n",
    "level = 4\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "results = select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer, prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(beam_results))\n",
    "# pprint.pprint(results)\n",
    "print(len(results[\"completions\"][2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
