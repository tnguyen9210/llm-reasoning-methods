{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import build_conv, generate_k_steps, last\n",
    "from sal.utils.score import aggregate_scores\n",
    "\n",
    "# from core import select_diverse_v31\n",
    "from core.reward_models import RLHFFlow\n",
    "\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 13:47:27 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 05-11 13:47:27 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 05-11 13:47:34 config.py:549] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-11 13:47:34 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-11 13:47:36 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 05-11 13:47:36 cuda.py:226] Using XFormers backend.\n",
      "INFO 05-11 13:47:36 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bc24dc6d7f45e5894d1345e2eb88a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 13:47:38 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 05-11 13:47:39 worker.py:267] Memory profiling takes 0.49 seconds\n",
      "INFO 05-11 13:47:39 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 05-11 13:47:39 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 05-11 13:47:39 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 05-11 13:47:39 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 05-11 13:47:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 13:47:56 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.13 GiB\n",
      "INFO 05-11 13:47:56 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 0)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82257a1e-d2d6-4ac4-9e2f-22dff8b23faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d4828-9a59-4431-8054-d1859ec213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    q_idx: int\n",
    "    question: str\n",
    "    templated_conv: str\n",
    "    current_text: str | None\n",
    "    next_texts: list[str] | None\n",
    "    lookahead_texts: list[str] | None\n",
    "    stop_reasons: list[str | None] | None\n",
    "    best_scores: list[float]  # the PRM scores\n",
    "    all_scores: list[list[float]]  # all PRM scores\n",
    "    previous_text: str | None\n",
    "    pruned: False\n",
    "    history: list[str]\n",
    "    completed: bool = False\n",
    "    completion_tokens: int = 0\n",
    "\n",
    "\n",
    "def _diverse_select(K, V, q_embeds, q_lprobs, q_ppl, q_scores, ds_alpha, ds_beta=0):\n",
    "    num_arms = len(q_embeds)\n",
    "    _V = copy.deepcopy(V)\n",
    "    selected_idxes = []\n",
    "    tol = 0.0001\n",
    "    for it in range(K):\n",
    "        _V_inv = np.linalg.inv(_V)\n",
    "        q_diversity = np.einsum('ij,jk,ik->i', q_embeds, _V_inv, q_embeds)\n",
    "        # print(q_scores.shape)\n",
    "        # print(q_diversity.shape)\n",
    "        q_vals = ds_beta*q_scores + ds_alpha*q_diversity\n",
    "        # print(q_vals.shape)\n",
    "        max_val = np.max([val for idx, val in enumerate(q_vals) if idx not in selected_idxes])\n",
    "        # candidate_idxes = np.where(np.abs(q_vals-max_val) < tol)[0]\n",
    "        candidate_idxes = [\n",
    "            arm_idx for arm_idx, arm_val in enumerate(q_vals)\n",
    "            if (np.abs(max_val - arm_val) <= tol) and (arm_idx not in selected_idxes)\n",
    "        ]\n",
    "\n",
    "        best_idx = max(candidate_idxes, key=lambda i: q_lprobs[i])\n",
    "        # print(q_vals)\n",
    "        # print(q_lprobs)\n",
    "        # print(candidate_idxes)\n",
    "        # print(best_idx)\n",
    "        \n",
    "        best_embeds = q_embeds[best_idx]\n",
    "        best_embeds = best_embeds.reshape(-1, 1)\n",
    "        # print(best_embeds.shape)\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(best_embeds, best_embeds.T)\n",
    "\n",
    "        # update selected_idxes\n",
    "        selected_idxes.append(best_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(selected_idxes)\n",
    "\n",
    "    return selected_idxes\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, llm_tokenizer, prm):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    ndim = 2048\n",
    "    V = config.lam*np.eye(ndim)\n",
    "    K = int(config.n / config.beam_width)\n",
    "    \n",
    "    completed_beams: list[Beam] = []\n",
    "    beams: list[Beam] = []\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        for _ in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    q_idx=q_idx,\n",
    "                    question=question,\n",
    "                    templated_conv=\"\",\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    active_beams = beams\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    start_time = time.time()\n",
    "    for it in range(config.num_iterations):\n",
    "        # print(f\"\\n-> {it}\")\n",
    "        # if it != 0:\n",
    "        #     # active_beams = [b for b in active_beams if not b.pruned]\n",
    "        #     extended_beams = []\n",
    "        #     for beam in active_beams:\n",
    "        #         if beam.pruned:\n",
    "        #             continue \n",
    "                    \n",
    "        #         for j in range(config.beam_width):\n",
    "        #             extended_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "        #     active_beams = extended_beams\n",
    "        \n",
    "        # print(len(active_beams))\n",
    "        convs = [\n",
    "            build_conv(b.question, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "\n",
    "        add_generation_prompt = it == 0\n",
    "        continue_final_message = it > 0\n",
    "    \n",
    "        tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "            \n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        # Last iteration, generate to EOS\n",
    "        if it == config.num_iterations - 1:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        lookahead = 0 if it == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm_vllm, sampling_params, 1\n",
    "        )\n",
    "        # total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "        # Collecct gen_results into beams\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += gen_result.next_texts[0]\n",
    "            # beam.history.append(beam.next_texts[0])\n",
    "            beam.templated_prompt = gen_result.prompt\n",
    "            # pprint.pprint(gen_result)\n",
    "            # print(f\"beam.next_texts = {beam.next_texts}\")\n",
    "            # print(f\"beam.stop_reasons = {beam.stop_reasons}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # stop\n",
    "            \n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "        # Filter out comleted beams \n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        # print(len(active_beams))\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            print(\"break\")\n",
    "            break\n",
    "\n",
    "        # Compute prm scores\n",
    "        all_prompts = []\n",
    "        all_completions = []\n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            all_prompts.append(beam.question)\n",
    "            all_completions.append([beam.current_text])\n",
    "\n",
    "        # print(all_completions)\n",
    "        all_scores = prm.score(all_prompts, all_completions)\n",
    "        # print(all_scores)\n",
    "        # for score in all_scores:\n",
    "        #     print(score)\n",
    "        #     for s in score:\n",
    "        #         print(s)\n",
    "        all_agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "            for score in all_scores\n",
    "        ]\n",
    "        # print(all_agg_scores)\n",
    "        \n",
    "        # Extract completion's embeddings and other info\n",
    "        batch_prms = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_embeds = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_beams = [[] for _ in range(len(batch_of_questions))]\n",
    "        \n",
    "        batch_scores = [[] for _ in range(len(batch_of_questions))]\n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            batch_scores[beam.q_idx].append(all_agg_scores[b_idx][0])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "                # print(gen_prompt)\n",
    "                # stop\n",
    "                inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "    \n",
    "                # Get last_token_embeds\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_token_embeds.shape)\n",
    "    \n",
    "                # Compute otuput_log_prob\n",
    "                # Prepare labels: shift input_ids to the right by one\n",
    "                labels = inputs['input_ids'][:, 1:]   \n",
    "                shifted_logits = outputs.logits[:, :-1, :]\n",
    "                loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "                completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "                completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "                # print(sent_ppl)\n",
    "                # print(loss)\n",
    "    \n",
    "                # normalize the embeds\n",
    "                if config.normalize_embeds:\n",
    "                    norm = np.linalg.norm(last_token_embeds)\n",
    "                    last_token_embeds /= norm\n",
    "                    # print(np.linalg.norm(last_token_embeds))\n",
    "    \n",
    "                batch_embeds[beam.q_idx].append(last_token_embeds)\n",
    "                batch_log_probs[beam.q_idx].append(completion_log_prob)\n",
    "                batch_ppl[beam.q_idx].append(completion_ppl)\n",
    "                batch_beams[beam.q_idx].append(beam)\n",
    "    \n",
    "        # pprint.pprint(len(batch_completions_embeds))\n",
    "        # pprint.pprint(len(batch_completions_log_probs))\n",
    "        # pprint.pprint(len(batch_completions_ppl))\n",
    "        # print(len(batch_beams))\n",
    "        # print(len(batch_beams[0]))\n",
    "        # total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "        \n",
    "        # Use _select_diverse to diversify embeddings \n",
    "        for q_idx in range(len(batch_of_questions)):\n",
    "            \n",
    "            if len(batch_beams[q_idx]) <= K:\n",
    "                continue \n",
    "            \n",
    "            selected_idxes = _select_diverse_prm(\n",
    "                K, V, batch_embeds[q_idx], batch_log_probs[q_idx], batch_ppl[q_idx], \n",
    "                batch_scores[q_idx], config.ds_alpha)\n",
    "                \n",
    "            for idx, beam in enumerate(batch_beams[q_idx]):\n",
    "                if idx not in selected_idxes:\n",
    "                    beam.pruned = True \n",
    "\n",
    "        # create next active beams\n",
    "        next_active_beams = []\n",
    "        for beam in active_beams:\n",
    "            if beam.pruned:\n",
    "                continue \n",
    "                \n",
    "            for j in range(config.beam_width):\n",
    "                next_active_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "        active_beams = next_active_beams\n",
    "\n",
    "                        \n",
    "        total_time = time.time() - start_time\n",
    "        # print(f\"it takes {total_time:0.4f}s\")\n",
    "\n",
    "    stop\n",
    "    # Filter duplicate active beams\n",
    "    if config.filter_duplicates:\n",
    "        # Create a dictionary to filter duplicates and retain order\n",
    "        unique_beam_dict = {}\n",
    "        for i, b in enumerate(completed_beams):\n",
    "            if b.current_text not in unique_beam_dict:\n",
    "                unique_beam_dict[b.current_text] = (\n",
    "                    i  # Map the unique text to its index\n",
    "                )\n",
    "        completed_beams = [completed_beams[i] for i in unique_beam_dict.values()]\n",
    "            \n",
    "    # Collect the completions from beams\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    # completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "    \n",
    "    for beam in completed_beams:\n",
    "        completions[beam.q_idx].append(beam.current_text)\n",
    "        # completion_ntokens[beam.q_idx].append(beam.current_text)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    # results[\"completion_ntokens\"] = completion_ntokens\n",
    "    \n",
    "    return results\n",
    "\n",
    "        \n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 8\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "config.ds_alpha = 1.0\n",
    "\n",
    "level = 4\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "results = select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer, prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(beam_results))\n",
    "# pprint.pprint(results)\n",
    "print(len(results[\"completions\"][2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
