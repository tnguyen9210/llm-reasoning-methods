{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02cf7ef-318d-4d71-a357-78376eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462227b4-2a94-479c-9c1d-0908c42db9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:3\")\n",
    "llm_tf.eval()\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187d2099-2e10-4ac4-9122-0184444b734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Explain quantum computing in simple terms.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b2aa25-839d-4998-bd2d-808e2aa2633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: Hello, how are you?\n",
      "Output 1: Hello, how are you? I'm excited to be here today to talk about one of the most amazing things I've experienced in\n",
      "\n",
      "Input 2: What is your name?\n",
      "Output 2: What is your name? I am a teacher at a school in a rural area. I have been teaching for many years and\n",
      "\n",
      "Input 3: Tell me a joke.\n",
      "Output 3: Tell me a joke. Why did the chicken cross the road?\n",
      "To get to the other side...of the joke!\n",
      "\n",
      "Input 4: Explain quantum computing in simple terms.\n",
      "Output 4: Explain quantum computing in simple terms. Imagine you have a magic box that can do calculations faster than any computer.\n",
      "\n",
      "## Step 1:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 1: no batching \n",
    "\n",
    "torch.manual_seed(100000+0)\n",
    "torch.cuda.manual_seed(100000+0)\n",
    "\n",
    "outputs_no_padding = []\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(llm_tf.device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = llm_tf.generate(**inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "    outputs_no_padding.append(tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n",
    "\n",
    "# Display results\n",
    "for i, (input_text, output_text) in enumerate(zip(texts, outputs_no_padding)):\n",
    "    print(f\"Input {i+1}: {input_text}\")\n",
    "    print(f\"Output {i+1}: {output_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed88aa8-bf7e-4893-82a6-1e31c89d2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: Hello, how are you?\n",
      "Output 1: Hello, how are you? I'm excited to be here today to talk about one of the most amazing things I've experienced in\n",
      "\n",
      "Input 2: What is your name?\n",
      "Output 2: What is your name? I am a friend of a friend, and I am not a professional in any field, but I\n",
      "\n",
      "Input 3: Tell me a joke.\n",
      "Output 3: Tell me a joke. Why did the chicken cross the playground?\n",
      "I don't know, why?\n",
      "\n",
      "Input 4: Explain quantum computing in simple terms.\n",
      "Output 4: Explain quantum computing in simple terms. Quantum computing is a type of computer that uses the principles of quantum mechanics to perform calculations. It's\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 2: batching with padding left\n",
    "\n",
    "torch.manual_seed(100000+0)\n",
    "torch.cuda.manual_seed(100000+0)\n",
    "\n",
    "llm_tf.eval()\n",
    "\n",
    "# Tokenize all texts as a batch\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(llm_tf.device)\n",
    "\n",
    "# Generate outputs for the entire batch\n",
    "with torch.no_grad():\n",
    "    output_tokens = llm_tf.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id  # prevent warning for models without pad_token\n",
    "    )\n",
    "\n",
    "output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Ensure output size matches input size\n",
    "assert len(output_texts) == len(texts)\n",
    "\n",
    "# Display results\n",
    "for i, (input_text, output_text) in enumerate(zip(texts, output_texts)):\n",
    "    print(f\"Input {i+1}: {input_text}\")\n",
    "    print(f\"Output {i+1}: {output_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b615392b-f2af-4f09-bcb9-29bcfef878a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: Hello, how are you?\n",
      "Output 1: Hello, how are you?assistant\n",
      "\n",
      "Hello! I'm just a language model, I don't have feelings or\n",
      "\n",
      "Input 2: What is your name?\n",
      "Output 2: What is your name???\n",
      "By the way, I am trying to get a better understanding of what constitutes a \"good\n",
      "\n",
      "Input 3: Tell me a joke.\n",
      "Output 3: Tell me a joke.!\n",
      "Here's one: A man walked into a library and asked the librarian, \"Do you have\n",
      "\n",
      "Input 4: Explain quantum computing in simple terms.\n",
      "Output 4: Explain quantum computing in simple terms. Quantum computing is a type of computer that uses the principles of quantum mechanics to perform calculations. It's\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 3: batching with padding left\n",
    "\n",
    "torch.manual_seed(100000+0)\n",
    "torch.cuda.manual_seed(100000+0)\n",
    "\n",
    "llm_tf.eval()\n",
    "\n",
    "# Tokenize all texts as a batch\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(llm_tf.device)\n",
    "\n",
    "# Generate outputs for the entire batch\n",
    "with torch.no_grad():\n",
    "    output_tokens = llm_tf.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        pad_token_id=tokenizer.eos_token_id  # prevent warning for models without pad_token\n",
    "    )\n",
    "\n",
    "output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Ensure output size matches input size\n",
    "assert len(output_texts) == len(texts)\n",
    "\n",
    "# Display results\n",
    "for i, (input_text, output_text) in enumerate(zip(texts, output_texts)):\n",
    "    print(f\"Input {i+1}: {input_text}\")\n",
    "    print(f\"Output {i+1}: {output_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc398398-c69a-475f-98e5-6492ca061d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "\n",
      "=== Input 1: Hello, how are you?\n",
      "Last Token Embedding (5 dims): tensor([-1.2149,  3.4383,  0.7283,  0.3183,  3.6557], device='cuda:3')\n",
      "Total Log Probability         : -19.602453231811523\n",
      "\n",
      "=== Input 2: What is your name?\n",
      "Last Token Embedding (5 dims): tensor([-1.9130,  2.6492,  1.4096,  0.7731,  1.9185], device='cuda:3')\n",
      "Total Log Probability         : -19.482925415039062\n",
      "\n",
      "=== Input 3: Tell me a joke.\n",
      "Last Token Embedding (5 dims): tensor([-0.3699,  5.0867,  2.6664,  1.7028,  4.2388], device='cuda:3')\n",
      "Total Log Probability         : -19.041711807250977\n",
      "\n",
      "=== Input 4: Explain quantum computing in simple terms.\n",
      "Last Token Embedding (5 dims): tensor([ 3.3191,  2.3091, -0.4360,  0.5728,  2.2908], device='cuda:3')\n",
      "Total Log Probability         : -31.33698844909668\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100000+0)\n",
    "torch.cuda.manual_seed(100000+0)\n",
    "\n",
    "# Tokenize as batch with padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(llm_tf.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Obtain hidden states from the LLM model \n",
    "    outputs = llm_tf(\n",
    "        **inputs,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    # Obtain hidden states and logits\n",
    "    # The hidden states are structured with a shape \n",
    "    # (num_layers, batch_size, seq_len, embed_dim)\n",
    "    hidden_states = outputs.hidden_states[-1] # the last hidden layer\n",
    "    \n",
    "    # The logits structured with a shape \n",
    "    # (batch_size, seq_len, embed_dim)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Prepare logits and labels\n",
    "# Shift logits and labels for LM scoring (predict token i based on 0:i-1)\n",
    "shifted_labels = inputs['input_ids'][:, 1:]   \n",
    "shifted_logits = outputs.logits[:, :-1, :]\n",
    "# loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "# completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "# completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "\n",
    "# Compute log_probs\n",
    "log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "selected_log_probs = log_probs.gather(2, shifted_labels.unsqueeze(-1)).squeeze(-1)\n",
    "print(selected_log_probs.shape)\n",
    "\n",
    "# Mask padding tokens\n",
    "attention_mask = inputs['attention_mask'][:, 1:]\n",
    "selected_log_probs = selected_log_probs * attention_mask\n",
    "\n",
    "total_log_probs = selected_log_probs.sum(dim=1)\n",
    "\n",
    "# Extract last token embedding (prompt only)\n",
    "# Find the last non-padding token index in each sequence\n",
    "last_token_indices = inputs['attention_mask'].sum(dim=1) - 1\n",
    "last_token_embeddings = hidden_states[torch.arange(hidden_states.size(0)), last_token_indices]\n",
    "\n",
    "# Display results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\n=== Input {i+1}: {text}\")\n",
    "    print(f\"Last Token Embedding (5 dims): {last_token_embeddings[i][:5]}\")\n",
    "    print(f\"Total Log Probability         : {total_log_probs[i].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122c1d87-f69a-4f60-9e9e-9f8427bac258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Input: Hello, how are you?\n",
      "Last Token Embedding (5 dims): tensor([-1.2149,  3.4383,  0.7283,  0.3183,  3.6557], device='cuda:3')\n",
      "Total Log Probability         : -19.602453231811523\n",
      "\n",
      "=== Input: What is your name?\n",
      "Last Token Embedding (5 dims): tensor([-1.9130,  2.6492,  1.4096,  0.7731,  1.9185], device='cuda:3')\n",
      "Total Log Probability         : -19.48291778564453\n",
      "\n",
      "=== Input: Tell me a joke.\n",
      "Last Token Embedding (5 dims): tensor([-0.3699,  5.0867,  2.6664,  1.7029,  4.2388], device='cuda:3')\n",
      "Total Log Probability         : -19.04171371459961\n",
      "\n",
      "=== Input: Explain quantum computing in simple terms.\n",
      "Last Token Embedding (5 dims): tensor([ 3.3191,  2.3091, -0.4360,  0.5728,  2.2908], device='cuda:3')\n",
      "Total Log Probability         : -31.336971282958984\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100000+0)\n",
    "torch.cuda.manual_seed(100000+0)\n",
    "\n",
    "outputs_no_padding = []\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(llm_tf.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        logits = outputs.logits  # (1, seq_len, vocab_size)\n",
    "        hidden_states = outputs.hidden_states[-1]  # (1, seq_len, hidden_size)\n",
    "\n",
    "    # Shift logits and labels\n",
    "    shifted_logits = logits[:, :-1, :]\n",
    "    shifted_labels = inputs['input_ids'][:, 1:]\n",
    "\n",
    "    # Compute log-probabilities\n",
    "    log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "    selected_log_probs = log_probs.gather(2, shifted_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Sum log-probs (all tokens except the first, since LM can't predict first token)\n",
    "    total_log_prob = selected_log_probs.sum()\n",
    "\n",
    "    # Compute log probabilities using CrossEntropyLoss \n",
    "    # loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "    # total_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
    "\n",
    "    # Extract last token embedding (prompt only)\n",
    "    last_token_index = inputs['input_ids'].shape[1] - 1\n",
    "    last_token_embedding = hidden_states[0, -1]\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n=== Input: {text}\")\n",
    "    print(f\"Last Token Embedding (5 dims): {last_token_embedding[:5]}\")\n",
    "    print(f\"Total Log Probability         : {total_log_prob.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d49d8309-0253-4d6b-be87-bc7358147dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_by_levels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      5\u001b[0m level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_by_levels\u001b[49m[level])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# num_questions = 2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m num_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_by_levels' is not defined"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 128\n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "method_number = 2\n",
    "if method_number == 1:\n",
    "    test_method = test_best_of_n_v11\n",
    "else:\n",
    "    test_method = test_best_of_n_v12\n",
    "print(test_method)\n",
    "\n",
    "batch_of_prompts = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "start_time = time.time()\n",
    "for t_idx in range(num_trials):\n",
    "    print(f\"trial {t_idx}\")\n",
    "    # test_method(batch_of_prompts, config, llm_vllm, random_seeds[t_idx])\n",
    "    test_method(batch_of_prompts, config, llm_vllm, 10000+t_idx)\n",
    "\n",
    "    # compute the time\n",
    "    total_time = time.time() - start_time\n",
    "    time_per_trial = total_time/(t_idx+1)\n",
    "    time_per_question = time_per_trial/num_questions\n",
    "    print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "    print(f\"it takes {time_per_trial:0.4f}s for this trial\")\n",
    "\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd6731-cfe6-4101-b5bc-7a6261735dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
