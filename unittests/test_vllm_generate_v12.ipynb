{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02cf7ef-318d-4d71-a357-78376eee49a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 00:31:44 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f44ed4-9242-47a6-8edc-f497382b8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNODE\tSYS\tSYS\t1-24\t0\t\tN/A\n",
      "GPU1\tNODE\t X \tSYS\tSYS\t1-24\t0\t\tN/A\n",
      "GPU2\tSYS\tSYS\t X \tNODE\t49-72\t1\t\tN/A\n",
      "GPU3\tSYS\tSYS\tNODE\t X \t49-72\t1\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7439bb1-d6e0-4d0a-8325-c00ee821f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"VLLM_USE_V1\"] = \"0\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,3\"\n",
    "# !export NCCL_P2P_LEVEL=NVL\n",
    "# os.environ['NCCL_P2P_LEVEL'] = 'NVL'\n",
    "# os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "# %env NCCL_P2P_DISABLE=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea33de5-14f2-49fd-987c-bd46295f7af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('NCCL_P2P_DISABLE'))  # Should print '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de10e2e-4a2d-405e-ba95-e5f9e9e72791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 503.68 GB\n",
      "Available RAM: 475.77 GB\n",
      "Used RAM: 14.14 GB\n",
      "RAM Usage Percentage: 5.5%\n",
      "['0', '1', '2', '3']\n",
      "\n",
      "-> gpu 0\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 1\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 2\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 3\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# cpu_percent = psutil.cpu_percent(interval=1)\n",
    "# print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# RAM usage\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {virtual_memory.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available RAM: {virtual_memory.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used RAM: {virtual_memory.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"RAM Usage Percentage: {virtual_memory.percent}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "    for gpu_index in GPUS:\n",
    "        print(f\"\\n-> gpu {gpu_index}\")\n",
    "        gpu_index = int(gpu_index)\n",
    "        # gpu_index = 0  # Change this if you have multiple GPUs\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(gpu_index)\n",
    "        allocated_memory = torch.cuda.memory_allocated(gpu_index)\n",
    "        free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "        print(f\"Total GPU Memory: {total_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Allocated GPU Memory: {allocated_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Available GPU Memory: {free_memory / 1024 ** 3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-23 00:31:48 [config.py:2599] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-23 00:31:57 [config.py:583] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 03-23 00:31:57 [arg_utils.py:1765] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-23 00:31:57 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-23 00:31:59 [cuda.py:234] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-23 00:31:59 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-23 00:31:59 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-23 00:31:59 [model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1246f8176a4fa1a5492182b503d719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 00:32:01 [loader.py:429] Loading weights took 1.36 seconds\n",
      "INFO 03-23 00:32:01 [model_runner.py:1146] Model loading took 2.3185 GB and 1.459759 seconds\n",
      "INFO 03-23 00:32:02 [worker.py:267] Memory profiling takes 0.59 seconds\n",
      "INFO 03-23 00:32:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.50) = 15.87GiB\n",
      "INFO 03-23 00:32:02 [worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 12.25GiB.\n",
      "INFO 03-23 00:32:02 [executor_base.py:111] # cuda blocks: 25088, # CPU blocks: 8192\n",
      "INFO 03-23 00:32:02 [executor_base.py:116] Maximum concurrency for 10000 tokens per request: 40.14x\n",
      "INFO 03-23 00:32:04 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 00:32:18 [model_runner.py:1570] Graph capturing finished in 14 secs, took 0.13 GiB\n",
      "INFO 03-23 00:32:18 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 16.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.584884643554688\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization = 0.5,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 10000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "# # use the gguf quantized model \n",
    "# llm_regular = LLM(\n",
    "#     model = llm_path,\n",
    "#     tokenizer = llm_tokenizer_path,\n",
    "#     tensor_parallel_size=1,\n",
    "#     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "#     max_model_len = 5000,\n",
    "#     dtype = \"float16\",\n",
    "#     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2865202-f09b-461f-b94d-40d7566174ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.584884643554688\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff66373-39b6-48be-82cc-353eb69dc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "data_by_levels = defaultdict(list)\n",
    "with open(f\"{dataset_path}/test.jsonl\", 'r', encoding='utf-8') as filein:\n",
    "    for line in filein:\n",
    "        if line.strip():\n",
    "            data = json.loads(line)\n",
    "            # print(data['level'])\n",
    "            data_by_levels[f\"{data['level']}\"].append(data)\n",
    "\n",
    "    # data =  [json.loads(line) for line in filein if line.strip()]\n",
    "    # pprint.pprint(data, compact=True)\n",
    "\n",
    "for key in range(1,6):\n",
    "    key = str(key)\n",
    "    print(f\"{key}: {len(data_by_levels[key])}\")\n",
    "    # pprint.pprint(data_by_levels[key][:2], compact=True)\n",
    "# print(data_by_levels.keys())\n",
    "# pprint.pprint(data_by_levels['2'], compact=True)\n",
    "\n",
    "random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06adc753-b349-4c62-80f6-5b5212aca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_best_of_n_v11(batch_of_prompts, config, llm_vllm, random_seed):\n",
    "    convs = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        for prompt in batch_of_prompts\n",
    "    ]\n",
    "    \n",
    "    tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "    # TODO: set the augmented template from a file\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs, add_generation_prompt=True, tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Duplicate convs to generate config.n completions per prompt so we can do continous batching\n",
    "    # This makes [p1, p2, p3, p4] become [p1, p1, p2, p2, p3, p3, p4, p4] for e.g. config.n=2\n",
    "    # templated_convs = [c for conv in templated_convs for c in [conv] * config.n]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        # temperature=0,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=config.n,  # generate n outputs\n",
    "        best_of=config.n,\n",
    "        # stop=[\n",
    "        #     \"\\n\\n\"\n",
    "        # ],  # we consider that a step in the problem is indicated by a double newline\n",
    "        # include_stop_str_in_output=True,\n",
    "        seed=random_seed,\n",
    "    )        \n",
    "\n",
    "    # Generate responses \n",
    "    responses = llm_vllm.generate(\n",
    "        templated_convs,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Re-generate responses if we get more responses than expected\n",
    "    if len(responses) != len(batch_of_prompts):\n",
    "        responses = llm_vllm.generate(\n",
    "            templated_convs,\n",
    "            sampling_params=sampling_params,\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "        assert len(responses) == len(batch_of_prompts), \\\n",
    "            f\"Generated {len(responses)} responses instead of {len(batch_of_prompts)}\"\n",
    "    \n",
    "    # Collect the completions from responses\n",
    "    completions = [[] for _ in range(len(batch_of_prompts))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_prompts))]\n",
    "\n",
    "    # for r_idx, r in enumerate(responses):\n",
    "    #     # print(r.request_id)\n",
    "    #     if len(r.outputs) != config.n:\n",
    "    #         raise ValueError(f\"Generated {len(r.outputs)} completions instead of {config.n}\")\n",
    "            \n",
    "    #     for output in r.outputs[:config.n]:\n",
    "    #         print(output.text)\n",
    "    #         # print(output.stop_reason)\n",
    "    #         completions[r_idx].append(output.text)\n",
    "    #         completion_ntokens[r_idx].append(len(output.token_ids))\n",
    "\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe7be59-5e59-4eb8-b346-55c58a46c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_of_n_v12(batch_of_prompts, config, llm_vllm, random_seed):\n",
    "    convs = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        for prompt in batch_of_prompts\n",
    "    ]\n",
    "    \n",
    "    tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "    # TODO: set the augmented template from a file\n",
    "    if config.custom_chat_template is not None:\n",
    "        tokenizer.chat_template = config.custom_chat_template\n",
    "        \n",
    "    templated_convs = tokenizer.apply_chat_template(\n",
    "        convs, add_generation_prompt=True, tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Duplicate convs to generate config.n completions per prompt so we can do continous batching\n",
    "    # This makes [p1, p2, p3, p4] become [p1, p1, p2, p2, p3, p3, p4, p4] for e.g. config.n=2\n",
    "    templated_convs = [c for conv in templated_convs for c in [conv] * config.n]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        # temperature=0,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        n=1,  # generate n outputs\n",
    "        # stop=[\n",
    "        #     \"\\n\\n\"\n",
    "        # ],  # we consider that a step in the problem is indicated by a double newline\n",
    "        # include_stop_str_in_output=True,\n",
    "        seed=random_seed,\n",
    "    )        \n",
    "\n",
    "    # Generate responses \n",
    "    responses = llm_vllm.generate(\n",
    "        templated_convs,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Re-generate responses if we get more responses than expected\n",
    "    if len(responses) != len(batch_of_prompts) * config.n:\n",
    "        responses = llm_vllm.generate(\n",
    "            templated_convs,\n",
    "            sampling_params=sampling_params,\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "        assert len(responses) == len(batch_of_prompts) * config.n, \\\n",
    "            f\"Generated {len(responses)} responses instead of {len(batch_of_prompts)}\"\n",
    "    \n",
    "    # Collect the completions from responses\n",
    "    completions = [[] for _ in range(len(batch_of_prompts))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_prompts))]\n",
    "\n",
    "    # for i in range(len(completions)):\n",
    "    #     completions[i] = [\n",
    "    #         output.text\n",
    "    #         for r in responses[i * config.n : (i + 1) * config.n]\n",
    "    #         for output in r.outputs\n",
    "    #     ]\n",
    "    #     completion_ntokens[i] = [\n",
    "    #         len(output.token_ids)\n",
    "    #         for r in responses[i * config.n : (i + 1) * config.n]\n",
    "    #         for output in r.outputs\n",
    "    #     ]\n",
    "    # print(responses)\n",
    "    \n",
    "    # for r_idx, r in enumerate(responses):\n",
    "    #     idx = r_idx // config.n\n",
    "    #     output = r.outputs[0]\n",
    "    #     # print(output.text)\n",
    "    #     completions[idx].append(output.text)\n",
    "    #     completion_ntokens[idx].append(output.token_ids)\n",
    "\n",
    "    # print(completions)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49d8309-0253-4d6b-be87-bc7358147dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 43\n",
      "<function test_best_of_n_v12 at 0x7f6d1649ff60>\n",
      "trial 0\n",
      "it takes 7.4377s per question\n",
      "it takes 319.8194s for this trial\n",
      "it takes 319.8194s in total\n"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 128\n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "method_number = 2\n",
    "if method_number == 1:\n",
    "    test_method = test_best_of_n_v11\n",
    "else:\n",
    "    test_method = test_best_of_n_v12\n",
    "print(test_method)\n",
    "\n",
    "batch_of_prompts = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "start_time = time.time()\n",
    "for t_idx in range(num_trials):\n",
    "    print(f\"trial {t_idx}\")\n",
    "    # test_method(batch_of_prompts, config, llm_vllm, random_seeds[t_idx])\n",
    "    test_method(batch_of_prompts, config, llm_vllm, 10000+t_idx)\n",
    "\n",
    "    # compute the time\n",
    "    total_time = time.time() - start_time\n",
    "    time_per_trial = total_time/(t_idx+1)\n",
    "    time_per_question = time_per_trial/num_questions\n",
    "    print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "    print(f\"it takes {time_per_trial:0.4f}s for this trial\")\n",
    "\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd6731-cfe6-4101-b5bc-7a6261735dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
