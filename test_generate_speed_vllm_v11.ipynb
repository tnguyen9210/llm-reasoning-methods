{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "import time \n",
    "import gc\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f69cfa-9e42-4826-9d07-a4899986e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 0.0%\n",
      "Total RAM: 503.68 GB\n",
      "Available RAM: 473.91 GB\n",
      "Used RAM: 15.95 GB\n",
      "RAM Usage Percentage: 5.9%\n",
      "['0', '1', '2', '3']\n",
      "\n",
      "-> gpu 0\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 1.44 GB\n",
      "Available GPU Memory: 0.02 GB\n",
      "\n",
      "-> gpu 1\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 1.59 GB\n",
      "Available GPU Memory: 0.02 GB\n",
      "\n",
      "-> gpu 2\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 1.59 GB\n",
      "Available GPU Memory: 0.02 GB\n",
      "\n",
      "-> gpu 3\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# cpu_percent = psutil.cpu_percent(interval=1)\n",
    "# print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# # RAM usage\n",
    "# virtual_memory = psutil.virtual_memory()\n",
    "# print(f\"Total RAM: {virtual_memory.total / (1024 ** 3):.2f} GB\")\n",
    "# print(f\"Available RAM: {virtual_memory.available / (1024 ** 3):.2f} GB\")\n",
    "# print(f\"Used RAM: {virtual_memory.used / (1024 ** 3):.2f} GB\")\n",
    "# print(f\"RAM Usage Percentage: {virtual_memory.percent}%\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "#     print(GPUS)\n",
    "#     for gpu_index in  GPUS:\n",
    "#         print(f\"\\n-> gpu {gpu_index}\")\n",
    "#         gpu_index = int(gpu_index)\n",
    "#         # gpu_index = 0  # Change this if you have multiple GPUs\n",
    "#         total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "#         reserved_memory = torch.cuda.memory_reserved(gpu_index)\n",
    "#         allocated_memory = torch.cuda.memory_allocated(gpu_index)\n",
    "#         free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "#         print(f\"Total GPU Memory: {total_memory / 1024 ** 3:.2f} GB\")\n",
    "#         print(f\"Allocated GPU Memory: {allocated_memory / 1024 ** 3:.2f} GB\")\n",
    "#         print(f\"Available GPU Memory: {free_memory / 1024 ** 3:.2f} GB\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79ac76ea-afff-419b-bce5-d00d3171f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'Below is the graph of $y = a \\\\sin (bx + c) + d$ for some positive constants $a,$ $b,$ $c,$ and $d.$  Find the smallest possible value of $c.$\\n\\n[asy]import TrigMacros;\\n\\nsize(400);\\n\\nreal f(real x)\\n{\\n\\treturn 2*sin(3*x + pi) + 1;\\n}\\n\\ndraw(graph(f,-3*pi,3*pi,n=700,join=operator ..),red);\\ntrig_axes(-3*pi,3*pi,-4,4,pi/2,1);\\nlayer();\\nrm_trig_labels(-5,5, 2);\\n\\nlabel(\"$1$\", (0,1), E);\\nlabel(\"$2$\", (0,2), E);\\nlabel(\"$3$\", (0,3), E);\\nlabel(\"$-1$\", (0,-1), E);\\nlabel(\"$-2$\", (0,-2), E);\\nlabel(\"$-3$\", (0,-3), E);\\n[/asy]' \n",
    "prompt =  'If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.'\n",
    "max_new_tokens = 1024 \n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e455f81a-c8ec-4c6e-83ca-1ba34f6bd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference(model, tokenizer, prompt, max_new_tokens, num_runs, use_vllm=False):\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        if use_vllm:\n",
    "            # vLLM generates text directly from the prompt\n",
    "            sampling_params = SamplingParams(temperature=0.8, max_tokens=max_new_tokens, top_p=1.0, n=1, seed=123)\n",
    "            output = model.generate(prompt, sampling_params, use_tqdm=False)\n",
    "            generated_text = output[0].outputs[0].text\n",
    "            generated_ids = output[0].outputs[0].token_ids\n",
    "            num_tokens = len(generated_ids)\n",
    "        else:\n",
    "            # Transformers requires tokenization\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            num_tokens = len(tokenizer.encode(generated_text))\n",
    "            \n",
    "        end_time = time.time()\n",
    "        total_time += (end_time - start_time)\n",
    "        # total_tokens += len(tokenizer.encode(generated_text)) if not use_vllm else max_new_tokens\n",
    "        total_tokens += num_tokens\n",
    "        \n",
    "    latency = total_time / num_runs\n",
    "    throughput = total_tokens / total_time\n",
    "    avg_tokens = total_tokens / num_runs\n",
    "    return latency, throughput, avg_tokens, generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58c7e34-cc55-4937-bdce-7a2e13ee7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39e0fc69-6fe1-4acc-95c4-f6b35ca11f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 1.439605712890625\n",
      "#--- memory: 1.5939788818359375\n",
      "#--- memory: 1.593986988067627\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_path)\n",
    "\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_tokenizer_path, device_map='cuda:0')\n",
    "\n",
    "model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_path, device_map='cuda:0')\n",
    "\n",
    "# quantized_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, \n",
    "#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_tokenizer_path, quantization_config=quantized_config, device_map='cuda:0')\n",
    "\n",
    "# model_id = \"QuantFactory/Llama-3.2-1B-Instruct-GGUF\"\n",
    "# filename = \"Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "# model_regular = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id, gguf_file=filename, device_map='cuda:0')\n",
    "\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066db4d7-061d-4c71-ad81-312ba5d58467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model_regular.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "935cd525-3b63-46b4-96a4-cf750b42d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers (Regular) - Latency: 10.1992s, Throughput: 55.33 tokens/s\n"
     ]
    }
   ],
   "source": [
    "latency_regular, throughput_regular, avg_tokens, generated_text = measure_inference(model_regular, tokenizer, prompt, max_new_tokens, num_runs)\n",
    "print(f\"Transformers (Regular) - Latency: {latency_regular:.4f}s, Throughput: {throughput_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355aaf7-440b-43cc-a0f8-dcbef6928a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c9b1b4-26c1-445b-937a-3d438a96a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0079345703125\n"
     ]
    }
   ],
   "source": [
    "del(tokenizer)\n",
    "del(model_regular)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-20 17:29:34 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-20 17:29:34 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-20 17:29:34 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [03:56<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 17:29:35 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b86696f5e84813afe1ee33c4aeedff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 17:29:38 model_runner.py:1115] Loading model weights took 2.3029 GB\n",
      "INFO 03-20 17:29:39 worker.py:267] Memory profiling takes 0.83 seconds\n",
      "INFO 03-20 17:29:39 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.20) = 6.35GiB\n",
      "INFO 03-20 17:29:39 worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 2.86GiB.\n",
      "INFO 03-20 17:29:40 executor_base.py:111] # cuda blocks: 5859, # CPU blocks: 8192\n",
      "INFO 03-20 17:29:40 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 18.75x\n",
      "INFO 03-20 17:29:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:25<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 17:30:06 model_runner.py:1562] Graph capturing finished in 26 secs, took 0.13 GiB\n",
      "INFO 03-20 17:30:06 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.209694862365723\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_regular = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "# use the gguf quantized model \n",
    "llm_regular = LLM(\n",
    "    model = llm_path,\n",
    "    tokenizer = llm_tokenizer_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12833482-d304-4da8-9caf-42da552b3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Regular) - Latency: 2.9329s, Throughput: 214.46 tokens/s\n"
     ]
    }
   ],
   "source": [
    "latency_vllm_regular, throughput_vllm_regular, avg_tokens, generated_text = measure_inference(llm_regular, None, prompt, max_new_tokens, num_runs, use_vllm=True)\n",
    "print(f\"vLLM (Regular) - Latency: {latency_vllm_regular:.4f}s, Throughput: {throughput_vllm_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "411741be-9225-4d44-a1bc-7028d66f219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0\n",
      " \n",
      "\n",
      "## Step 1: Plug in the value of x in the function f(x) to find f(-2)\n",
      "To find the value of $f(-2)$, we plug in $x = -2$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-2) = \\frac{3(-2)-2}{-2-2} = \\frac{-6-2}{-4} = \\frac{-8}{-4} = 2$.\n",
      "\n",
      "## Step 2: Plug in the value of x in the function f(x) to find f(-1)\n",
      "To find the value of $f(-1)$, we plug in $x = -1$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-1) = \\frac{3(-1)-2}{-1-2} = \\frac{-3-2}{-3} = \\frac{-5}{-3} = \\frac{5}{3}$.\n",
      "\n",
      "## Step 3: Plug in the value of x in the function f(x) to find f(0)\n",
      "To find the value of $f(0)$, we plug in $x = 0$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(0) = \\frac{3(0)-2}{0-2} = \\frac{-2}{-2} = 1$.\n",
      "\n",
      "## Step 4: Add up the values of f(-2), f(-1), and f(0)\n",
      "Now that we have found the values of $f(-2)$, $f(-1)$, and $f(0)$, which are 2, $\\frac{5}{3}$, and 1 respectively, we can add them up. $f(-2) + f(-1) + f(0) = 2 + \\frac{5}{3} + 1$.\n",
      "\n",
      "## Step 5: Calculate the sum\n",
      "To find the sum, we can first find a common denominator. Since 2 and 1 can both be written as $\\frac{6}{6}$, we can rewrite the sum as $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6}$. Now we can add the fractions together. $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6} = \\frac{6}{6} + \\frac{10}{6} + \\frac{6}{6} = \\frac{22}{6}$. Finally, we can simplify the fraction by dividing both the numerator and denominator by their greatest common divisor, which is 2. This gives us $\\frac{22}{6} = \\frac{11}{3}$.\n",
      "\n",
      "The final answer is: $\\boxed{\\frac{11}{3}}$\n"
     ]
    }
   ],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eff66373-39b6-48be-82cc-353eb69dc8fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_regular' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(\u001b[43mllm_regular\u001b[49m)\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect();torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache();\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#--- memory:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_regular' is not defined"
     ]
    }
   ],
   "source": [
    "del(llm_regular)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00291de5-4c0f-42aa-986f-d666e082f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-17 17:42:41 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-17 17:42:41 config.py:549] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-17 17:42:41 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-17 17:42:42 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb93b0e7086d4be699d57fccddc2a12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 17:42:44 model_runner.py:1115] Loading model weights took 2.3029 GB\n",
      "INFO 03-17 17:42:44 worker.py:267] Memory profiling takes 0.38 seconds\n",
      "INFO 03-17 17:42:44 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 03-17 17:42:44 worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 18.73GiB.\n",
      "INFO 03-17 17:42:44 executor_base.py:111] # cuda blocks: 38353, # CPU blocks: 8192\n",
      "INFO 03-17 17:42:44 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.73x\n",
      "INFO 03-17 17:42:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 17:43:00 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.04 GiB\n",
      "INFO 03-17 17:43:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 16.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# increase gpu_memory_utilization=0.5\n",
    "llm_regular = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f47c83-57e6-40bc-8a2c-0ac681272ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it, est. speed input: 16.64 toks/s, output: 213.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.72 toks/s, output: 214.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.72 toks/s, output: 214.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.71 toks/s, output: 214.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 16.71 toks/s, output: 214.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Regular) - Latency: 2.9366s, Throughput: 214.19 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latency_vllm_regular, throughput_vllm_regular, avg_tokens, generated_text = measure_inference(llm_regular, None, prompt, max_new_tokens, num_runs, use_vllm=True)\n",
    "print(f\"vLLM (Regular) - Latency: {latency_vllm_regular:.4f}s, Throughput: {throughput_vllm_regular:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85e7222e-3c18-4df9-9295-47f196458f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0\n",
      " \n",
      "\n",
      "## Step 1: Plug in the value of x in the function f(x) to find f(-2)\n",
      "To find the value of $f(-2)$, we plug in $x = -2$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-2) = \\frac{3(-2)-2}{-2-2} = \\frac{-6-2}{-4} = \\frac{-8}{-4} = 2$.\n",
      "\n",
      "## Step 2: Plug in the value of x in the function f(x) to find f(-1)\n",
      "To find the value of $f(-1)$, we plug in $x = -1$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(-1) = \\frac{3(-1)-2}{-1-2} = \\frac{-3-2}{-3} = \\frac{-5}{-3} = \\frac{5}{3}$.\n",
      "\n",
      "## Step 3: Plug in the value of x in the function f(x) to find f(0)\n",
      "To find the value of $f(0)$, we plug in $x = 0$ into the function $f(x) = \\frac{3x-2}{x-2}$. This gives us $f(0) = \\frac{3(0)-2}{0-2} = \\frac{-2}{-2} = 1$.\n",
      "\n",
      "## Step 4: Add up the values of f(-2), f(-1), and f(0)\n",
      "Now that we have found the values of $f(-2)$, $f(-1)$, and $f(0)$, which are 2, $\\frac{5}{3}$, and 1 respectively, we can add them up. $f(-2) + f(-1) + f(0) = 2 + \\frac{5}{3} + 1$.\n",
      "\n",
      "## Step 5: Calculate the sum\n",
      "To find the sum, we can first find a common denominator. Since 2 and 1 can both be written as $\\frac{6}{6}$, we can rewrite the sum as $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6}$. Now we can add the fractions together. $\\frac{6}{6} + \\frac{5}{3} + \\frac{6}{6} = \\frac{6}{6} + \\frac{10}{6} + \\frac{6}{6} = \\frac{22}{6}$. Finally, we can simplify the fraction by dividing both the numerator and denominator by their greatest common divisor, which is 2. This gives us $\\frac{22}{6} = \\frac{11}{3}$.\n",
      "\n",
      "The final answer is: $\\boxed{\\frac{11}{3}}$\n"
     ]
    }
   ],
   "source": [
    "print(avg_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06adc753-b349-4c62-80f6-5b5212aca561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it, est. speed input: 16.52 toks/s, output: 212.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, max_tokens=max_new_tokens, top_p=1.0, n=1)\n",
    "output = llm_regular.generate(prompt, sampling_params)\n",
    "generated_text = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5463dfae-4c58-462f-8e3e-21a5e17c2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "print(len(output[0].outputs[0].token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
