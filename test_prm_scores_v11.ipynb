{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f10c28-3f2d-47c0-ace3-b7a5d1463ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cae36f-3789-45cf-9533-1842d3c71779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import aggregate_scores\n",
    "\n",
    "# from core.reward_models import RLHFFlow, SkyworkO1\n",
    "from core import reward_models\n",
    "\n",
    "from core import best_of_n\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd9a9bc-0205-4b75-8d7a-d1027487be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24dfa33-ee28-4654-b0e1-24a1c4eff608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b42d59a-2eb5-4be7-812a-e4705522df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs = ['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "if torch.cuda.is_available():\n",
    "    GPUs = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(f\"GPUs = {GPUs}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d660ac45-b760-43eb-90fd-8b199803aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'core.reward_models' from '/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(reward_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c733bc0-30b0-439c-a2b4-a6288b620083",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm_dir = base_dir + \"/Skywork-o1-Open-PRM-Qwen-2.5-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd04a282-5d08-4831-affb-35bf31080db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u20/tnguyen9210/tnn1/LLMs/search-and-learn/src/sal/models/skywork_o1_prm/modeling_base.py:309: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.7842240333557129\n",
      "#--- memory: 0.8716773986816406\n",
      "#--- memory: 0.8716773986816406\n",
      "#--- memory: 0.34867429733276367\n"
     ]
    }
   ],
   "source": [
    "prm = reward_models.SkyworkO1(model_path=prm_dir, device_map=\"auto\")\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fb8188-2064-444e-9e37-985486d3a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.7842240333557129\n",
      "#--- memory: 0.8716773986816406\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a129354-c8ce-4c23-bef0-6684bb9d73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:0')\n",
    "\n",
    "# gc.collect();torch.cuda.empty_cache();\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac147209-6f17-4d7d-a94d-bdaeccbba42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.7842240333557129\n",
      "#--- memory: 0.8716773986816406\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a4e1bf-b56b-4874-98cf-0ed225567369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "\n",
    "# ds_completions = load_completions(completions_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97197039-298e-407e-ae80-876cc65dd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 20\n",
      "[997, 308, 392, 826, 719, 400, 411, 411, 418, 498, 459, 382, 347, 450, 547, 348]\n",
      "#--- memory: 0.7842240333557129\n",
      "#--- memory: 0.8716773986816406\n",
      "#--- memory: 0.8716773986816406\n",
      "#--- memory: 0.34867429733276367\n",
      "cuda:0\n",
      "[175, 210, 287, 200, 190, 235, 219, 310, 223, 301, 344, 391, 284, 187, 197, 289]\n",
      "#--- memory: 6.695084571838379\n",
      "#--- memory: 1.0324516296386719\n",
      "#--- memory: 1.0324516296386719\n",
      "#--- memory: 0.41746091842651367\n",
      "cuda:0\n",
      "[722, 489, 1495, 709, 517, 565, 788, 631, 565, 602, 2469, 748, 583, 555, 429, 518]\n",
      "#--- memory: 3.1055707931518555\n",
      "#--- memory: 0.9419517517089844\n",
      "#--- memory: 0.9419517517089844\n",
      "#--- memory: 0.38047361373901367\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.36 GiB. GPU 0 has a total capacity of 31.73 GiB of which 7.79 GiB is free. Including non-PyTorch memory, this process has 23.94 GiB memory in use. Of the allocated memory 17.71 GiB is allocated by PyTorch, and 5.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m trial_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute the scores of completions\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(len(trial_data[\"completions\"]))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(len(batch_of_questions))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(trial_data[\"completions\"][0][0])\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mprm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_of_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m agg_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     37\u001b[0m     [aggregate_scores(s, config\u001b[38;5;241m.\u001b[39magg_strategy) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m score] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(agg_scores)\u001b[39;00m\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:239\u001b[0m, in \u001b[0;36mSkyworkO1.score\u001b[0;34m(self, questions, outputs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 239\u001b[0m     _, _, rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     all_step_scores \u001b[38;5;241m=\u001b[39m derive_step_rewards(\n\u001b[1;32m    245\u001b[0m         rewards\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), reward_flags\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m all_scores\u001b[38;5;241m.\u001b[39mappend(all_step_scores)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/tnn1/LLMs/search-and-learn/src/sal/models/skywork_o1_prm/prm_model.py:179\u001b[0m, in \u001b[0;36mSkyworkPRMModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, return_past_key_values, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# force upcast in fp32 if logits are in half-precision\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lm_logits\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m--> 179\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlm_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_past_key_values:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (lm_logits, loss, value, base_model_output\u001b[38;5;241m.\u001b[39mpast_key_values)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.36 GiB. GPU 0 has a total capacity of 31.73 GiB of which 7.79 GiB is free. Including non-PyTorch memory, this process has 23.94 GiB memory in use. Of the allocated memory 17.71 GiB is allocated by PyTorch, and 5.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 16\n",
    "config.agg_strategy = 'last'\n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 20\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "# load completions \n",
    "completions_dir = f\"results/generate_bon_prm800k_level{level}_n16_v11.jsonl\"\n",
    "scores_dir = f\"../results/scores_bon_prm800k_level{level}_n16_v11.jsonl\"\n",
    "\n",
    "# compute results\n",
    "fout = open(scores_dir, 'w', encoding = 'utf-8')\n",
    "start_time = time.time()\n",
    "with open(scores_dir, 'w', encoding = 'utf-8') as fout:\n",
    "    with open(completions_dir, 'r', encoding = 'utf-8') as fin:\n",
    "        trial_idx = 0\n",
    "        for line in fin:\n",
    "            if trial_idx >= num_trials:\n",
    "                break\n",
    "                \n",
    "            trial_data = json.loads(line)\n",
    "    \n",
    "            # Compute the scores of completions\n",
    "            # print(len(trial_data[\"completions\"]))\n",
    "            # print(len(batch_of_questions))\n",
    "            # print(trial_data[\"completions\"][0][0])\n",
    "            scores = prm.score(batch_of_questions, trial_data[\"completions\"][:num_questions])\n",
    "            agg_scores = [\n",
    "                [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores\n",
    "            ]\n",
    "            # print(agg_scores)\n",
    "            \n",
    "            trial_data[\"agg_scores\"] = agg_scores\n",
    "            # print(trial_data.keys())\n",
    "            json.dump(trial_data, fout)\n",
    "            fout.write('\\n')\n",
    "    \n",
    "            # compute the time\n",
    "            if trial_idx % 1 == 0:\n",
    "                total_time = time.time() - start_time\n",
    "                time_per_trial = total_time/(trial_idx+1)\n",
    "                time_per_question = time_per_trial/num_questions\n",
    "                print(f\"trial {trial_idx}\")\n",
    "                print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "                print(f\"it takes {time_per_trial:0.4f}s per trial\")\n",
    "    \n",
    "            trial_idx += 1\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55bed91e-4be5-46a7-a9c9-7ac91f86030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 17.7143816947937\n",
      "#--- memory: 1.3186912536621094\n",
      "#--- memory: 1.3186912536621094\n",
      "#--- memory: 0.5311694145202637\n"
     ]
    }
   ],
   "source": [
    "# gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
