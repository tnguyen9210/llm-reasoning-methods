{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317acedd-5d55-44b8-aa4b-414a0bdcd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "import pprint\n",
    "import json\n",
    "import os, psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f520a4a1-f7d1-4716-83ba-3ca15bae90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from sal.models.reward_models import RLHFFlow\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import aggregate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a17b14-00cd-43e8-a6b9-7ac08a1f0936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 0.1%\n",
      "Total RAM: 503.68 GB\n",
      "Available RAM: 475.61 GB\n",
      "Used RAM: 14.14 GB\n",
      "RAM Usage Percentage: 5.6%\n",
      "['0', '1', '2', '3']\n",
      "\n",
      "-> gpu 0\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 1\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 2\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n",
      "\n",
      "-> gpu 3\n",
      "Total GPU Memory: 31.73 GB\n",
      "Allocated GPU Memory: 0.00 GB\n",
      "Available GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "cpu_percent = psutil.cpu_percent(interval=1)\n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# RAM usage\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {virtual_memory.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available RAM: {virtual_memory.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used RAM: {virtual_memory.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"RAM Usage Percentage: {virtual_memory.percent}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "    for gpu_index in  GPUS:\n",
    "        print(f\"\\n-> gpu {gpu_index}\")\n",
    "        gpu_index = int(gpu_index)\n",
    "        # gpu_index = 0  # Change this if you have multiple GPUs\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(gpu_index)\n",
    "        allocated_memory = torch.cuda.memory_allocated(gpu_index)\n",
    "        free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "        print(f\"Total GPU Memory: {total_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Allocated GPU Memory: {allocated_memory / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Available GPU Memory: {free_memory / 1024 ** 3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19189287-009f-40cf-a7bf-b81f8ca00b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path\n",
    "base_path = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "dataset_path = base_path + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_path = base_path + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_path = base_path + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_path = base_path + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25799fbb-4167-44dc-82db-b58732be2a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 13:41:38 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-19 13:41:38 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-19 13:41:45 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-19 13:41:45 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-19 13:41:46 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-19 13:41:46 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-19 13:41:47 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b148df2e44f40cf8a6cc139e9ba24cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 13:41:49 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 03-19 13:41:49 worker.py:267] Memory profiling takes 0.50 seconds\n",
      "INFO 03-19 13:41:49 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.20) = 6.35GiB\n",
      "INFO 03-19 13:41:49 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 2.75GiB.\n",
      "INFO 03-19 13:41:50 executor_base.py:111] # cuda blocks: 5631, # CPU blocks: 8192\n",
      "INFO 03-19 13:41:50 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 18.02x\n",
      "INFO 03-19 13:41:51 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 13:42:08 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 03-19 13:42:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.084694862365723\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "vllm_inference = LLM(\n",
    "    model = llm_tokenizer_path,\n",
    "    gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    max_model_len = 5000,\n",
    "    dtype = \"float16\",\n",
    "    seed = 123)\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4aaae0f-57ff-4daa-84d6-810be7684c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 5.084694862365723\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_path)\n",
    "llm_transformer = AutoModelForCausalLM.from_pretrained(llm_tokenizer_path).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6973fa31-0cf8-44dc-8265-f3a2a81d52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\"\n",
    "input_batch = {\"problem\": [question_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81834aab-2a16-4554-a84a-0a9807b868ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12aec3fd9a70400d86bf05c820df0632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prm = RLHFFlow(prm_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a524c1bb-c115-46e8-bad5-8c30009efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 4\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 10\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeddings = True\n",
    "\n",
    "\n",
    "def _embed_diverse_search(batch_of_prompts, config: Config, llm: LLM, llm_tf, llm_tokenizer, prm) -> list[Beam]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    beams: list[Beam] = []\n",
    "    for prompt in batch_of_prompts:\n",
    "        for i in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    prompt=prompt,\n",
    "                    index=i,\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for i in range(config.num_iterations):\n",
    "        if i == 0:\n",
    "            active_beams = [b for b in beams if not b.pruned]\n",
    "        else:\n",
    "            active_beams = [b for b in active_beams if not b.pruned]\n",
    "\n",
    "        # Duplicate active beams to ensure that we have config.n beams per iteration\n",
    "        if len(active_beams) != config.n:\n",
    "            repeats = (config.n // len(active_beams)) + 1\n",
    "            print(\n",
    "                f\"Extending active_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "            )\n",
    "            extended_active_beams = [\n",
    "                copy.deepcopy(b) for b in (active_beams * repeats)[: config.n]\n",
    "            ]\n",
    "            active_beams = extended_active_beams\n",
    "            if len(active_beams) != config.n:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {config.n} active beams, but got {len(active_beams)}\"\n",
    "                )\n",
    "\n",
    "        if i == config.num_iterations - 1:\n",
    "            # Last iteration, generate to EOS\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        convs = [\n",
    "            build_conv(b.prompt, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "        continue_final_message = i > 0\n",
    "        add_generation_prompt = i == 0\n",
    "\n",
    "        tokenizer = llm.get_tokenizer()\n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        lookahead = 0 if i == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        prompts, completions = [], []\n",
    "        next_active_beams = []\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += beam.next_texts[0]\n",
    "            beam.history.append(beam.next_texts[0])\n",
    "\n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "                \n",
    "            prompts.append(beam.prompt)\n",
    "            completions.append([beam.current_text])\n",
    "\n",
    "        scores = prm.score(prompts, completions)\n",
    "\n",
    "        agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "            for score in scores\n",
    "        ]\n",
    "\n",
    "        for beam, score in zip(active_beams, scores, strict=True):\n",
    "            beam.all_scores = score[0]\n",
    "\n",
    "        # Now filter active_beams and agg_scores for beams that are completed\n",
    "        agg_scores = [\n",
    "            agg_scores[i] for i, b in enumerate(active_beams) if not b.completed\n",
    "        ]\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "        # get completion's embeddings \n",
    "        completions_embeds = []\n",
    "        for beam in active_beams:\n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now \n",
    "                inputs = llm_tokenizer(beam.current_text, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                output = llm_tf(**inputs, output_hidden_states=True)\n",
    "                # print(output)\n",
    "                last_hidden_state = output.hidden_states[-1]\n",
    "                last_token_embedding = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_hidden_state.shape)\n",
    "                # print(last_token_embedding)\n",
    "                \n",
    "                # normalize the embeddings\n",
    "                if config.normalize_embeddings:\n",
    "                    norm = np.linalg.norm(last_token_embedding)\n",
    "                    last_token_embedding /= norm\n",
    "                    \n",
    "                completions_embeds.append(last_token_embedding)\n",
    "            \n",
    "        V = config.lam*np.eye(2048)\n",
    "        K = int(config.n / 2)\n",
    "        selected_idxes = _select_diverse(completions_embeds, K, V)\n",
    "        print(len(completions_embeds))\n",
    "        print(selected_idxes)\n",
    "\n",
    "        for idx, beam in enumerate(active_beams):\n",
    "            if idx not in selected_idxes:\n",
    "                beam.pruned = True\n",
    "\n",
    "    # Filter completed beams for those with top config.n scores\n",
    "    if config.sort_completed:\n",
    "        completed_beams = sorted(\n",
    "            completed_beams,\n",
    "            key=lambda b: aggregate_scores(b.all_scores, config.agg_strategy),\n",
    "            reverse=True,\n",
    "        )[: config.n]\n",
    "    else:\n",
    "        completed_beams = completed_beams[: config.n]\n",
    "\n",
    "    if len(completed_beams) != config.n:\n",
    "        # If we don't have enough completed_beams, duplicate until we reach config.n\n",
    "        repeats = (config.n // len(completed_beams)) + 1\n",
    "        print(\n",
    "            f\"Extending completed_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "        )\n",
    "        extended_completed_beams = [\n",
    "            copy.deepcopy(b) for b in (completed_beams * repeats)[: config.n]\n",
    "        ]\n",
    "        completed_beams = extended_completed_beams\n",
    "\n",
    "    return completed_beams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1407eeda-eb5b-4613-8957-1eb507466aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1, 2]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "4\n",
      "[2, 1]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "4\n",
      "[1, 2]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "4\n",
      "[0, 1]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "4\n",
      "[3, 1]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "4\n",
      "[1, 2]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n",
      "2\n",
      "[1, 0]\n",
      "Extending active_beams with 3 repetitions to reach size 4\n"
     ]
    }
   ],
   "source": [
    "def _select_diverse(embeds_list, K, V):\n",
    "    num_arms = len(embeds_list)\n",
    "    _V = copy.deepcopy(V)\n",
    "    # S_embeds = copy.deepcopy(embeds_list)\n",
    "    A_idxes = []\n",
    "    A_embeds = []\n",
    "    \n",
    "    for it in range(K):\n",
    "        max_val = -10\n",
    "        max_idx = None\n",
    "        max_embeds = None\n",
    "        for arm_idx, arm_embed in enumerate(embeds_list):\n",
    "            # print(arm_idx)\n",
    "            # print(arm_embed.shape)\n",
    "            \n",
    "            if arm_idx in A_idxes:\n",
    "                continue \n",
    "\n",
    "            # normalize the embeddings\n",
    "            # norm = np.linalg.norm(arm_embed)\n",
    "            # arm_embed /= norm\n",
    "            \n",
    "            # compute Mahalanobis norm\n",
    "            arm_val = np.matmul(np.matmul(arm_embed, np.linalg.inv(_V)), arm_embed.T)\n",
    "            # print(arm_val)\n",
    "            if arm_val > max_val:\n",
    "                max_val = arm_val\n",
    "                max_idx = arm_idx\n",
    "                max_embed = arm_embed\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(max_embed, max_embed.T)\n",
    "\n",
    "        # update A \n",
    "        A_idxes.append(max_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(A_idxes)\n",
    "    \n",
    "    return A_idxes\n",
    "\n",
    "\n",
    "beam_results = _embed_diverse_search(input_batch['problem'], config, vllm_inference, llm_transformer, tokenizer, prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e554f46f-9a9c-4406-8859-ec1647514168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam(prompt='Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\\\le \\theta < 2 \\\\pi.$', index=2, current_text='## Step 1: Understand the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the formulas $r = \\\\sqrt{x^2 + y^2}$ and $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$.\\n\\n## Step 2: Apply the conversion formulas\\nWe have the point $(0, 3)$ in rectangular coordinates. We will substitute $x = 0$ and $y = 3$ into the formulas.\\n\\n## Step 3: Calculate the value of $r$\\n$r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$.\\n\\n## Step 4: Calculate the value of $\\\\theta$\\n$\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$\\n\\n## Step 5: Evaluate the arctangent\\nSince $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$ is undefined, we must use the fact that $\\\\tan^{-1}(0) = 0$ and the periodicity of the arctangent function. So $\\\\theta = \\\\pi$.\\n\\n## Step 6: Write the polar coordinates\\nTherefore, the polar coordinates of $(0, 3)$ are $\\\\left(3, \\\\pi\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\pi\\\\right)}$', next_texts=['The final answer is: $\\\\boxed{\\\\left(3, \\\\pi\\\\right)}$'], lookahead_texts=['The final answer is: $\\\\boxed{\\\\left(3, \\\\pi\\\\right)}$'], stop_reasons=['EOS'], best_scores=[], all_scores=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], previous_text=None, pruned=False, history=['## Step 1: Understand the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the formulas $r = \\\\sqrt{x^2 + y^2}$ and $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$.\\n\\n', '## Step 2: Apply the conversion formulas\\nWe have the point $(0, 3)$ in rectangular coordinates. We will substitute $x = 0$ and $y = 3$ into the formulas.\\n\\n', '## Step 3: Calculate the value of $r$\\n$r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$.\\n\\n', '## Step 4: Calculate the value of $\\\\theta$\\n$\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$\\n\\n', '## Step 5: Evaluate the arctangent\\nSince $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$ is undefined, we must use the fact that $\\\\tan^{-1}(0) = 0$ and the periodicity of the arctangent function. So $\\\\theta = \\\\pi$.\\n\\n', '## Step 6: Write the polar coordinates\\nTherefore, the polar coordinates of $(0, 3)$ are $\\\\left(3, \\\\pi\\\\right)$.\\n\\n', 'The final answer is: $\\\\boxed{\\\\left(3, \\\\pi\\\\right)}$'], completed=True, completion_tokens=0)\n"
     ]
    }
   ],
   "source": [
    "print(beam_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f1401-dada-4608-8f35-2f199a9c4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(llm_transformer)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
