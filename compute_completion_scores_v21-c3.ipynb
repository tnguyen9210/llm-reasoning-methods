{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f10c28-3f2d-47c0-ace3-b7a5d1463ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cae36f-3789-45cf-9533-1842d3c71779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import score, aggregate_scores\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from core.reward_models import RLHFFlow\n",
    "\n",
    "from core import best_of_n\n",
    "from utils.load_data import load_data_prm800k\n",
    "\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24dfa33-ee28-4654-b0e1-24a1c4eff608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b42d59a-2eb5-4be7-812a-e4705522df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs = ['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "if torch.cuda.is_available():\n",
    "    GPUs = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(f\"GPUs = {GPUs}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a129354-c8ce-4c23-bef0-6684bb9d73e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63563fa4532f4fe092bcba0ff8e52017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:3')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac147209-6f17-4d7d-a94d-bdaeccbba42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a979351-8a3f-43ab-95fe-e6f1b39e4886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.load_data' from '/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/utils/load_data.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a4e1bf-b56b-4874-98cf-0ed225567369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n",
      "Dataset({\n",
      "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "# print(data_by_levels)\n",
    "\n",
    "# print(config.dataset_name)\n",
    "orig_dataset = load_dataset(config.dataset_name, split='test', cache_dir=data_dir)\n",
    "print(orig_dataset)\n",
    "# stop\n",
    "# for data in dataset:\n",
    "#     pprint.pprint(data)\n",
    "#     stop\n",
    "# ds_completions = load_completions(completions_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149abeb0-dd68-4c7b-9a40-811b5991cfc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97197039-298e-407e-ae80-876cc65dd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 128\n",
      "Dataset({\n",
      "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
      "    num_rows: 128\n",
      "})\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.81 GiB. GPU 3 has a total capacity of 31.73 GiB of which 8.41 GiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 15.59 GiB is allocated by PyTorch, and 7.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m completions \u001b[38;5;241m=\u001b[39m [trial_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m][q_idx] \u001b[38;5;28;01mfor\u001b[39;00m q_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_questions)]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# print(f\"{len(completions)}\")\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(f\"len = {len(completions[0])}\")\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# stop\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mprm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_of_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m agg_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     68\u001b[0m     [aggregate_scores(s, config\u001b[38;5;241m.\u001b[39magg_strategy) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m score] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores\n\u001b[1;32m     69\u001b[0m ]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scores))\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:76\u001b[0m, in \u001b[0;36mRLHFFlow.score\u001b[0;34m(self, questions, outputs, batched, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m     questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_single(questions, outputs)\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:162\u001b[0m, in \u001b[0;36mRLHFFlow._score_batched\u001b[0;34m(self, questions, outputs, batch_size)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inputs_batch\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs2_batch\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 162\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidate_tokens]\n\u001b[1;32m    163\u001b[0m     scores \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\n\u001b[1;32m    164\u001b[0m         :, :, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    165\u001b[0m     ]  \u001b[38;5;66;03m# 0 means the prob of + (1 mean -)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(convs_batch)):\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# We slice on the N-1 token since the model is trained to predict the Nth one (\"+\" in this case)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:859\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    858\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 859\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.81 GiB. GPU 3 has a total capacity of 31.73 GiB of which 8.41 GiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 15.59 GiB is allocated by PyTorch, and 7.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 8\n",
    "config.beam_width = 4\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 40\n",
    "config.sort_completed = False\n",
    "config.seed = 0\n",
    "config.version = \"v21\"\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "config.num_proc = 12 \n",
    "\n",
    "dataset_id = \"tnguyen9210/LLM-Reasoning-Math-500\"\n",
    "level = 4\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 2\n",
    "num_trials = 5\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "orig_dataset_by_level = orig_dataset.filter(lambda example: example['level'] == int(level))\n",
    "orig_dataset_by_level = orig_dataset_by_level.select(range(num_questions))\n",
    "print(orig_dataset_by_level)\n",
    "# for data in orig_dataset_by_level:\n",
    "#     print(data)\n",
    "\n",
    "# load completions\n",
    "# config_name = f\"sd--n-{config.n}--bw-{config.beam_width}--depth-{config.num_iterations}--lam-{config.lam}--{config.normalize_embeds}--seed-{config.seed}--level-{level}--{config.version}\"\n",
    "config_name = \"sd--n-16--bw-2--depth-2--lam-10--True--seed-0--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-2--lam-10--True--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-2--lam-10--True--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-10--lam-10--True--level-4--v11\"\n",
    "\n",
    "completions_dir = f\"results/generate_{config_name}.jsonl\"\n",
    "scores_dir = f\"results/scores_{config_name}.jsonl\"\n",
    "start_idx = 1\n",
    "\n",
    "# compute results\n",
    "start_time = time.time()    \n",
    "with open(completions_dir, 'r', encoding = 'utf-8') as fin:\n",
    "    trial_idx = 0\n",
    "    for line in fin:\n",
    "        \n",
    "        if trial_idx >= num_trials:\n",
    "            break\n",
    "\n",
    "        if trial_idx < start_idx:\n",
    "            trial_idx += 1\n",
    "            continue\n",
    "            \n",
    "        trial_data = json.loads(line)\n",
    "        # print(len(trial_data[\"completions\"][0]))\n",
    "        # stop\n",
    "        completions = [trial_data[\"completions\"][q_idx] for q_idx in range(num_questions)]\n",
    "        \n",
    "        # print(f\"{len(completions)}\")\n",
    "        # print(f\"len = {len(completions[0])}\")\n",
    "        # stop\n",
    "\n",
    "        scores = prm.score(batch_of_questions, completions)\n",
    "        agg_scores = [\n",
    "            [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores\n",
    "        ]\n",
    "        print(len(scores))\n",
    "        print(len(scores[0]))\n",
    "        _orig_dataset_by_level = orig_dataset_by_level.add_column(\"completions\", completions)\n",
    "        _orig_dataset_by_level = _orig_dataset_by_level.add_column(\"scores\", scores)\n",
    "        print(_orig_dataset_by_level)\n",
    "\n",
    "        # for data in orig_dataset_by_level:\n",
    "        #     print(data.keys())\n",
    "            # print(data[\"completions\"])\n",
    "                \n",
    "        _orig_dataset_by_level = score(_orig_dataset_by_level, config)\n",
    "        # for data in orig_dataset_by_level:\n",
    "        #     print(data)\n",
    "        #     stop\n",
    "\n",
    "        # _orig_dataset_by_level.push_to_hub(dataset_id, config_name=f\"{config_name}--trial-{trial_idx}\", split='test')\n",
    "        _orig_dataset_by_level.to_json(f\"results/{config_name}--trial-{trial_idx}.jsonl\")\n",
    "        \n",
    "        # compute the time\n",
    "        if trial_idx % 1 == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            time_per_trial = total_time/(trial_idx+1)\n",
    "            time_per_question = time_per_trial/num_questions\n",
    "            print(f\"trial {trial_idx}\")\n",
    "            print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "            print(f\"it takes {time_per_trial:0.4f}s per trial\")\n",
    "\n",
    "        trial_idx += 1\n",
    "        \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224bdaa-2f21-4aaa-8b9d-2c1182bef68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"tnguyen9210/LLM-Reasoning-Math-500\"\n",
    "orig_dataset_by_level.push_to_hub(dataset_id, config_name=config_name, split='test')\n",
    "orig_dataset_by_level.to_json(f\"results/{config_name}.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
