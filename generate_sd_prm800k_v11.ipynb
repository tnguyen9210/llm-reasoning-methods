{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02cf7ef-318d-4d71-a357-78376eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "from sal.config import Config\n",
    "\n",
    "\n",
    "from core import best_of_n\n",
    "from utils.load_data import load_data_prm800k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589fee98-466c-457a-80c1-bf8533ef829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5fed180-f809-4702-b14c-821646e2fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 08:48:58 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-26 08:48:58 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-26 08:49:14 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-26 08:49:14 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-26 08:49:16 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-26 08:49:16 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-26 08:49:17 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79efcb519b634c799c2297dce56c3904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 08:49:19 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 03-26 08:49:20 worker.py:267] Memory profiling takes 0.69 seconds\n",
      "INFO 03-26 08:49:20 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 03-26 08:49:20 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 03-26 08:49:20 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 03-26 08:49:20 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 03-26 08:49:22 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 08:49:36 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.13 GiB\n",
      "INFO 03-26 08:49:36 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9612f06-551f-40b7-8e95-c2efb6ddfb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f033ca0-bd4e-42e2-ba42-de97c799c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "\n",
    "from sal.search.utils import Beam, build_conv, generate_k_steps, last\n",
    "\n",
    "\n",
    "\n",
    "def _select_diverse(embeds_list, K, V):\n",
    "    num_arms = len(embeds_list)\n",
    "    _V = copy.deepcopy(V)\n",
    "    # S_embeds = copy.deepcopy(embeds_list)\n",
    "    A_idxes = []\n",
    "    A_embeds = []\n",
    "\n",
    "    for it in range(K):\n",
    "        max_val = -10\n",
    "        max_idx = None\n",
    "        max_embeds = None\n",
    "        for arm_idx, arm_embed in enumerate(embeds_list):\n",
    "            # print(arm_idx)\n",
    "            # print(arm_embed.shape)\n",
    "\n",
    "            if arm_idx in A_idxes:\n",
    "                continue\n",
    "\n",
    "            # normalize the embeddings\n",
    "            # norm = np.linalg.norm(arm_embed)\n",
    "            # arm_embed /= norm\n",
    "\n",
    "            # compute Mahalanobis norm\n",
    "            arm_val = np.matmul(np.matmul(arm_embed, np.linalg.inv(_V)), arm_embed.T)\n",
    "            # print(arm_val)\n",
    "            if arm_val > max_val:\n",
    "                max_val = arm_val\n",
    "                max_idx = arm_idx\n",
    "                max_embed = arm_embed\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(max_embed, max_embed.T)\n",
    "\n",
    "        # update A\n",
    "        A_idxes.append(max_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(A_idxes)\n",
    "\n",
    "    return A_idxes\n",
    "\n",
    "def _select_diverse_search(batch_of_prompts, config: Config, llm: LLM, llm_tf, llm_tokenizer, prm) -> list[Beam]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    beams: list[Beam] = []\n",
    "    for prompt in batch_of_prompts:\n",
    "        for i in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    prompt=prompt,\n",
    "                    index=i,\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for i in range(config.num_iterations):\n",
    "        if i == 0:\n",
    "            active_beams = [b for b in beams if not b.pruned]\n",
    "        else:\n",
    "            active_beams = [b for b in active_beams if not b.pruned]\n",
    "\n",
    "        # Duplicate active beams to ensure that we have config.n beams per iteration\n",
    "        if len(active_beams) != config.n:\n",
    "            repeats = (config.n // len(active_beams)) + 1\n",
    "            # print(\n",
    "            #     f\"Extending active_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "            # )\n",
    "            extended_active_beams = [\n",
    "                copy.deepcopy(b) for b in (active_beams * repeats)[: config.n]\n",
    "            ]\n",
    "            active_beams = extended_active_beams\n",
    "            if len(active_beams) != config.n:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {config.n} active beams, but got {len(active_beams)}\"\n",
    "                )\n",
    "\n",
    "        if i == config.num_iterations - 1:\n",
    "            # Last iteration, generate to EOS\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        convs = [\n",
    "            build_conv(b.prompt, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "        continue_final_message = i > 0\n",
    "        add_generation_prompt = i == 0\n",
    "\n",
    "        tokenizer = llm.get_tokenizer()\n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        lookahead = 0 if i == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        prompts, completions = [], []\n",
    "        next_active_beams = []\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += beam.next_texts[0]\n",
    "            beam.history.append(beam.next_texts[0])\n",
    "\n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "\n",
    "            prompts.append(beam.prompt)\n",
    "            completions.append([beam.current_text])\n",
    "\n",
    "        # scores = prm.score(prompts, completions)\n",
    "\n",
    "        # agg_scores = [\n",
    "        #     [aggregate_scores(s, config.agg_strategy) for s in score]\n",
    "        #     for score in scores\n",
    "        # ]\n",
    "\n",
    "        # for beam, score in zip(active_beams, scores, strict=True):\n",
    "        #     beam.all_scores = score[0]\n",
    "\n",
    "        # # Now filter active_beams and agg_scores for beams that are completed\n",
    "        # agg_scores = [\n",
    "        #     agg_scores[i] for i, b in enumerate(active_beams) if not b.completed\n",
    "        # ]\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "        # get completion's embeddings\n",
    "        completions_embeds = []\n",
    "        for beam in active_beams:\n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                inputs = llm_tokenizer(beam.current_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "                output = llm_tf(**inputs, output_hidden_states=True)\n",
    "                # print(output)\n",
    "                last_hidden_state = output.hidden_states[-1]\n",
    "                last_token_embedding = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_hidden_state.shape)\n",
    "                # print(last_token_embedding)\n",
    "\n",
    "                # normalize the embeddings\n",
    "                if config.normalize_embeddings:\n",
    "                    norm = np.linalg.norm(last_token_embedding)\n",
    "                    last_token_embedding /= norm\n",
    "\n",
    "                completions_embeds.append(last_token_embedding)\n",
    "\n",
    "        V = config.lam*np.eye(2048)\n",
    "        K = int(config.n / 2)\n",
    "        selected_idxes = _select_diverse(completions_embeds, K, V)\n",
    "        # print(len(completions_embeds))\n",
    "        # print(selected_idxes)\n",
    "\n",
    "        for idx, beam in enumerate(active_beams):\n",
    "            if idx not in selected_idxes:\n",
    "                beam.pruned = True\n",
    "\n",
    "    # Filter completed beams for those with top config.n scores\n",
    "    if config.sort_completed:\n",
    "        completed_beams = sorted(\n",
    "            completed_beams,\n",
    "            key=lambda b: aggregate_scores(b.all_scores, config.agg_strategy),\n",
    "            reverse=True,\n",
    "        )[: config.n]\n",
    "    else:\n",
    "        completed_beams = completed_beams[: config.n]\n",
    "\n",
    "    if len(completed_beams) != config.n:\n",
    "        # If we don't have enough completed_beams, duplicate until we reach config.n\n",
    "        repeats = (config.n // len(completed_beams)) + 1\n",
    "        # print(\n",
    "        #     f\"Extending completed_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "        # )\n",
    "        extended_completed_beams = [\n",
    "            copy.deepcopy(b) for b in (completed_beams * repeats)[: config.n]\n",
    "        ]\n",
    "        completed_beams = extended_completed_beams\n",
    "\n",
    "    return completed_beams\n",
    "\n",
    "def select_diverse_search(batch_of_prompts, config: Config, llm: LLM, llm_tf, llm_tokenizer, prm):\n",
    "    # problems = examples[\"problem\"]\n",
    "    beam_results = _select_diverse_search(batch_of_prompts, config, llm, llm_tf, llm_tokenizer, prm)\n",
    "\n",
    "    # Group together alike beams and store in the dataset\n",
    "    grouped_results = defaultdict(list)\n",
    "    for results in beam_results:\n",
    "        grouped_results[results.prompt].append(results)\n",
    "\n",
    "    results = {\"completions\": [], \"pred\": [], \"completion_tokens\": [], \"best_score\": []}\n",
    "\n",
    "    for p in batch_of_prompts:\n",
    "        beams = grouped_results[p]\n",
    "        completions = [b.current_text for b in beams]\n",
    "        agg_scores = [\n",
    "            aggregate_scores(b.all_scores, config.agg_strategy) for b in beams\n",
    "        ]\n",
    "        best_idx = np.argmax(agg_scores)\n",
    "        # best_pred = completions[best_idx]\n",
    "        results[\"completions\"].append(completions)\n",
    "        # results[\"scores\"].append([b.all_scores for b in beams])\n",
    "        results[\"best_score\"].append(agg_scores[best_idx])\n",
    "        results[\"pred\"].append(completions[best_idx])\n",
    "        # results[\"completion_tokens\"].append([b.completion_tokens for b in beams])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d49d8309-0253-4d6b-be87-bc7358147dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trials = 50\n",
      "num_questions = 43\n",
      "<function best_of_n_v11 at 0x7f4a52bc1440>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(result_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trial_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trials):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# best_of_n(batch_of_questions, config, llm_vllm, random_seeds[trial_idx])\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_of_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_vllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtrial_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, fout)\n\u001b[1;32m     30\u001b[0m         fout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/best_of_n.py:43\u001b[0m, in \u001b[0;36mbest_of_n_v11\u001b[0;34m(batch_of_questions, config, llm_vllm, random_seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     29\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# temperature=0,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[1;32m     40\u001b[0m )        \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate responses \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mllm_vllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemplated_convs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Re-generate responses if we get more responses than expected\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(responses) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_of_questions):\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/utils.py:1066\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1061\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1062\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1063\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m         )\n\u001b[0;32m-> 1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:464\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    454\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    457\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    458\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    462\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1371\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1369\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1371\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1379\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;66;03m# Skip the scheduler if there are any remaining steps in the seq groups.\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;66;03m# This ensures that the scheduler is only called again when the current\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# batch has completed.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;66;03m# The scheduler is also skipped if a single request caused the last\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;66;03m# engine step to fail, and the previous schedule needs to be rerun.\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_remaining_steps(\n\u001b[1;32m   1374\u001b[0m         seq_group_metadata_list\n\u001b[1;32m   1375\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_scheduling_next_step:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# Schedule iteration\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     (seq_group_metadata_list, scheduler_outputs,\n\u001b[1;32m   1378\u001b[0m      allow_async_output_proc\n\u001b[0;32m-> 1379\u001b[0m      ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1381\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mseq_group_metadata_list \u001b[38;5;241m=\u001b[39m seq_group_metadata_list\n\u001b[1;32m   1382\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscheduler_outputs \u001b[38;5;241m=\u001b[39m scheduler_outputs\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/core/scheduler.py:1574\u001b[0m, in \u001b[0;36mScheduler.schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;66;03m# It assumes the scheduled_seq_groups is ordered by\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;66;03m# prefill < decoding.\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_first_prefill \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39msend_delta_data:\n\u001b[0;32m-> 1574\u001b[0m     seq_group_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceGroupMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_block_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_computed_block_nums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_seq_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_seq_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_block_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_block_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will only be present for the 1st comm\u001b[39;49;00m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# between engine and worker.\u001b[39;49;00m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# the subsequent comms can still use delta, but\u001b[39;49;00m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will be None.\u001b[39;49;00m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_data\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prefill_groups\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_placeholders\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prefill_groups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;66;03m# When SPMD mode is enabled, we only send delta data except for\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;66;03m# the first request to reduce serialization cost.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m     seq_data_delta \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/sequence.py:990\u001b[0m, in \u001b[0;36mSequenceGroupMetadata.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m### Stateful fields that are lazily defined. ###\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# The number of speculative tokens adopted in this request.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# None means specuative decoding is not used.\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# Zero means speculative decoding is disabled for some reasons.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# TODO: We should maintain this states out of the sequence group.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m num_speculative_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_chunk_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_prompt:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 20\n",
    "num_trials = 200\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "# select search algo\n",
    "search_name = 'best_of_n'\n",
    "algo_type = 1\n",
    "if search_name == 'best_of_n':\n",
    "    if algo_type == 1:\n",
    "        search_algo = best_of_n.best_of_n_v11\n",
    "    else:\n",
    "        search_algo = best_of_n.best_of_n_v12\n",
    "print(search_algo)\n",
    "\n",
    "# run search_algo and save results\n",
    "result_dir = f\"results/generate_bon_prm800k_level{level}_n{config.n}_v11.jsonl\"\n",
    "start_time = time.time()\n",
    "with open(result_dir, 'w', encoding = 'utf-8') as fout:\n",
    "    for trial_idx in range(num_trials):\n",
    "        # best_of_n(batch_of_questions, config, llm_vllm, random_seeds[trial_idx])\n",
    "        results = search_algo(batch_of_questions, config, llm_vllm, 10000+trial_idx)\n",
    "        json.dump(results, fout)\n",
    "        fout.write('\\n')\n",
    "    \n",
    "        # compute the time\n",
    "        if trial_idx % 1 == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            time_per_trial = total_time/(trial_idx+1)\n",
    "            time_per_question = time_per_trial/num_questions\n",
    "            print(f\"trial {trial_idx}\")\n",
    "            print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "            print(f\"it takes {time_per_trial:0.4f}s per trial\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
