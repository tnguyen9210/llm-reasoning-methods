{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f10c28-3f2d-47c0-ace3-b7a5d1463ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute prm scores for each question instead of multiple questions\n",
    "'''\n",
    "\n",
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cae36f-3789-45cf-9533-1842d3c71779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.models.reward_models import PRM\n",
    "from sal.utils.score import score, aggregate_scores\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from core.reward_models import RLHFFlow\n",
    "\n",
    "from core import best_of_n\n",
    "from utils.load_data import load_data_prm800k\n",
    "\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24dfa33-ee28-4654-b0e1-24a1c4eff608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b42d59a-2eb5-4be7-812a-e4705522df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs = ['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "if torch.cuda.is_available():\n",
    "    GPUs = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(f\"GPUs = {GPUs}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a129354-c8ce-4c23-bef0-6684bb9d73e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8eab1217e4403e91643e42d497ad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:3')\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(2)/(1024**3))\n",
    "# print('#--- memory:', torch.cuda.memory_allocated(3)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac147209-6f17-4d7d-a94d-bdaeccbba42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 0.0\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# del(prm)\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a979351-8a3f-43ab-95fe-e6f1b39e4886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.load_data' from '/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/utils/load_data.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a4e1bf-b56b-4874-98cf-0ed225567369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n",
      "Dataset({\n",
      "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "# print(data_by_levels)\n",
    "\n",
    "# print(config.dataset_name)\n",
    "orig_dataset = load_dataset(config.dataset_name, split='test', cache_dir=data_dir)\n",
    "print(orig_dataset)\n",
    "# stop\n",
    "# for data in dataset:\n",
    "#     pprint.pprint(data)\n",
    "#     stop\n",
    "# ds_completions = load_completions(completions_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149abeb0-dd68-4c7b-9a40-811b5991cfc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97197039-298e-407e-ae80-876cc65dd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 128\n",
      "Dataset({\n",
      "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
      "    num_rows: 128\n",
      "})\n",
      "128\n",
      "len = 16\n",
      "q_idx: 0 - 16\n",
      "q_idx: 1 - 9\n",
      "q_idx: 2 - 21\n",
      "q_idx: 3 - 14\n",
      "q_idx: 4 - 14\n",
      "q_idx: 5 - 11\n",
      "q_idx: 6 - 13\n",
      "q_idx: 7 - 14\n",
      "q_idx: 8 - 27\n",
      "q_idx: 9 - 18\n",
      "q_idx: 10 - 17\n",
      "q_idx: 11 - 12\n",
      "q_idx: 12 - 12\n",
      "q_idx: 13 - 14\n",
      "q_idx: 14 - 15\n",
      "q_idx: 15 - 18\n",
      "q_idx: 16 - 10\n",
      "q_idx: 17 - 14\n",
      "q_idx: 18 - 16\n",
      "q_idx: 19 - 14\n",
      "q_idx: 20 - 12\n",
      "q_idx: 21 - 24\n",
      "q_idx: 22 - 8\n",
      "q_idx: 23 - 11\n",
      "q_idx: 24 - 13\n",
      "q_idx: 25 - 16\n",
      "q_idx: 26 - 19\n",
      "q_idx: 27 - 14\n",
      "q_idx: 28 - 10\n",
      "q_idx: 29 - 11\n",
      "q_idx: 30 - 14\n",
      "q_idx: 31 - 14\n",
      "q_idx: 32 - 27\n",
      "q_idx: 33 - 18\n",
      "q_idx: 34 - 13\n",
      "q_idx: 35 - 22\n",
      "q_idx: 36 - 20\n",
      "q_idx: 37 - 20\n",
      "q_idx: 38 - 12\n",
      "q_idx: 39 - 12\n",
      "q_idx: 40 - 11\n",
      "q_idx: 41 - 17\n",
      "q_idx: 42 - 16\n",
      "q_idx: 43 - 18\n",
      "q_idx: 44 - 10\n",
      "q_idx: 45 - 8\n",
      "q_idx: 46 - 9\n",
      "q_idx: 47 - 12\n",
      "q_idx: 48 - 10\n",
      "q_idx: 49 - 23\n",
      "q_idx: 50 - 22\n",
      "q_idx: 51 - 24\n",
      "q_idx: 52 - 20\n",
      "q_idx: 53 - 14\n",
      "q_idx: 54 - 14\n",
      "q_idx: 55 - 12\n",
      "q_idx: 56 - 16\n",
      "q_idx: 57 - 14\n",
      "q_idx: 58 - 20\n",
      "q_idx: 59 - 8\n",
      "q_idx: 60 - 14\n",
      "q_idx: 61 - 10\n",
      "q_idx: 62 - 11\n",
      "q_idx: 63 - 13\n",
      "q_idx: 64 - 19\n",
      "q_idx: 65 - 16\n",
      "q_idx: 66 - 14\n",
      "q_idx: 67 - 13\n",
      "q_idx: 68 - 20\n",
      "q_idx: 69 - 14\n",
      "q_idx: 70 - 30\n",
      "q_idx: 71 - 17\n",
      "q_idx: 72 - 19\n",
      "q_idx: 73 - 22\n",
      "q_idx: 74 - 14\n",
      "q_idx: 75 - 12\n",
      "q_idx: 76 - 12\n",
      "q_idx: 77 - 16\n",
      "q_idx: 78 - 13\n",
      "q_idx: 79 - 19\n",
      "q_idx: 80 - 18\n",
      "q_idx: 81 - 19\n",
      "q_idx: 82 - 17\n",
      "q_idx: 83 - 14\n",
      "q_idx: 84 - 15\n",
      "q_idx: 85 - 12\n",
      "q_idx: 86 - 14\n",
      "q_idx: 87 - 14\n",
      "q_idx: 88 - 18\n",
      "q_idx: 89 - 12\n",
      "q_idx: 90 - 21\n",
      "q_idx: 91 - 14\n",
      "q_idx: 92 - 10\n",
      "q_idx: 93 - 20\n",
      "q_idx: 94 - 10\n",
      "q_idx: 95 - 12\n",
      "q_idx: 96 - 13\n",
      "q_idx: 97 - 8\n",
      "q_idx: 98 - 24\n",
      "q_idx: 99 - 15\n",
      "q_idx: 100 - 19\n",
      "q_idx: 101 - 26\n",
      "q_idx: 102 - 8\n",
      "q_idx: 103 - 15\n",
      "q_idx: 104 - 13\n",
      "q_idx: 105 - 14\n",
      "q_idx: 106 - 14\n",
      "q_idx: 107 - 26\n",
      "q_idx: 108 - 16\n",
      "q_idx: 109 - 16\n",
      "q_idx: 110 - 8\n",
      "q_idx: 111 - 8\n",
      "q_idx: 112 - 14\n",
      "q_idx: 113 - 12\n",
      "q_idx: 114 - 21\n",
      "q_idx: 115 - 16\n",
      "q_idx: 116 - 9\n",
      "q_idx: 117 - 20\n",
      "q_idx: 118 - 10\n",
      "q_idx: 119 - 18\n",
      "q_idx: 120 - 12\n",
      "q_idx: 121 - 12\n",
      "q_idx: 122 - 8\n",
      "q_idx: 123 - 12\n",
      "q_idx: 124 - 23\n",
      "q_idx: 125 - 24\n",
      "q_idx: 126 - 23\n",
      "q_idx: 127 - 26\n",
      "128\n",
      "16\n",
      "Dataset({\n",
      "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id', 'completions', 'scores'],\n",
      "    num_rows: 128\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c6d1b639494f7ab9db0eefa6048a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a419abef804932b64c25d2643f49ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 4 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69e8b5a5089473e80a20ea4c7195a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 4 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be508b3daff487d86eeed2fad249b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 4 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafcd7236a9c40a291a215080bf40d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 4 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018a9d0aed3749c8bf8fb87b8557a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 4 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:  33%|███▎      | 1/3 [00:04<00:08,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaabe6c7a1442dfa756544db989b75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 8 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf2790002424efd965b64df3ecb77b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 8 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f99cd4cb7a42da9e5d1a872ddc2773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 8 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac79b3170e934d4cbb81b928f757bf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 8 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2078254878524df68a7beedefe63c343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 8 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:  67%|██████▋   | 2/3 [00:07<00:03,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d76289a89140e2b7f6d5cfc11dc029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 16 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1822bbdecefe4af79519fe4424d7f366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 16 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981a9c96b2694afea00b0a5d7faf2ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 16 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08b56e9d8334265a12e069160c62e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 16 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7ec0ef8cc9438a868817e1e24e4ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 16 (num_proc=12):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions: 100%|██████████| 3/3 [00:11<00:00,  3.94s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48947c2c5a0f4772ba619314dc6c65d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "it takes 4.4147s per question\n",
      "it takes 565.0823s per trial\n",
      "128\n",
      "len = 12\n",
      "q_idx: 0 - 12\n",
      "q_idx: 1 - 25\n",
      "q_idx: 2 - 28\n",
      "q_idx: 3 - 24\n",
      "q_idx: 4 - 14\n",
      "q_idx: 5 - 15\n",
      "q_idx: 6 - 17\n",
      "q_idx: 7 - 14\n",
      "q_idx: 8 - 31\n",
      "q_idx: 9 - 16\n",
      "q_idx: 10 - 25\n",
      "q_idx: 11 - 10\n",
      "q_idx: 12 - 17\n",
      "q_idx: 13 - 16\n",
      "q_idx: 14 - 22\n",
      "q_idx: 15 - 14\n",
      "q_idx: 16 - 10\n",
      "q_idx: 17 - 14\n",
      "q_idx: 18 - 22\n",
      "q_idx: 19 - 18\n",
      "q_idx: 20 - 14\n",
      "q_idx: 21 - 12\n",
      "q_idx: 22 - 10\n",
      "q_idx: 23 - 9\n",
      "q_idx: 24 - 14\n",
      "q_idx: 25 - 8\n",
      "q_idx: 26 - 17\n",
      "q_idx: 27 - 16\n",
      "q_idx: 28 - 13\n",
      "q_idx: 29 - 15\n",
      "q_idx: 30 - 14\n",
      "q_idx: 31 - 18\n",
      "q_idx: 32 - 10\n",
      "q_idx: 33 - 17\n",
      "q_idx: 34 - 10\n",
      "q_idx: 35 - 22\n",
      "q_idx: 36 - 17\n",
      "q_idx: 37 - 18\n",
      "q_idx: 38 - 13\n",
      "q_idx: 39 - 20\n",
      "q_idx: 40 - 9\n",
      "q_idx: 41 - 20\n",
      "q_idx: 42 - 15\n",
      "q_idx: 43 - 12\n",
      "q_idx: 44 - 18\n",
      "q_idx: 45 - 19\n",
      "q_idx: 46 - 8\n",
      "q_idx: 47 - 14\n",
      "q_idx: 48 - 10\n",
      "q_idx: 49 - 27\n",
      "q_idx: 50 - 13\n",
      "q_idx: 51 - 19\n",
      "q_idx: 52 - 17\n",
      "q_idx: 53 - 12\n",
      "q_idx: 54 - 13\n",
      "q_idx: 55 - 14\n",
      "q_idx: 56 - 14\n",
      "q_idx: 57 - 11\n",
      "q_idx: 58 - 22\n",
      "q_idx: 59 - 14\n",
      "q_idx: 60 - 14\n",
      "q_idx: 61 - 12\n",
      "q_idx: 62 - 13\n",
      "q_idx: 63 - 10\n",
      "q_idx: 64 - 19\n",
      "q_idx: 65 - 20\n",
      "q_idx: 66 - 14\n",
      "q_idx: 67 - 14\n",
      "q_idx: 68 - 14\n",
      "q_idx: 69 - 30\n",
      "q_idx: 70 - 19\n",
      "q_idx: 71 - 12\n",
      "q_idx: 72 - 22\n",
      "q_idx: 73 - 17\n",
      "q_idx: 74 - 10\n",
      "q_idx: 75 - 19\n",
      "q_idx: 76 - 13\n",
      "q_idx: 77 - 16\n",
      "q_idx: 78 - 19\n",
      "q_idx: 79 - 15\n",
      "q_idx: 80 - 21\n",
      "q_idx: 81 - 11\n",
      "q_idx: 82 - 18\n",
      "q_idx: 83 - 22\n",
      "q_idx: 84 - 19\n",
      "q_idx: 85 - 16\n",
      "q_idx: 86 - 16\n",
      "q_idx: 87 - 14\n",
      "q_idx: 88 - 10\n",
      "q_idx: 89 - 8\n",
      "q_idx: 90 - 23\n",
      "q_idx: 91 - 14\n",
      "q_idx: 92 - 12\n",
      "q_idx: 93 - 23\n",
      "q_idx: 94 - 12\n",
      "q_idx: 95 - 15\n",
      "q_idx: 96 - 10\n",
      "q_idx: 97 - 18\n",
      "q_idx: 98 - 9\n",
      "q_idx: 99 - 17\n",
      "q_idx: 100 - 17\n",
      "q_idx: 101 - 23\n",
      "q_idx: 102 - 13\n",
      "q_idx: 103 - 15\n",
      "q_idx: 104 - 21\n",
      "q_idx: 105 - 17\n",
      "q_idx: 106 - 19\n",
      "q_idx: 107 - 15\n",
      "q_idx: 108 - 15\n",
      "q_idx: 109 - 14\n",
      "q_idx: 110 - 10\n",
      "q_idx: 111 - 14\n",
      "q_idx: 112 - 18\n",
      "q_idx: 113 - 11\n",
      "q_idx: 114 - 17\n",
      "q_idx: 115 - 14\n",
      "q_idx: 116 - 10\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.81 GiB. GPU 3 has a total capacity of 31.73 GiB of which 8.41 GiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 7.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_idx: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(completions[q_idx])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# print([batch_of_questions[q_idx]])\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# print([completions[q_idx]])\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mprm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_of_questions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     batch_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m scores\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# agg_scores = [\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#     [aggregate_scores(s, config.agg_strategy) for s in score] for score in batch_scores\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:76\u001b[0m, in \u001b[0;36mRLHFFlow.score\u001b[0;34m(self, questions, outputs, batched, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m     questions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_single(questions, outputs)\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/reward_models.py:162\u001b[0m, in \u001b[0;36mRLHFFlow._score_batched\u001b[0;34m(self, questions, outputs, batch_size)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inputs_batch\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs2_batch\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 162\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidate_tokens]\n\u001b[1;32m    163\u001b[0m     scores \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\n\u001b[1;32m    164\u001b[0m         :, :, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    165\u001b[0m     ]  \u001b[38;5;66;03m# 0 means the prob of + (1 mean -)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(convs_batch)):\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# We slice on the N-1 token since the model is trained to predict the Nth one (\"+\" in this case)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:859\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    858\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 859\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.81 GiB. GPU 3 has a total capacity of 31.73 GiB of which 8.41 GiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 7.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 16\n",
    "config.beam_width = 4\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 40\n",
    "config.sort_completed = False\n",
    "config.seed = 0\n",
    "config.version = \"v21\"\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "config.num_proc = 12 \n",
    "\n",
    "dataset_id = \"tnguyen9210/LLM-Reasoning-Math-500\"\n",
    "level = 4\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 2\n",
    "num_trials = 5\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "orig_dataset_by_level = orig_dataset.filter(lambda example: example['level'] == int(level))\n",
    "orig_dataset_by_level = orig_dataset_by_level.select(range(num_questions))\n",
    "print(orig_dataset_by_level)\n",
    "# for data in orig_dataset_by_level:\n",
    "#     print(data)\n",
    "\n",
    "# load completions\n",
    "# config_name = f\"sd--n-{config.n}--bw-{config.beam_width}--depth-{config.num_iterations}--lam-{config.lam}--{config.normalize_embeds}--seed-{config.seed}--level-{level}--{config.version}\"\n",
    "config_name = \"sd--n-16--bw-2--depth-2--lam-10--True--seed-0--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-2--lam-10--True--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-2--lam-10--True--level-4--v21\"\n",
    "config_name = \"sd--n-8--bw-2--d-10--lam-10--True--level-4--v11\"\n",
    "\n",
    "completions_dir = f\"results/generate_{config_name}.jsonl\"\n",
    "scores_dir = f\"results/scores_{config_name}.jsonl\"\n",
    "start_idx = 0\n",
    "\n",
    "# compute results\n",
    "start_time = time.time()    \n",
    "with open(completions_dir, 'r', encoding = 'utf-8') as fin:\n",
    "    trial_idx = 0\n",
    "    for line in fin:\n",
    "        \n",
    "        if trial_idx >= num_trials:\n",
    "            break\n",
    "\n",
    "        if trial_idx < start_idx:\n",
    "            trial_idx += 1\n",
    "            continue\n",
    "            \n",
    "        trial_data = json.loads(line)\n",
    "        # print(len(trial_data[\"completions\"][0]))\n",
    "        # stop\n",
    "        completions = [trial_data[\"completions\"][q_idx] for q_idx in range(num_questions)]\n",
    "        \n",
    "        print(f\"{len(completions)}\")\n",
    "        print(f\"len = {len(completions[0])}\")\n",
    "        # print(batch_of_questions)\n",
    "        batch_scores = []\n",
    "        for q_idx in range(len(batch_of_questions)):\n",
    "            print(f\"q_idx: {q_idx} - {len(completions[q_idx])}\")\n",
    "            # print([batch_of_questions[q_idx]])\n",
    "            # print([completions[q_idx]])\n",
    "            scores = prm.score([batch_of_questions[q_idx]], [completions[q_idx]])\n",
    "            batch_scores += scores\n",
    "            \n",
    "        # agg_scores = [\n",
    "        #     [aggregate_scores(s, config.agg_strategy) for s in score] for score in batch_scores\n",
    "        # ]\n",
    "        print(len(batch_scores))\n",
    "        print(len(batch_scores[0]))\n",
    "        \n",
    "        _orig_dataset_by_level = orig_dataset_by_level.add_column(\"completions\", completions)\n",
    "        _orig_dataset_by_level = _orig_dataset_by_level.add_column(\"scores\", batch_scores)\n",
    "        print(_orig_dataset_by_level)\n",
    "\n",
    "        # for data in orig_dataset_by_level:\n",
    "        #     print(data.keys())\n",
    "            # print(data[\"completions\"])\n",
    "                \n",
    "        _orig_dataset_by_level = score(_orig_dataset_by_level, config)\n",
    "        # for data in orig_dataset_by_level:\n",
    "        #     print(data)\n",
    "        #     stop\n",
    "\n",
    "        # _orig_dataset_by_level.push_to_hub(dataset_id, config_name=f\"{config_name}--trial-{trial_idx}\", split='test')\n",
    "        _orig_dataset_by_level.to_json(f\"results/{config_name}--nolimit--trial-{trial_idx}.jsonl\")\n",
    "        \n",
    "        # compute the time\n",
    "        if trial_idx % 1 == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            time_per_trial = total_time/(trial_idx+1)\n",
    "            time_per_question = time_per_trial/num_questions\n",
    "            print(f\"trial {trial_idx}\")\n",
    "            print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "            print(f\"it takes {time_per_trial:0.4f}s per trial\")\n",
    "\n",
    "        trial_idx += 1\n",
    "        \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224bdaa-2f21-4aaa-8b9d-2c1182bef68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"tnguyen9210/LLM-Reasoning-Math-500\"\n",
    "orig_dataset_by_level.push_to_hub(dataset_id, config_name=config_name, split='test')\n",
    "orig_dataset_by_level.to_json(f\"results/{config_name}.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
