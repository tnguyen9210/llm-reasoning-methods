/var/spool/slurm/d/job12894448/slurm_script: line 18: Usage:
# Best-of-N on the MATH-500 dataset

sbatch recipes/launch_array.slurm recipes/Llama-3.2-1B-Instruct/best_of_n.yaml \
    --hub_dataset_id=<YOUR_ORG>/Llama-3.2-1B-Instruct-bon-completions
: No such file or directory
+ micromamba activate py311
+ __mamba_wrap activate py311
+ local cmd=activate
+ case "${cmd}" in
+ __mamba_xctivate activate py311
+ local ask_mamba
++ PS1='(py311) '
++ __mamba_exe shell activate py311 --shell bash
++ /opt/ohpc/pub/apps/micromamba/2.0.2-2/bin/micromamba shell activate py311 --shell bash
+ ask_mamba='. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libxml2_deactivate.sh"
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libglib_deactivate.sh"
PS1='\''(py311) '\''
export PATH='\''/home/u20/tnguyen9210/micromamba/envs/py311/bin:/opt/ohpc/pub/mpi/libfabric/1.18.0/bin:/opt/ohpc/pub/mpi/ucx-ohpc/1.17.0/bin:/opt/ohpc/pub/libs/hwloc/bin:/opt/ohpc/pub/mpi/openmpi5-gnu13/5.0.5/bin:/opt/ohpc/pub/compiler/gcc/13.2.0/bin:/opt/ohpc/pub/utils/prun/2.2:/opt/ohpc/pub/utils/autotools/bin:/opt/ohpc/pub/bin:/home/u20/tnguyen9210/micromamba/condabin:/opt/TurboVNC/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u20/tnguyen9210/.local/bin:/home/u20/tnguyen9210/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(py311) '\''
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libglib_activate.sh"
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libxml2_activate.sh"'
+ eval '. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libxml2_deactivate.sh"
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libglib_deactivate.sh"
PS1='\''(py311) '\''
export PATH='\''/home/u20/tnguyen9210/micromamba/envs/py311/bin:/opt/ohpc/pub/mpi/libfabric/1.18.0/bin:/opt/ohpc/pub/mpi/ucx-ohpc/1.17.0/bin:/opt/ohpc/pub/libs/hwloc/bin:/opt/ohpc/pub/mpi/openmpi5-gnu13/5.0.5/bin:/opt/ohpc/pub/compiler/gcc/13.2.0/bin:/opt/ohpc/pub/utils/prun/2.2:/opt/ohpc/pub/utils/autotools/bin:/opt/ohpc/pub/bin:/home/u20/tnguyen9210/micromamba/condabin:/opt/TurboVNC/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u20/tnguyen9210/.local/bin:/home/u20/tnguyen9210/bin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(py311) '\''
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libglib_activate.sh"
. "/home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libxml2_activate.sh"'
++ . /home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libxml2_deactivate.sh
+++ test -n ''
+++ unset XML_CATALOG_FILES
+++ unset xml_catalog_files_libxml2
++ . /home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/deactivate.d/libglib_deactivate.sh
+++ export GSETTINGS_SCHEMA_DIR=
+++ GSETTINGS_SCHEMA_DIR=
+++ unset GSETTINGS_SCHEMA_DIR_CONDA_BACKUP
+++ '[' -z ']'
+++ unset GSETTINGS_SCHEMA_DIR
++ PS1='(py311) '
++ export PATH=/home/u20/tnguyen9210/micromamba/envs/py311/bin:/opt/ohpc/pub/mpi/libfabric/1.18.0/bin:/opt/ohpc/pub/mpi/ucx-ohpc/1.17.0/bin:/opt/ohpc/pub/libs/hwloc/bin:/opt/ohpc/pub/mpi/openmpi5-gnu13/5.0.5/bin:/opt/ohpc/pub/compiler/gcc/13.2.0/bin:/opt/ohpc/pub/utils/prun/2.2:/opt/ohpc/pub/utils/autotools/bin:/opt/ohpc/pub/bin:/home/u20/tnguyen9210/micromamba/condabin:/opt/TurboVNC/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u20/tnguyen9210/.local/bin:/home/u20/tnguyen9210/bin
++ PATH=/home/u20/tnguyen9210/micromamba/envs/py311/bin:/opt/ohpc/pub/mpi/libfabric/1.18.0/bin:/opt/ohpc/pub/mpi/ucx-ohpc/1.17.0/bin:/opt/ohpc/pub/libs/hwloc/bin:/opt/ohpc/pub/mpi/openmpi5-gnu13/5.0.5/bin:/opt/ohpc/pub/compiler/gcc/13.2.0/bin:/opt/ohpc/pub/utils/prun/2.2:/opt/ohpc/pub/utils/autotools/bin:/opt/ohpc/pub/bin:/home/u20/tnguyen9210/micromamba/condabin:/opt/TurboVNC/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u20/tnguyen9210/.local/bin:/home/u20/tnguyen9210/bin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(py311) '
++ CONDA_PROMPT_MODIFIER='(py311) '
++ . /home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libglib_activate.sh
+++ export GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
+++ GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
+++ export GSETTINGS_SCHEMA_DIR=/home/u20/tnguyen9210/micromamba/envs/py311/share/glib-2.0/schemas
+++ GSETTINGS_SCHEMA_DIR=/home/u20/tnguyen9210/micromamba/envs/py311/share/glib-2.0/schemas
++ . /home/u20/tnguyen9210/micromamba/envs/py311/etc/conda/activate.d/libxml2_activate.sh
+++ test -n ''
+++ xml_catalog_files_libxml2=
+++ XML_CATALOG_FILES=
+++ conda_catalog_files=
+++ ifs_libxml2=' 	
'
+++ IFS=' '
+++ rem=/home/u20/tnguyen9210/micromamba/envs/py311
+++ for pre in ${rem}
+++ test '' = /home/u20/tnguyen9210/micromamba/envs/py311
+++ conda_catalog_files=/home/u20/tnguyen9210/micromamba/envs/py311
+++ rem=
+++ IFS=' 	
'
+++ conda_catalog_files='file:///home/u20/tnguyen9210/micromamba/envs/py311/etc/xml/catalog file:///etc/xml/catalog'
+++ export 'XML_CATALOG_FILES=file:///home/u20/tnguyen9210/micromamba/envs/py311/etc/xml/catalog file:///etc/xml/catalog'
+++ XML_CATALOG_FILES='file:///home/u20/tnguyen9210/micromamba/envs/py311/etc/xml/catalog file:///etc/xml/catalog'
+++ unset conda_catalog_files ifs_libxml2 rem
+ __mamba_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python generate_sdp_prm800k_v29.py
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.10s/it]

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:15,  2.26it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:13,  2.42it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:12,  2.46it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:12,  2.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:12,  2.44it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:11,  2.47it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:11,  2.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:10,  2.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:10,  2.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:09,  2.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:09,  2.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:09,  2.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:08,  2.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:05<00:08,  2.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:07,  2.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:06<00:07,  2.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:06<00:07,  2.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:06,  2.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:07<00:06,  2.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:07<00:05,  2.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:08<00:05,  2.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:08<00:05,  2.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:09<00:04,  2.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:09<00:03,  2.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:10<00:03,  2.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:10<00:03,  2.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:11<00:02,  2.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:11<00:01,  2.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:12<00:01,  2.48it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:12<00:01,  2.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:13<00:00,  2.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
[rank0]: multiprocessing.pool.RemoteTraceback: 
[rank0]: """
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/u20/tnguyen9210/micromamba/envs/py311/lib/python3.11/multiprocessing/pool.py", line 125, in worker
[rank0]:     result = (True, func(*args, **kwds))
[rank0]:                     ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/micromamba/envs/py311/lib/python3.11/multiprocessing/pool.py", line 51, in starmapstar
[rank0]:     return list(itertools.starmap(args[0], args[1]))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/core/select_diverse_plus_v21.py", line 92, in process_select_diverse
[rank0]:     selected_idxes  = _select_diverse(K, V, q_embeds, q_log_probs, q_ppl, q_scores, ds_alpha)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/core/select_diverse_plus_v21.py", line 54, in _select_diverse
[rank0]:     q_vals = ds_beta*q_scores + ds_alpha*q_diversity
[rank0]:              ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
[rank0]: ValueError: operands could not be broadcast together with shapes (0,) (8,) 
[rank0]: """

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/generate_sdp_prm800k_v29.py", line 141, in <module>
[rank0]:     main()
[rank0]:   File "/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/generate_sdp_prm800k_v29.py", line 121, in main
[rank0]:     results = search_algo(batch_of_questions, config, llm_vllm, llm_tf, tokenizer, prm)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/core/select_diverse_plus_v21.py", line 278, in select_diverse_plus_search
[rank0]:     pool_results = pool.starmap(process_select_diverse, tasks)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/micromamba/envs/py311/lib/python3.11/multiprocessing/pool.py", line 375, in starmap
[rank0]:     return self._map_async(func, iterable, starmapstar, chunksize).get()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u20/tnguyen9210/micromamba/envs/py311/lib/python3.11/multiprocessing/pool.py", line 774, in get
[rank0]:     raise self._value
[rank0]: ValueError: operands could not be broadcast together with shapes (0,) (8,) 
[rank0]:[W507 21:55:12.569122168 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
