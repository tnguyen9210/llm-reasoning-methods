{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02cf7ef-318d-4d71-a357-78376eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search import beam_search, best_of_n, dvts\n",
    "from sal.utils.data import get_dataset, save_dataset\n",
    "from sal.utils.parser import H4ArgumentParser\n",
    "from sal.utils.score import score\n",
    "\n",
    "# from sal.models.reward_models import load_prm\n",
    "from core.reward_models import RLHFFlow\n",
    "\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589fee98-466c-457a-80c1-bf8533ef829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/math500\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac9b83c-ece8-4cc5-b27f-0ee32ade05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.n = 8\n",
    "config.approach = \"best_of_n\"\n",
    "config.search_batch_size = 25\n",
    "config.sort_completed = True\n",
    "config.filter_duplicates = True\n",
    "config.num_samples = 10         # REMOVE THIS LINE TO RUN ON THE WHOLE DATASET\n",
    "config.seed = 0\n",
    "# pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5fed180-f809-4702-b14c-821646e2fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "level = 3\n",
    "\n",
    "#  load data \n",
    "# data_by_levels = load_data_prm800k(data_dir)\n",
    "dataset = load_dataset(config.dataset_name, split=config.dataset_split, cache_dir=data_dir)\n",
    "dataset = dataset.filter(lambda example: example['level'] == 2)\n",
    "print(len(dataset))\n",
    "\n",
    "# dataset = dataset.select(range(min(len(dataset), config.num_samples)))\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 15:01:04 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-10 15:01:05 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-10 15:01:13 config.py:549] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-10 15:01:13 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-10 15:01:15 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-10 15:01:15 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-10 15:01:16 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876a5fcbf30b41f4b3bb53d4cad7e5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 15:01:17 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-10 15:01:18 worker.py:267] Memory profiling takes 0.52 seconds\n",
      "INFO 04-10 15:01:18 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-10 15:01:18 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 04-10 15:01:18 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 04-10 15:01:18 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 04-10 15:01:20 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 15:01:37 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 04-10 15:01:37 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c58bcf6-f131-4523-acb7-7ffd75261548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59ab647448f4962b4f72939d2b84fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_tokenizer_dir, device_map='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49d8309-0253-4d6b-be87-bc7358147dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf434d568af45ba8f15e74b22a67487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running search:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "approach_fn = best_of_n\n",
    "\n",
    "dataset = dataset.map(\n",
    "    approach_fn,\n",
    "    batched=True,\n",
    "    batch_size=config.search_batch_size,\n",
    "    fn_kwargs={\"config\": config, \"llm\": llm_vllm, \"prm\": prm},\n",
    "    desc=\"Running search\",\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d347c32e-dba1-4997-8d74-45757293e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14021201efbe4ae2b1f6c3ac8310d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc342fe8e068419c8a0e5fea0c1e0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 1:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cade3b135a45a985196825408a9693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 1:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9161648875f4d81be090ce6ed1b627e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 1:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac99fc406e834eb0ad361fe895a3bee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 1:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56178ec26a994d78992092ad82d6cc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 1:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:  25%|██▌       | 1/4 [00:00<00:01,  2.78it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d291f88562084669b339a10268dafd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 2:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eade37cbf204935aef9389ace5ec81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 2:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc99310e6a454f49ad8cc7dce05eb48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 2:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e19ef25bce4231b63e231d44895e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 2:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8eb4633f424549b0c5a3627c828821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 2:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:  50%|█████     | 2/4 [00:00<00:00,  2.57it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105402b62840462bb8d0a186d25e0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 4:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670fcdaad5d04c118f584733c19e2308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 4:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfe63bf898342feb63be99a6c2bb16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 4:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6b6f56b54844b18faedea7bf76d73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 4:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf86469b41b840dd9e411cc73f271132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 4:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions:  75%|███████▌  | 3/4 [00:01<00:00,  1.88it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa9c9c592f24df2a124d068626be931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subsample 8:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f47cdb3812540f0b93c3d6b6335387e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract answers 8:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4558af19f58f42c6ab4a08c8aa72632f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute weighted pred 8:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0abedaf260b490fb8fa7a5341b320e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute majority pred 8:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5672d966221d4598aa56c3194d234397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute naive pred 8:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing majority & weighted predictions: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = score(dataset, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c58b27-3254-408a-bbfe-64377f43fb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7f404db35a4b6793db849649a44249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d358b14caa145619649f4946e752359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261af569a021458a86ebc5d0a3077fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tnguyen9210/LLM-Reasoning-Math-500/commit/e654adf78d01ae9092a31df6b1025082cfe9f0d3', commit_message='Upload dataset', commit_description='', oid='e654adf78d01ae9092a31df6b1025082cfe9f0d3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tnguyen9210/LLM-Reasoning-Math-500', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tnguyen9210/LLM-Reasoning-Math-500'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"tnguyen9210/LLM-Reasoning-Math-500\"\n",
    "revision = \"bon-n4-level2-v11\"\n",
    "dataset.push_to_hub(dataset_id, config_name=revision)\n",
    "\n",
    "# save_dataset(dataset, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
