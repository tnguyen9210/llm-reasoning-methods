{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f10c28-3f2d-47c0-ace3-b7a5d1463ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cae36f-3789-45cf-9533-1842d3c71779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sal.config import Config\n",
    "\n",
    "from core import best_of_n\n",
    "from utils.load_data import load_data_prm800k\n",
    "from utils import grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24dfa33-ee28-4654-b0e1-24a1c4eff608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a4e1bf-b56b-4874-98cf-0ed225567369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "\n",
    "# ds_completions = load_completions(completions_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97197039-298e-407e-ae80-876cc65dd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 128\n"
     ]
    }
   ],
   "source": [
    "level = '4'\n",
    "num_questions = len(data_by_levels[level])\n",
    "# num_questions = 1\n",
    "num_trials = 1\n",
    "num_budgets = 8\n",
    "print(f\"num_questions = {num_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "457c29a6-0b47-42ef-91ce-720a6a30eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bon_score = 0.328125\n",
      "sd_score = 0.3359375\n",
      "32\n",
      "-0.21740424873680092\n",
      "0.8282422121612354\n"
     ]
    }
   ],
   "source": [
    "def evaluate_correctness(data_dir, data_by_levels, num_trials, num_budgets=None):\n",
    "    with open(data_dir, 'r', encoding='utf-8') as fin:\n",
    "        all_correctness = []\n",
    "        best_correctness = []\n",
    "        best_completions = []\n",
    "        best_prm_scores = []\n",
    "        pred_answers = []\n",
    "        gt_answers = []\n",
    "        trial_idx = 0\n",
    "        for line in fin:\n",
    "            if trial_idx >= num_trials:\n",
    "                break\n",
    "                \n",
    "            trial_data = json.loads(line)\n",
    "            for q_idx in range(len(data_by_levels)):\n",
    "                if num_budgets is not None:\n",
    "                    completions = trial_data['completions'][q_idx][:num_budgets]\n",
    "                    best_idx = np.argmax(trial_data['agg_scores'][q_idx][:num_budgets])\n",
    "                else:\n",
    "                    completions = trial_data['completions'][q_idx]\n",
    "                    best_idx = np.argmax(trial_data['agg_scores'][q_idx])\n",
    "                best_completion = trial_data['completions'][q_idx][best_idx]\n",
    "                pred_answer = grader.extract_last_boxed_answer(best_completion)\n",
    "                gt_answer = data_by_levels[q_idx]['answer']\n",
    "                is_correct = False\n",
    "                for cidx, completion in enumerate(completions):\n",
    "                    c_answer = grader.extract_last_boxed_answer(completion)\n",
    "                    if grader.grade_answer(c_answer, gt_answer):\n",
    "                        is_correct = True\n",
    "                        break\n",
    "                best_is_correct = grader.grade_answer(pred_answer, gt_answer)\n",
    "                # if best_is_correct != is_correct and q_idx > 22:\n",
    "                #     print(f\"\\n-> question {q_idx}\")\n",
    "                #     print(f\"question : {data_by_levels[q_idx]['problem']}\")\n",
    "                #     print(f\"pred answer: {pred_answer}\")\n",
    "                #     print(f\"gt answer: {gt_answer}\")\n",
    "                #     print(f\"is correct: {is_correct}\")\n",
    "                #     print(f\"is correct (best): {best_is_correct}\")\n",
    "                #     # print(best_completion)\n",
    "                #     print(trial_data['agg_scores'][q_idx][:num_budgets])\n",
    "                #     print(best_idx)\n",
    "                #     for cidx, completion in enumerate(trial_data['completions'][q_idx][:num_budgets]):\n",
    "                #         print(f\"cidx = {cidx}\")\n",
    "                #         print(completion)\n",
    "                # is_correct = grader.grade_answer(pred_answer, gt_answer)\n",
    "\n",
    "                # print(len(trial_data['completions'][q_idx]))\n",
    "                # if pred_answer is None and q_idx > 22:\n",
    "                #     print(f\"\\n-> question {q_idx}\")\n",
    "                #     print(f\"question : {data_by_levels[q_idx]['problem']}\")\n",
    "                #     print(f\"pred answer: {pred_answer}\")\n",
    "                #     print(f\"gt answer: {gt_answer}\")\n",
    "                #     print(f\"is correct: {is_correct}\")\n",
    "                #     # print(best_completion)\n",
    "                #     print(trial_data['agg_scores'][q_idx][:num_budgets])\n",
    "                #     print(best_idx)\n",
    "                #     for cidx, completion in enumerate(trial_data['completions'][q_idx][:num_budgets]):\n",
    "                #         print(f\"cidx = {cidx}\")\n",
    "                #         print(completion)\n",
    "                # print(f\"all scores = {trial_data['agg_scores'][q_idx]}\")\n",
    "                # print(f\"best score = {trial_data['agg_scores'][q_idx][best_idx]}\")\n",
    "                all_correctness.append(is_correct)\n",
    "                best_correctness.append(best_is_correct)\n",
    "                # results.append(trial_data['agg_scores'][q_idx][best_idx])\n",
    "                best_completions.append(best_completion)\n",
    "                pred_answers.append(pred_answer)\n",
    "                gt_answers.append(gt_answer)\n",
    "                best_prm_scores.append(trial_data['agg_scores'][q_idx][best_idx])\n",
    "\n",
    "        trial_idx += 1\n",
    "        \n",
    "    return all_correctness, best_correctness, best_completions, best_prm_scores, pred_answers, gt_answers\n",
    "\n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "config.n = 8\n",
    "config.beam_width = 4\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 1\n",
    "config.sort_completed = False\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "bon_dir = \"results/scores_bon_prm800k_level4_n16_v11.jsonl\" \n",
    "sd_dir = f\"results/scores_sd_prm800k_level{level}_n{config.n}_bw{config.beam_width}_depth{config.num_iterations}_lam{config.lam}_v11.jsonl\"\n",
    "\n",
    "bon_correctness, bon_best_correctness, bon_best_completions, bon_best_prm_scores, bon_pred_answers, bon_gt_answer = \\\n",
    "    evaluate_correctness(bon_dir, data_by_levels[level], num_trials, config.n)\n",
    "sd_correctness, sd_best_correctness, sd_best_completions, sd_best_prm_scores, sd_pred_answers, sd_gt_answer = \\\n",
    "    evaluate_correctness(sd_dir, data_by_levels[level], num_trials)\n",
    "# print(sd_results)\n",
    "# print(sd_best_completions)\n",
    "# print(sd_pred_answers)\n",
    "# print(sd_gt_answer)\n",
    "# print(f\"bon_score = {np.mean(bon_correctness)}\")\n",
    "# print(f\"sd_score = {np.mean(sd_correctness)}\")\n",
    "print(f\"bon_score = {np.mean(bon_best_correctness)}\")\n",
    "print(f\"sd_score = {np.mean(sd_best_correctness)}\")\n",
    "\n",
    "# print(bon_correctness)\n",
    "# print(bon_best_correctness)\n",
    "num_differences = np.sum(np.array(bon_correctness) != np.array(bon_best_correctness))\n",
    "print(num_differences)\n",
    "# print(np.sum(sd_correctness != sd_best_correctness))\n",
    "\n",
    "t_stat, p_value = ttest_rel(np.array(bon_correctness).astype(int), np.array(sd_correctness).astype(int))\n",
    "t_stat, p_value = ttest_rel(np.array(bon_best_correctness).astype(int), np.array(sd_best_correctness).astype(int))\n",
    "print(t_stat)\n",
    "print(p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
