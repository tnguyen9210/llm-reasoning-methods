{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f5342f-19ab-4749-8ed0-85f7b999b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cf7ef-318d-4d71-a357-78376eee49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "\n",
    "from core.best_of_n import best_of_n_v11, best_of_n_v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fee98-466c-457a-80c1-bf8533ef829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e68c5-4858-4a96-895e-1e38328c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fed180-f809-4702-b14c-821646e2fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e91f35-840d-4aa8-ad83-063d9ba9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-23 11:42:20 [config.py:2599] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-23 11:42:28 [config.py:583] This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-23 11:42:28 [arg_utils.py:1765] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-23 11:42:28 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-23 11:42:30 [cuda.py:234] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-23 11:42:30 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-23 11:42:30 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-23 11:42:30 [model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232f927a6fea4b13aa47e6d240eac059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 11:42:32 [loader.py:429] Loading weights took 1.29 seconds\n",
      "INFO 03-23 11:42:32 [model_runner.py:1146] Model loading took 2.3185 GB and 1.388610 seconds\n",
      "INFO 03-23 11:42:33 [worker.py:267] Memory profiling takes 0.61 seconds\n",
      "INFO 03-23 11:42:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.50) = 15.87GiB\n",
      "INFO 03-23 11:42:33 [worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 12.25GiB.\n",
      "INFO 03-23 11:42:33 [executor_base.py:111] # cuda blocks: 25088, # CPU blocks: 8192\n",
      "INFO 03-23 11:42:33 [executor_base.py:116] Maximum concurrency for 10000 tokens per request: 40.14x\n",
      "INFO 03-23 11:42:34 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:13<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-23 11:42:48 [model_runner.py:1570] Graph capturing finished in 14 secs, took 0.13 GiB\n",
      "INFO 03-23 11:42:48 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 16.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 14.584884643554688\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9612f06-551f-40b7-8e95-c2efb6ddfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_path)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_path).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d49d8309-0253-4d6b-be87-bc7358147dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trials = 50\n",
      "num_questions = 43\n",
      "<function best_of_n_v11 at 0x7f4a52bc1440>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(result_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trial_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trials):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# best_of_n(batch_of_questions, config, llm_vllm, random_seeds[trial_idx])\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_of_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_vllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtrial_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, fout)\n\u001b[1;32m     30\u001b[0m         fout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/tnn1/LLMs/llm-reasoning-methods/core/best_of_n.py:43\u001b[0m, in \u001b[0;36mbest_of_n_v11\u001b[0;34m(batch_of_questions, config, llm_vllm, random_seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     29\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# temperature=0,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[1;32m     40\u001b[0m )        \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate responses \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mllm_vllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemplated_convs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Re-generate responses if we get more responses than expected\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(responses) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_of_questions):\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/utils.py:1066\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1061\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1062\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1063\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m         )\n\u001b[0;32m-> 1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:464\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    454\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    457\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    458\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    462\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1371\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1369\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1371\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1379\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;66;03m# Skip the scheduler if there are any remaining steps in the seq groups.\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;66;03m# This ensures that the scheduler is only called again when the current\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# batch has completed.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;66;03m# The scheduler is also skipped if a single request caused the last\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;66;03m# engine step to fail, and the previous schedule needs to be rerun.\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_remaining_steps(\n\u001b[1;32m   1374\u001b[0m         seq_group_metadata_list\n\u001b[1;32m   1375\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_scheduling_next_step:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# Schedule iteration\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     (seq_group_metadata_list, scheduler_outputs,\n\u001b[1;32m   1378\u001b[0m      allow_async_output_proc\n\u001b[0;32m-> 1379\u001b[0m      ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1381\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mseq_group_metadata_list \u001b[38;5;241m=\u001b[39m seq_group_metadata_list\n\u001b[1;32m   1382\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mscheduler_outputs \u001b[38;5;241m=\u001b[39m scheduler_outputs\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/core/scheduler.py:1574\u001b[0m, in \u001b[0;36mScheduler.schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;66;03m# It assumes the scheduled_seq_groups is ordered by\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;66;03m# prefill < decoding.\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_first_prefill \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39msend_delta_data:\n\u001b[0;32m-> 1574\u001b[0m     seq_group_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceGroupMetadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooling_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_block_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_computed_block_nums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_seq_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_seq_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_block_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_block_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will only be present for the 1st comm\u001b[39;49;00m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# between engine and worker.\u001b[39;49;00m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# the subsequent comms can still use delta, but\u001b[39;49;00m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `multi_modal_data` will be None.\u001b[39;49;00m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_data\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prefill_groups\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_modal_placeholders\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_prefill_groups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;66;03m# When SPMD mode is enabled, we only send delta data except for\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;66;03m# the first request to reduce serialization cost.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m     seq_data_delta \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/micromamba/envs/py311/lib/python3.11/site-packages/vllm/sequence.py:990\u001b[0m, in \u001b[0;36mSequenceGroupMetadata.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m### Stateful fields that are lazily defined. ###\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# The number of speculative tokens adopted in this request.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# None means specuative decoding is not used.\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# Zero means speculative decoding is disabled for some reasons.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# TODO: We should maintain this states out of the sequence group.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m num_speculative_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_chunk_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_prompt:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "\n",
    "level = '1'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 20\n",
    "num_trials = 200\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "# select search algo\n",
    "search_name = 'best_of_n'\n",
    "algo_type = 1\n",
    "if search_name == 'best_of_n':\n",
    "    if algo_type == 1:\n",
    "        search_algo = best_of_n.best_of_n_v11\n",
    "    else:\n",
    "        search_algo = best_of_n.best_of_n_v12\n",
    "print(search_algo)\n",
    "\n",
    "# run search_algo and save results\n",
    "result_dir = f\"results/generate_bon_prm800k_level{level}_n{config.n}_v11.jsonl\"\n",
    "start_time = time.time()\n",
    "with open(result_dir, 'w', encoding = 'utf-8') as fout:\n",
    "    for trial_idx in range(num_trials):\n",
    "        # best_of_n(batch_of_questions, config, llm_vllm, random_seeds[trial_idx])\n",
    "        results = search_algo(batch_of_questions, config, llm_vllm, 10000+trial_idx)\n",
    "        json.dump(results, fout)\n",
    "        fout.write('\\n')\n",
    "    \n",
    "        # compute the time\n",
    "        if trial_idx % 1 == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            time_per_trial = total_time/(trial_idx+1)\n",
    "            time_per_question = time_per_trial/num_questions\n",
    "            print(f\"trial {trial_idx}\")\n",
    "            print(f\"it takes {time_per_question:0.4f}s per question\")\n",
    "            print(f\"it takes {time_per_trial:0.4f}s per trial\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"it takes {total_time:0.4f}s in total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
