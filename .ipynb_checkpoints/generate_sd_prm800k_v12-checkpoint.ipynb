{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "import copy \n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(4)\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import build_conv, generate_k_steps, last\n",
    "\n",
    "from core import select_diverse\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 09:47:32 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-09 09:47:32 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-09 09:47:39 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-09 09:47:39 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-09 09:47:40 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-09 09:47:40 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-09 09:47:41 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b9703c1d2e47f1a78db5dbdff80bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 09:47:43 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-09 09:47:43 worker.py:267] Memory profiling takes 0.48 seconds\n",
      "INFO 04-09 09:47:43 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-09 09:47:43 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 04-09 09:47:44 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 04-09 09:47:44 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 04-09 09:47:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 09:48:00 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.13 GiB\n",
      "INFO 04-09 09:48:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "590d4828-9a59-4431-8054-d1859ec213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    q_idx: int\n",
    "    question: str\n",
    "    templated_conv: str\n",
    "    current_text: str | None\n",
    "    next_texts: list[str] | None\n",
    "    lookahead_texts: list[str] | None\n",
    "    stop_reasons: list[str | None] | None\n",
    "    best_scores: list[float]  # the PRM scores\n",
    "    all_scores: list[list[float]]  # all PRM scores\n",
    "    previous_text: str | None\n",
    "    pruned: False\n",
    "    history: list[str]\n",
    "    completed: bool = False\n",
    "    completion_tokens: int = 0\n",
    "\n",
    "\n",
    "def _select_diverse(X_embeds, X_lprobs, X_ppl, K, V):\n",
    "    num_arms = len(X_embeds)\n",
    "    _V = copy.deepcopy(V)\n",
    "    A_idxes = []\n",
    "    A_embeds = []\n",
    "    arm_vals_arr = []\n",
    "    V_arr = [_V]\n",
    "    tol = 0.0001\n",
    "    for it in range(K):\n",
    "        _V_inv = np.linalg.inv(_V)\n",
    "        arm_vals = np.einsum('ij,jk,ik->i', X_embeds, _V_inv, X_embeds)\n",
    "        arm_vals_arr.append(arm_vals)\n",
    "        max_val = np.max([val for idx, val in enumerate(arm_vals) if idx not in A_idxes])\n",
    "        # candidate_idxes = np.where(np.abs(arm_vals-max_val) < tol)[0]\n",
    "        candidate_idxes = [\n",
    "            arm_idx for arm_idx, arm_val in enumerate(arm_vals)\n",
    "            if (np.abs(max_val - arm_val) <= tol) and (arm_idx not in A_idxes)\n",
    "        ]\n",
    "\n",
    "        best_idx = max(candidate_idxes, key=lambda i: X_lprobs[i])\n",
    "        # print(arm_vals)\n",
    "        # print(X_lprobs)\n",
    "        # print(candidate_idxes)\n",
    "        # print(best_idx)\n",
    "\n",
    "        best_embeds = X_embeds[best_idx]\n",
    "        best_embeds = best_embeds.reshape(-1,1)\n",
    "        # print(best_embeds.shape)\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(best_embeds, best_embeds.T)\n",
    "        V_arr.append(_V)\n",
    "\n",
    "        # update A\n",
    "        A_idxes.append(best_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(A_idxes)\n",
    "\n",
    "\n",
    "    return A_idxes, arm_vals_arr, V_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c694118-92ff-4bca-933f-8510663c117c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 2\n",
      "num_trials = 1\n",
      "\n",
      "-> 0\n",
      "16\n",
      "16\n",
      "2\n",
      "8\n",
      "\n",
      "-> 1\n",
      "16\n",
      "16\n",
      "2\n",
      "8\n",
      "\n",
      "-> 2\n",
      "16\n",
      "0\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "def select_diverse_search(batch_of_questions, config: Config, llm_vllm: LLM, llm_tf, llm_tokenizer) -> list[Beam]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "    \n",
    "    completed_beams: list[Beam] = []\n",
    "    beams: list[Beam] = []\n",
    "    for q_idx, question in enumerate(batch_of_questions):\n",
    "        for _ in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    q_idx=q_idx,\n",
    "                    question=question,\n",
    "                    templated_conv=\"\",\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            ) \n",
    "\n",
    "\n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    batch_arm_vals = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_all_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_all_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_all_next_texts = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_V = [[] for _ in range(len(batch_of_questions))]\n",
    "    batch_A_idxes = [[] for _ in range(len(batch_of_questions))]\n",
    "    \n",
    "    \n",
    "    for it in range(config.num_iterations):\n",
    "        print(f\"\\n-> {it}\")\n",
    "        if it == 0:\n",
    "            active_beams = beams\n",
    "        else:\n",
    "            # active_beams = [b for b in active_beams if not b.pruned]\n",
    "            extended_beams = []\n",
    "            for beam in active_beams:\n",
    "                if beam.pruned:\n",
    "                    continue \n",
    "                    \n",
    "                for j in range(config.beam_width):\n",
    "                    extended_beams.append(copy.deepcopy(beam))\n",
    "\n",
    "            active_beams = extended_beams\n",
    "        \n",
    "        print(len(active_beams))\n",
    "        convs = [\n",
    "            build_conv(b.question, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "\n",
    "        add_generation_prompt = it == 0\n",
    "        continue_final_message = it > 0\n",
    "    \n",
    "        tokenizer = llm_vllm.get_tokenizer()\n",
    "    \n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "            \n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        # Last iteration, generate to EOS\n",
    "        if it == config.num_iterations - 1:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        lookahead = 0 if it == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm_vllm, sampling_params, 1\n",
    "        )\n",
    "\n",
    "        # Collecct gen_results into beams\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += gen_result.next_texts[0]\n",
    "            # beam.history.append(beam.next_texts[0])\n",
    "            beam.templated_prompt = gen_result.prompt\n",
    "            # pprint.pprint(gen_result)\n",
    "            # print(f\"beam.next_texts = {beam.next_texts}\")\n",
    "            # print(f\"beam.stop_reasons = {beam.stop_reasons}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # print(f\"beam.lookahead_texts = {beam.lookahead_texts}\")\n",
    "            # stop\n",
    "            \n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "                # continue\n",
    "        \n",
    "        # Filter out comleted beams \n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        print(len(active_beams))\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            print(\"break\")\n",
    "            break\n",
    "        \n",
    "        # Extract completion's embeddings and other info\n",
    "        batch_embeds = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_log_probs = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_ppl = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_beams = [[] for _ in range(len(batch_of_questions))]\n",
    "        batch_next_texts = [[] for _ in range(len(batch_of_questions))]\n",
    "    \n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "                # print(gen_prompt)\n",
    "                # stop\n",
    "                inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "    \n",
    "                # Get last_token_embeds\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_token_embeds.shape)\n",
    "    \n",
    "                # Compute otuput_log_prob\n",
    "                # Prepare labels: shift input_ids to the right by one\n",
    "                labels = inputs['input_ids'][:, 1:]   \n",
    "                shifted_logits = outputs.logits[:, :-1, :]\n",
    "                loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "                completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "                completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "                # print(sent_ppl)\n",
    "                # print(loss)\n",
    "    \n",
    "                # normalize the embeds\n",
    "                if config.normalize_embeds:\n",
    "                    norm = np.linalg.norm(last_token_embeds)\n",
    "                    last_token_embeds /= norm\n",
    "                    # print(np.linalg.norm(last_token_embeds))\n",
    "    \n",
    "                batch_embeds[beam.q_idx].append(last_token_embeds)\n",
    "                batch_log_probs[beam.q_idx].append(completion_log_prob)\n",
    "                batch_ppl[beam.q_idx].append(completion_ppl)\n",
    "                batch_beams[beam.q_idx].append(beam)\n",
    "                batch_next_texts[beam.q_idx].append(beam.current_text)\n",
    "\n",
    "        \n",
    "    \n",
    "        # pprint.pprint(len(batch_completions_embeds))\n",
    "        # pprint.pprint(len(batch_completions_log_probs))\n",
    "        # pprint.pprint(len(batch_completions_ppl))\n",
    "        print(len(batch_beams))\n",
    "        print(len(batch_beams[0]))\n",
    "\n",
    "        # Use _select_diverse to diversify embeddings \n",
    "        for q_idx in range(len(batch_of_questions)):\n",
    "            V = config.lam*np.eye(2048)\n",
    "            K = int(config.n / config.beam_width)\n",
    "            if len(batch_beams[q_idx]) <= K:\n",
    "                continue \n",
    "    \n",
    "            selected_idxes, arm_vals_arr, V_arr = _select_diverse(\n",
    "                batch_embeds[q_idx], batch_log_probs[q_idx], batch_ppl[q_idx], K, V)\n",
    "\n",
    "            batch_arm_vals[q_idx].append(arm_vals_arr)\n",
    "            batch_V[q_idx].append(V_arr)\n",
    "            batch_A_idxes[q_idx].append(selected_idxes)\n",
    "            batch_all_log_probs[q_idx].append(batch_log_probs[q_idx])\n",
    "            batch_all_ppl[q_idx].append(batch_ppl[q_idx])\n",
    "            batch_all_next_texts[q_idx].append(batch_next_texts[q_idx])\n",
    "    \n",
    "            # print(selected_idxes)\n",
    "            \n",
    "            for idx, beam in enumerate(batch_beams[q_idx]):\n",
    "                if idx not in selected_idxes:\n",
    "                    beam.pruned = True \n",
    "\n",
    "    # Collect the completions from beams\n",
    "    completions = [[] for _ in range(len(batch_of_questions))]\n",
    "    completion_ntokens = [[] for _ in range(len(batch_of_questions))]\n",
    "\n",
    "    for beam in completed_beams:\n",
    "        completions[beam.q_idx].append(beam.current_text)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    results[\"completions\"] = completions\n",
    "    # results[\"completion_ntokens\"] = completion_ntokens\n",
    "    \n",
    "    return results, batch_arm_vals, batch_V, batch_A_idxes, batch_all_log_probs, batch_all_ppl, batch_all_next_texts\n",
    "        \n",
    "\n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 8\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 3\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 0.1\n",
    "config.normalize_embeds = True\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "level = '4'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "results, batch_arm_vals, batch_V, batch_A_idxes, batch_all_log_probs, batch_all_ppl, batch_all_next_texts = \\\n",
    "    select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cff0703d-524a-424f-91f9-982bb863858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> q_idx = 0\n",
      "question: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.\n",
      "\n",
      "->it = 0\n",
      "[array([10., 10., 10., 10., 10., 10., 10., 10.]), array([0.9091, 2.46  , 7.0008, 2.4909, 2.3948, 2.6201, 2.3224, 2.3299]), array([0.875 , 2.3894, 0.875 , 2.4139, 2.3245, 2.5418, 2.2618, 2.2729]), array([0.7087, 1.1287, 0.8723, 1.103 , 1.1355, 0.7177, 1.0359, 1.0473])]\n",
      "[0, 2, 5, 4]\n",
      "[-581.05237, -589.41754, -592.5062, -587.9727, -586.74414, -588.1382, -582.5619, -581.97876]\n",
      "-581.05237\n",
      "-592.5062\n",
      "['## Step 1: Substitute x = -2 into the function f(x)\\n'\n",
      " '$f(-2) = \\\\frac{3(-2)-2}{-2-2} = \\\\frac{-6-2}{-4} = \\\\frac{-8}{-4} = 2$\\n'\n",
      " '\\n',\n",
      " '## Step 1: Evaluate f(-2)\\n'\n",
      " 'To find the value of $f(-2)$, substitute $x = -2$ into the equation $f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}$. So $f(-2) = \\\\frac{3(-2)-2}{-2-2} = \\\\frac{-6-2}{-4} = '\n",
      " '\\\\frac{-8}{-4} = 2$.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Evaluate f(-2)\\n'\n",
      " 'To find the value of f(-2), substitute x = -2 into the given function f(x) = '\n",
      " '(3x - 2)/(x - 2). Then calculate the result.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To find the value of $f(-2)$, we will substitute $x=-2$ into the function '\n",
      " '$f(x) = \\\\frac{3x-2}{x-2}$. So $f(-2) = \\\\frac{3(-2)-2}{-2-2} = '\n",
      " '\\\\frac{-6-2}{-4} = \\\\frac{-8}{-4} = 2$.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To find f(-2), substitute x = -2 into the function f(x) = (3x - 2)/(x - 2).\\n'\n",
      " 'f(-2) = (3(-2) - 2)/(−2 - 2) = (-6 - 2)/(-4) = -8/(-4) = 2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To calculate f(-2), we substitute x = -2 into the equation f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}. This gives us f(-2) = \\\\frac{3(-2)-2}{-2-2} = '\n",
      " '\\\\frac{-8-2}{-4} = \\\\frac{-10}{-4} = \\\\frac{5}{2}.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To calculate f(-2), we substitute x = -2 into the function f(x) = (3x - 2) / '\n",
      " '(x - 2).\\n'\n",
      " 'f(-2) = (3(-2) - 2) / (-2 - 2) = (-6 - 2) / (-4) = -8 / -4 = 2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To find f(-2), we substitute x = -2 into the function f(x) = (3x - 2) / (x - '\n",
      " '2).\\n'\n",
      " 'f(-2) = (3(-2) - 2) / (-2 - 2) = (-6 - 2) / (-4) = -8 / -4 = 2.\\n'\n",
      " '\\n']\n",
      "\n",
      "->it = 1\n",
      "[array([10., 10., 10., 10., 10., 10., 10., 10.]), array([0.9091, 0.9091, 5.3346, 4.124 , 2.4025, 2.4016, 2.3594, 2.3594]), array([0.8421, 0.8421, 0.8421, 3.8722, 1.9852, 1.9804, 2.055 , 2.055 ]), array([0.768 , 0.768 , 0.834 , 0.7948, 1.8234, 1.8156, 1.6661, 1.6661])]\n",
      "[0, 2, 3, 4]\n",
      "[-581.6847, -581.6847, -596.29114, -594.4594, -587.5688, -587.39136, -588.1475, -588.1475]\n",
      "-581.6847\n",
      "-596.29114\n",
      "['## Step 1: Substitute x = -2 into the function f(x)\\n'\n",
      " '$f(-2) = \\\\frac{3(-2)-2}{-2-2} = \\\\frac{-6-2}{-4} = \\\\frac{-8}{-4} = 2$\\n'\n",
      " '\\n'\n",
      " '## Step 2: Substitute x = -1 into the function f(x)\\n'\n",
      " '$f(-1) = \\\\frac{3(-1)-2}{-1-2} = \\\\frac{-3-2}{-3} = \\\\frac{-5}{-3} = '\n",
      " '\\\\frac{5}{3}$\\n'\n",
      " '\\n',\n",
      " '## Step 1: Substitute x = -2 into the function f(x)\\n'\n",
      " '$f(-2) = \\\\frac{3(-2)-2}{-2-2} = \\\\frac{-6-2}{-4} = \\\\frac{-8}{-4} = 2$\\n'\n",
      " '\\n'\n",
      " '## Step 2: Substitute x = -1 into the function f(x)\\n'\n",
      " '$f(-1) = \\\\frac{3(-1)-2}{-1-2} = \\\\frac{-3-2}{-3} = \\\\frac{-5}{-3} = '\n",
      " '\\\\frac{5}{3}$\\n'\n",
      " '\\n',\n",
      " '## Step 1: Evaluate f(-2)\\n'\n",
      " 'To find the value of f(-2), substitute x = -2 into the given function f(x) = '\n",
      " '(3x - 2)/(x - 2). Then calculate the result.\\n'\n",
      " '\\n'\n",
      " 'f(-2) = (3*(-2) - 2)/(-2 - 2)\\n'\n",
      " 'f(-2) = (-6 - 2)/(-4)\\n'\n",
      " 'f(-2) = -8/-4\\n'\n",
      " 'f(-2) = 2\\n'\n",
      " '\\n',\n",
      " '## Step 1: Evaluate f(-2)\\n'\n",
      " 'To find the value of f(-2), substitute x = -2 into the given function f(x) = '\n",
      " '(3x - 2)/(x - 2). Then calculate the result.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Evaluate f(-1)\\n'\n",
      " 'To find the value of f(-1), substitute x = -1 into the given function f(x) = '\n",
      " '(3x - 2)/(x - 2). Then calculate the result.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To find f(-2), substitute x = -2 into the function f(x) = (3x - 2)/(x - 2).\\n'\n",
      " 'f(-2) = (3(-2) - 2)/(−2 - 2) = (-6 - 2)/(-4) = -8/(-4) = 2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Calculate f(-1)\\n'\n",
      " 'To find f(-1), substitute x = -1 into the function f(x) = (3x - 2)/(x - 2).\\n'\n",
      " 'f(-1) = (3(-1) - 2)/(−1 - 2) = (-3 - 2)/(-3) = -5/(-3) = 5/3.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To find f(-2), substitute x = -2 into the function f(x) = (3x - 2)/(x - 2).\\n'\n",
      " 'f(-2) = (3(-2) - 2)/(−2 - 2) = (-6 - 2)/(-4) = -8/(-4) = 2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Calculate f(-1)\\n'\n",
      " 'To find f(-1), substitute x = -1 into the function f(x) = (3x - 2)/(x - 2).\\n'\n",
      " 'f(-1) = (3(-1) - 2)/(-1 - 2) = (-3 - 2)/(-3) = -5/(-3) = 5/3.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To calculate f(-2), we substitute x = -2 into the equation f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}. This gives us f(-2) = \\\\frac{3(-2)-2}{-2-2} = '\n",
      " '\\\\frac{-8-2}{-4} = \\\\frac{-10}{-4} = \\\\frac{5}{2}.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Calculate f(-1)\\n'\n",
      " 'To calculate f(-1), we substitute x = -1 into the equation f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}. This gives us f(-1) = \\\\frac{3(-1)-2}{-1-2} = '\n",
      " '\\\\frac{-3-2}{-3} = \\\\frac{-5}{-3} = \\\\frac{5}{3}.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Calculate f(-2)\\n'\n",
      " 'To calculate f(-2), we substitute x = -2 into the equation f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}. This gives us f(-2) = \\\\frac{3(-2)-2}{-2-2} = '\n",
      " '\\\\frac{-8-2}{-4} = \\\\frac{-10}{-4} = \\\\frac{5}{2}.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Calculate f(-1)\\n'\n",
      " 'To calculate f(-1), we substitute x = -1 into the equation f(x) = '\n",
      " '\\\\frac{3x-2}{x-2}. This gives us f(-1) = \\\\frac{3(-1)-2}{-1-2} = '\n",
      " '\\\\frac{-3-2}{-3} = \\\\frac{-5}{-3} = \\\\frac{5}{3}.\\n'\n",
      " '\\n']\n",
      "\n",
      "-> q_idx = 1\n",
      "question: How many positive whole-number divisors does 196 have?\n",
      "\n",
      "->it = 0\n",
      "[array([10., 10., 10., 10., 10., 10., 10., 10.]), array([4.0203, 0.9091, 1.0087, 6.3931, 7.2685, 7.46  , 7.4586, 7.5036]), array([3.3011, 0.8824, 0.9732, 3.4481, 2.3299, 1.6467, 1.7232, 0.8824]), array([2.8514, 0.8632, 0.9411, 0.7752, 2.0787, 1.4324, 1.5595, 0.8045])]\n",
      "[1, 7, 3, 0]\n",
      "[-546.28754, -541.70197, -542.5673, -545.822, -547.64886, -544.795, -544.68475, -553.75134]\n",
      "-541.70197\n",
      "-553.75134\n",
      "['## Step 1: Find the prime factorization of 196\\n'\n",
      " 'To find the number of positive whole-number divisors of 196, we first need '\n",
      " 'to find its prime factorization. We can start by dividing 196 by the '\n",
      " 'smallest prime number, 2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: To find the number of positive whole-number divisors of 196, '\n",
      " 'first, we need to find the prime factorization of 196.\\n'\n",
      " '\\n',\n",
      " '## Step 1: To find the number of positive whole-number divisors of 196, we '\n",
      " 'need to first find the prime factorization of 196.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Understand the problem\\n'\n",
      " 'To find the positive whole-number divisors of 196, we need to identify all '\n",
      " 'the numbers that can divide 196 without leaving a remainder.\\n'\n",
      " '\\n',\n",
      " '## Step 1:  First, we need to find the prime factorization of 196.\\n'\n",
      " 'We find that 196 = 2^2 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: To find the number of positive whole-number divisors of 196, '\n",
      " 'first, we need to determine the prime factorization of 196.\\n'\n",
      " 'The prime factorization of 196 is 2^2 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Prime factorize the number 196\\n'\n",
      " 'The prime factorization of 196 is 2^2 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Prime factorize 196\\n'\n",
      " 'To determine the number of divisors, we first need to prime factorize 196. '\n",
      " '196 can be broken down into its prime factors: 2^2 * 7^2.\\n'\n",
      " '\\n']\n",
      "\n",
      "->it = 1\n",
      "[array([10., 10., 10., 10., 10., 10., 10., 10.]), array([2.913 , 3.0471, 0.9091, 3.7306, 7.3329, 3.185 , 5.5325, 5.8704]), array([2.7322, 2.9069, 0.88  , 3.4772, 0.88  , 2.8582, 5.1446, 5.4359]), array([2.2571, 2.3938, 0.8411, 2.9924, 0.8719, 2.6075, 1.4383, 0.8446])]\n",
      "[2, 4, 7, 3]\n",
      "[-570.2701, -569.6874, -544.1284, -547.84247, -563.66534, -555.6102, -575.7228, -576.71906]\n",
      "-544.1284\n",
      "-576.71906\n",
      "['## Step 1: Find the prime factorization of 196\\n'\n",
      " 'To find the number of positive whole-number divisors of 196, we first need '\n",
      " 'to find its prime factorization. We can start by dividing 196 by the '\n",
      " 'smallest prime number, 2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Prime factorize 196\\n'\n",
      " '196 ÷ 2 = 98, 98 ÷ 2 = 49. Since 49 is not divisible by 2, we move to the '\n",
      " 'next prime number, which is 3. However, 49 is not divisible by 3. We move to '\n",
      " 'the next prime number, which is 5. 49 ÷ 7 = 7. So, the prime factorization '\n",
      " 'of 196 is 2^2 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Find the prime factorization of 196\\n'\n",
      " 'To find the number of positive whole-number divisors of 196, we first need '\n",
      " 'to find its prime factorization. We can start by dividing 196 by the '\n",
      " 'smallest prime number, 2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Continue prime factorization\\n'\n",
      " '196 ÷ 2 = 98. So, 196 can be further factorized as 2^2 * 49. Since 49 can be '\n",
      " 'further factorized as 7^2, the prime factorization of 196 is 2^3 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: To find the number of positive whole-number divisors of 196, '\n",
      " 'first, we need to find the prime factorization of 196.\\n'\n",
      " '\\n'\n",
      " 'The prime factorization of 196 is 2^2 * 7^2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: To find the number of positive whole-number divisors of 196, '\n",
      " 'first, we need to find the prime factorization of 196.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Prime factorizing 196, we get $196 = 2^2 \\\\times 7^2$.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Understand the problem\\n'\n",
      " 'To find the positive whole-number divisors of 196, we need to identify all '\n",
      " 'the numbers that can divide 196 without leaving a remainder.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Prime factorization\\n'\n",
      " \"First, let's do the prime factorization of 196. We know that 196 is \"\n",
      " \"divisible by 2 (since it's an even number), so we start by dividing 196 by \"\n",
      " '2.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Understand the problem\\n'\n",
      " 'To find the positive whole-number divisors of 196, we need to identify all '\n",
      " 'the numbers that can divide 196 without leaving a remainder.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Factorize 196\\n'\n",
      " 'We start by finding the prime factorization of 196. We find that 196 = 2 * 2 '\n",
      " '* 7 * 7.\\n'\n",
      " '\\n',\n",
      " '## Step 1: Prime factorize 196\\n'\n",
      " 'To determine the number of divisors, we first need to prime factorize 196. '\n",
      " '196 can be broken down into its prime factors: 2^2 * 7^2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Determine the formula to calculate the number of divisors\\n'\n",
      " 'The formula to calculate the number of divisors of a number, given its prime '\n",
      " 'factorization, is to add 1 to each exponent and multiply the results '\n",
      " 'together. For 196 = 2^2 * 7^2, the calculation would be (2+1) * (2+1).\\n'\n",
      " '\\n',\n",
      " '## Step 1: Prime factorize 196\\n'\n",
      " 'To determine the number of divisors, we first need to prime factorize 196. '\n",
      " '196 can be broken down into its prime factors: 2^2 * 7^2.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Apply the divisor function formula\\n'\n",
      " \"The formula to find the number of divisors given a number's prime \"\n",
      " 'factorization is (a+1)(b+1)..., where a, b, etc., are the powers of the '\n",
      " 'prime factors. In this case, we have two prime factors with powers of 2 and '\n",
      " '7, so we apply the formula as (2+1)(2+1).\\n'\n",
      " '\\n']\n"
     ]
    }
   ],
   "source": [
    "# print(len(beam_results))\n",
    "# pprint.pprint(results)\n",
    "# print(batch_arm_vals[0])\n",
    "for q_idx in range(len(batch_arm_vals)):\n",
    "    print(f\"\\n-> q_idx = {q_idx}\")\n",
    "    print(f\"question: {batch_of_questions[q_idx]}\")\n",
    "    # for it in range(config.num_iterations-2):\n",
    "    for it in range(len(batch_arm_vals[q_idx])):\n",
    "        print(f\"\\n->it = {it}\")\n",
    "        print(batch_arm_vals[q_idx][it])\n",
    "        print(batch_A_idxes[q_idx][it])\n",
    "        # print(batch_V[idx])\n",
    "        print(batch_all_log_probs[q_idx][it])\n",
    "        print(max(batch_all_log_probs[q_idx][it]))\n",
    "        print(min(batch_all_log_probs[q_idx][it]))\n",
    "        pprint.pprint(batch_all_next_texts[q_idx][it])\n",
    "    # print(batch_all_ppl[q_idx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
