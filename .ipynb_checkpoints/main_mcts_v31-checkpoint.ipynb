{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab828d1-ec5a-410c-a667-5aed000126d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 17:35:30 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "\n",
    "from core import mcts_search_v51\n",
    "from core.reward_models import RLHFFlow\n",
    "from utils.load_data import load_data_prm800k\n",
    "\n",
    "# from core.llm_engine import rm_engine\n",
    "# from core.llms import rm_generate\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eb26b4-3bff-44f6-a460-788ea7507582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b454024-b159-4f4f-932b-9dbce13017a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "# llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "# prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\"\n",
    "# prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-Modified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8816a0c-7f37-4950-bdb8-fcd3c2d37838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 17:35:44 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 07-14 17:35:44 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 07-14 17:35:44 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 07-14 17:35:44 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 17:35:44 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 17:35:45 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-14 17:35:45 [cuda.py:324] Using XFormers backend.\n",
      "INFO 07-14 17:35:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-14 17:35:46 [model_runner.py:1171] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11996e0afeed49f8adb21bc62d0a84ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 17:35:48 [default_loader.py:272] Loading weights took 1.33 seconds\n",
      "INFO 07-14 17:35:48 [model_runner.py:1203] Model loading took 2.3185 GiB and 1.460454 seconds\n",
      "INFO 07-14 17:35:49 [worker.py:294] Memory profiling takes 0.51 seconds\n",
      "INFO 07-14 17:35:49 [worker.py:294] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.20) = 6.35GiB\n",
      "INFO 07-14 17:35:49 [worker.py:294] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 2.75GiB.\n",
      "INFO 07-14 17:35:49 [executor_base.py:113] # cuda blocks: 5631, # CPU blocks: 32768\n",
      "INFO 07-14 17:35:49 [executor_base.py:118] Maximum concurrency for 5000 tokens per request: 18.02x\n",
      "INFO 07-14 17:35:55 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 7.07 seconds\n",
      "#--- memory: 5.07647705078125\n"
     ]
    }
   ],
   "source": [
    "llm_total_gpu = 0.4\n",
    "llm_gpu_memory_utilization = 0.2\n",
    "# llm_vllm = LLM(\n",
    "#     model = llm_dir,\n",
    "#     tensor_parallel_size=1,\n",
    "#     gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "#     # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "#     # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "#     max_model_len = 5000,\n",
    "#     dtype = \"float16\",\n",
    "#     seed = config.seed)\n",
    "\n",
    "llm_vllm = LLM(\n",
    "    model=llm_dir, \n",
    "    tensor_parallel_size=1, \n",
    "    # trust_remote_code=True,\n",
    "    swap_space=16,\n",
    "    max_model_len=5000,\n",
    "    gpu_memory_utilization=llm_gpu_memory_utilization,\n",
    "    enforce_eager=True,\n",
    "    distributed_executor_backend=None,\n",
    "    dtype=\"float16\",\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2819c3-fd28-4d4e-b5c5-cbef9b5dd009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-14 17:35:56 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 07-14 17:35:56 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 17:35:56 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 17:35:56 [model_runner.py:1171] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f5ce7f270458195463086ce69c3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 17:35:58 [default_loader.py:272] Loading weights took 1.32 seconds\n",
      "INFO 07-14 17:35:59 [model_runner.py:1203] Model loading took 2.3029 GiB and 1.372816 seconds\n",
      "#--- memory: 7.379390716552734\n"
     ]
    }
   ],
   "source": [
    "llm_vllm_embeds = LLM(\n",
    "    model=llm_dir, \n",
    "    tensor_parallel_size=1, \n",
    "    # trust_remote_code=True,\n",
    "    task=\"embed\",\n",
    "    swap_space=16,\n",
    "    max_model_len=5000,\n",
    "    gpu_memory_utilization=llm_total_gpu-llm_gpu_memory_utilization,\n",
    "    enforce_eager=True,\n",
    "    distributed_executor_backend=None,\n",
    "    dtype=\"float16\",\n",
    "    seed=0,\n",
    ")\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8c7826-f9e8-410a-b843-e913f78045d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be220ed9fc4c6ba33a9ac50c02c677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 22.336918354034424\n"
     ]
    }
   ],
   "source": [
    "prm = RLHFFlow(model_path=prm_dir, device_map='cuda:0')\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace4c93a-369f-41e4-aa5e-1b2867458990",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe38723-c26b-40c7-aa09-6f95c6d1692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb03592-aeb7-4130-aa75-b1c0f7cf5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.agg_strategy = 'last'\n",
    "\n",
    "config.n = 4                      # number of budgets to be generated per depth\n",
    "config.beam_width = 4             # number of nodes left after selection\n",
    "config.lookahead = 0              # don't use it for now\n",
    "config.max_depths = 10            # max depths, after reaching max_depth then terminate search \n",
    "config.sort_completed = False      \n",
    "config.filter_duplicates = True   # remove any duplicates in the last list of trajs\n",
    "config.seed = 0\n",
    "\n",
    "# mcts parameter\n",
    "config.num_phases = 100\n",
    "config.num_batches = 1\n",
    "config.batch_budget = config.num_batches*config.max_depths \n",
    "\n",
    "config.lam = 10 \n",
    "config.normalize_embeds = True\n",
    "\n",
    "config.cpuct_root = 0\n",
    "config.cpuct_leaf = 0\n",
    "config.ds_beta = 1.0\n",
    "config.ds_alpha = 10.0\n",
    "config.use_ppl = True\n",
    "\n",
    "config.version = \"v51\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fc45cc-ef3e-461b-bb1a-2b72d9c681e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_questions = 1\n",
      "num_trials = 1\n"
     ]
    }
   ],
   "source": [
    "level = 4                                   # level of difficulty of questions\n",
    "num_questions = len(data_by_levels[level])  # level 4 has 128 questions\n",
    "num_questions = 1\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions ['q1', 'q2', ...]\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c4857-e29b-4cca-baf7-fdb51584d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_of_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c155c57-80f7-4b58-9b4d-8d5f0dddd5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'core.mcts_search_v51' from '/home/u20/tnguyen9210/tnn1/LLMs/llm-reasoning-methods/core/mcts_search_v51.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mcts_search_v51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5a786c6-9454-405a-b9cd-2702325a92a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "-> p = 0\n",
      "\n",
      "-> d = 0\n",
      "select_child\n",
      "node.__visit_count = 0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 1\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 2\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 3\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 4\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 5\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 6\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 7\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 8\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "\n",
      "-> d = 9\n",
      "select_child\n",
      "node.__visit_count = 1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for question in batch_of_questions:\n",
    "    agent = mcts_search_v51.MCTS(config=config, question=question)\n",
    "    agent_completions = mcts_search_v51.mcts_search(question, agent, config, llm_vllm, llm_vllm_embeds, prm)\n",
    "    print(agent_completions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205786aa-2712-479d-a538-be770ebc6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, node in enumerate(agent_completions):\n",
    "#     print(f\"\\n-> idx = {idx}\")\n",
    "#     print(node.state[\"text\"])\n",
    "print(agent_completions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
