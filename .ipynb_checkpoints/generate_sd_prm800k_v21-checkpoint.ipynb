{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377db697-470b-4157-b1cc-dad33550763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil, gc\n",
    "import time \n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81dea19-ed66-4a38-9e93-dd39d877ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.distributed as dist\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams, PoolingParams\n",
    "\n",
    "from sal.config import Config\n",
    "from sal.search.utils import build_conv, generate_k_steps, last\n",
    "\n",
    "from core import select_diverse\n",
    "from utils.load_data import load_data_prm800k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d7873-a655-43ef-89e0-5bcd583f63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    GPUS = os.environ.get('CUDA_VISIBLE_DEVICES', \"0\").split(',')\n",
    "    print(GPUS)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c27af7-1c68-40c2-9366-5984d581a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "base_dir = '/groups/kjun/tnn/datasets/'\n",
    "\n",
    "# dataset path\n",
    "data_dir = base_dir + \"/prm800k/math_splits\"\n",
    "\n",
    "# llm and prm path\n",
    "llm_dir = base_dir + \"/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
    "prm_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data-GGUF/Llama3.1-8B-PRM-Deepseek-Data.Q4_K_M.gguf\"\n",
    "\n",
    "llm_tokenizer_dir = base_dir + \"/Llama-3.2-1B-Instruct\"\n",
    "prm_tokenizer_dir = base_dir + \"/Llama3.1-8B-PRM-Deepseek-Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6be7e90-c551-44ca-bedd-f5236e09dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 43\n",
      "2: 90\n",
      "3: 105\n",
      "4: 128\n",
      "5: 134\n"
     ]
    }
   ],
   "source": [
    "#  load data \n",
    "data_by_levels = load_data_prm800k(data_dir)\n",
    "\n",
    "# load random_seeds     \n",
    "# random_seeds = np.loadtxt(\"random_seeds.txt\").astype(\"int64\")\n",
    "# random_seeds = [int(seed) for seed in random_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831c2a5d-a3be-4679-9f20-936175aea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:17:25 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-07 20:17:25 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-07 20:17:33 config.py:549] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-07 20:17:33 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=123, served_model_name=/groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-07 20:17:34 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-07 20:17:34 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-07 20:17:34 model_runner.py:1110] Starting to load model /groups/kjun/tnn/datasets//Llama-3.2-1B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237594a535ed4d79b82f71b34546be87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:17:36 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-07 20:17:37 worker.py:267] Memory profiling takes 0.50 seconds\n",
      "INFO 04-07 20:17:37 worker.py:267] the current vLLM instance can use total_gpu_memory (31.73GiB) x gpu_memory_utilization (0.70) = 22.21GiB\n",
      "INFO 04-07 20:17:37 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 18.62GiB.\n",
      "INFO 04-07 20:17:37 executor_base.py:111] # cuda blocks: 38125, # CPU blocks: 8192\n",
      "INFO 04-07 20:17:37 executor_base.py:116] Maximum concurrency for 5000 tokens per request: 122.00x\n",
      "INFO 04-07 20:17:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:17:55 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.13 GiB\n",
      "INFO 04-07 20:17:55 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 19.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n",
      "#--- memory: 0.0\n"
     ]
    }
   ],
   "source": [
    "# baseline: gpu_memory_utilization=0.2\n",
    "# use the standard model \n",
    "llm_vllm = LLM(\n",
    "        model = llm_tokenizer_dir,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization = 0.7,  # Utilize 50% of GPU memory\n",
    "        # enable_prefix_caching=True,  # V100 doesn't support enable_prefix_caching \n",
    "        # enable_chunked_prefill=False, # and enable_chunked_prefill\n",
    "        max_model_len = 5000,\n",
    "        dtype = \"float16\",\n",
    "        seed = 123)\n",
    "    \n",
    "    # # use the gguf quantized model \n",
    "    # llm_regular = LLM(\n",
    "    #     model = llm_dir,\n",
    "    #     tokenizer = llm_tokenizer_dir,\n",
    "    #     tensor_parallel_size=1,\n",
    "    #     gpu_memory_utilization = 0.2,  # Utilize 50% of GPU memory\n",
    "    #     max_model_len = 5000,\n",
    "    #     dtype = \"float16\",\n",
    "    #     seed = 123)\n",
    "\n",
    "\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))\n",
    "print('#--- memory:', torch.cuda.memory_allocated(1)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6491650-0cc7-4363-bbb1-b8fc9c08f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- memory: 20.959694862365723\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer_dir)\n",
    "llm_tf = AutoModelForCausalLM.from_pretrained(llm_tokenizer_dir).to(\"cuda:1\")\n",
    "# model_regular.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "gc.collect();torch.cuda.empty_cache();\n",
    "print('#--- memory:', torch.cuda.memory_allocated(0)/(1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c547353a-d221-4ec5-850e-a6e029921bdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590d4828-9a59-4431-8054-d1859ec213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beam:\n",
    "    prompt: str\n",
    "    templated_prompt: str\n",
    "    index: int\n",
    "    current_text: str | None\n",
    "    next_texts: list[str] | None\n",
    "    lookahead_texts: list[str] | None\n",
    "    stop_reasons: list[str | None] | None\n",
    "    best_scores: list[float]  # the PRM scores\n",
    "    all_scores: list[list[float]]  # all PRM scores\n",
    "    previous_text: str | None\n",
    "    pruned: False\n",
    "    history: list[str]\n",
    "    completed: bool = False\n",
    "    completion_tokens: int = 0\n",
    "\n",
    "\n",
    "def _select_diverse(X_embeds, X_lprobs, X_ppl, K, V):\n",
    "    num_arms = len(X_embeds)\n",
    "    _V = copy.deepcopy(V)\n",
    "    A_idxes = []\n",
    "    A_embeds = []\n",
    "    tol = 0.0001\n",
    "    for it in range(K):\n",
    "        _V_inv = np.linalg.inv(_V)\n",
    "        arm_vals = np.einsum('ij,jk,ik->i', X_embeds, _V_inv, X_embeds)\n",
    "        max_val = np.max([val for idx, val in enumerate(arm_vals) if idx not in A_idxes])\n",
    "        # candidate_idxes = np.where(np.abs(arm_vals-max_val) < tol)[0]\n",
    "        candidate_idxes = [\n",
    "            arm_idx for arm_idx, arm_val in enumerate(arm_vals)\n",
    "            if (np.abs(max_val - arm_val) <= tol) and (arm_idx not in A_idxes)\n",
    "        ]\n",
    "\n",
    "        best_idx = max(candidate_idxes, key=lambda i: X_ppl[i])\n",
    "        # print(arm_vals)\n",
    "        # print(X_lprobs)\n",
    "        # print(candidate_idxes)\n",
    "        # print(best_idx)\n",
    "\n",
    "        best_embeds = X_embeds[best_idx]\n",
    "        # print(best_embeds.shape)\n",
    "\n",
    "        # update V\n",
    "        _V = _V + np.matmul(best_embeds, best_embeds.T)\n",
    "\n",
    "        # update A\n",
    "        A_idxes.append(best_idx)\n",
    "\n",
    "        # print(_V.shape)\n",
    "        # print(max_val)\n",
    "        # print(max_idx)\n",
    "        # print(A_idxes)\n",
    "\n",
    "    return A_idxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8582e6a-b80c-47e7-ba9b-f8c1296b816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_diverse_search(batch_of_questions, config: Config, llm: LLM, llm_tf, llm_tokenizer) -> list[Beam]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        max_tokens=config.max_tokens,\n",
    "        top_p=config.top_p,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        include_stop_str_in_output=True,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    beams: list[Beam] = []\n",
    "    for prompt in batch_of_questions:\n",
    "        for i in range(config.n):\n",
    "            beams.append(\n",
    "                Beam(\n",
    "                    prompt=prompt,\n",
    "                    templated_prompt=prompt,\n",
    "                    index=i,\n",
    "                    current_text=\"\",\n",
    "                    next_texts=None,\n",
    "                    lookahead_texts=None,\n",
    "                    pruned=False,\n",
    "                    completed=False,  # New flag to track completion\n",
    "                    stop_reasons=None,\n",
    "                    history=[],\n",
    "                    best_scores=[],\n",
    "                    all_scores=[],\n",
    "                    previous_text=None,\n",
    "                    completion_tokens=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    completed_beams: list[Beam] = []\n",
    "    \n",
    "    # for i in tqdm(range(config.num_iterations), desc=\"Beam search iterations\"):\n",
    "    for i in range(config.num_iterations):\n",
    "        # print(f\"iteration {i}\")\n",
    "        if i == 0:\n",
    "            active_beams = [b for b in beams if not b.pruned]\n",
    "        else:\n",
    "            active_beams = [b for b in active_beams if not b.pruned]\n",
    "\n",
    "        # Duplicate active beams to ensure that we have config.n beams per iteration\n",
    "        if len(active_beams) != config.n:\n",
    "            repeats = (config.n // len(active_beams)) + 1\n",
    "            # print(\n",
    "            #     f\"Extending active_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "            # )\n",
    "            extended_active_beams = [\n",
    "                copy.deepcopy(b) for b in (active_beams * repeats)[: config.n]\n",
    "            ]\n",
    "            active_beams = extended_active_beams\n",
    "            if len(active_beams) != config.n:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {config.n} active beams, but got {len(active_beams)}\"\n",
    "                )\n",
    "\n",
    "        if i == config.num_iterations - 1:\n",
    "            # Last iteration, generate to EOS\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p,\n",
    "                n=1,\n",
    "            )\n",
    "\n",
    "        convs = [\n",
    "            build_conv(b.prompt, b.current_text, config.system_prompt)\n",
    "            for b in active_beams\n",
    "        ]\n",
    "        continue_final_message = i > 0\n",
    "        add_generation_prompt = i == 0\n",
    "\n",
    "        tokenizer = llm.get_tokenizer()\n",
    "        if config.custom_chat_template is not None:\n",
    "            tokenizer.chat_template = config.custom_chat_template\n",
    "        templated_convs = tokenizer.apply_chat_template(\n",
    "            convs,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            continue_final_message=continue_final_message,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        \n",
    "        lookahead = 0 if i == config.num_iterations - 1 else config.lookahead\n",
    "        gen_results = generate_k_steps(\n",
    "            templated_convs, lookahead, llm, sampling_params, 1\n",
    "        )\n",
    "        # print(gen_results)\n",
    "        # stop\n",
    "\n",
    "        prompts, completions = [], []\n",
    "        next_active_beams = []\n",
    "        for beam, gen_result in zip(active_beams, gen_results, strict=True):\n",
    "            beam.next_texts = gen_result.next_texts\n",
    "            beam.stop_reasons = gen_result.stop_reasons\n",
    "            beam.lookahead_texts = gen_result.lookahead_texts\n",
    "            beam.completion_tokens += gen_result.completion_tokens\n",
    "            beam.current_text += beam.next_texts[0]\n",
    "            beam.history.append(beam.next_texts[0])\n",
    "            beam.templated_prompt = gen_result.prompt\n",
    "\n",
    "            if (\n",
    "                beam.stop_reasons[0] == \"EOS\"\n",
    "                or beam.stop_reasons[0] == \"length\"\n",
    "                or beam.next_texts[0] == \"\"\n",
    "            ):\n",
    "                beam.completed = True\n",
    "                completed_beams.append(beam)\n",
    "\n",
    "            prompts.append(beam.prompt)\n",
    "            completions.append([beam.current_text])\n",
    "\n",
    "        active_beams = [b for b in active_beams if not b.completed]\n",
    "        # print(active_beams)\n",
    "\n",
    "        # Early stopping if all beams are completed\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "        # # get completion's embeddings\n",
    "        completions_embeds = np.zeros((len(active_beams), 2048))\n",
    "        completions_log_probs = np.zeros(len(active_beams))\n",
    "        completions_ppl = np.zeros(len(active_beams))\n",
    "    \n",
    "        for b_idx, beam in enumerate(active_beams):\n",
    "            with torch.no_grad():\n",
    "                # get beam.current_text which include previous all steps upto now\n",
    "                gen_prompt = beam.templated_prompt + beam.next_texts[0]\n",
    "                inputs = llm_tokenizer(gen_prompt, return_tensors=\"pt\").to(llm_tf.device)\n",
    "                outputs = llm_tf(**inputs, output_hidden_states=True)\n",
    "\n",
    "                # Get last_token_embeds\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                last_token_embeds = last_hidden_state[:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "                # print(last_token_embeds.shape)\n",
    "\n",
    "                # Compute otuput_log_prob\n",
    "                # Prepare labels: shift input_ids to the right by one\n",
    "                # print(inputs)\n",
    "                labels = inputs['input_ids'][:, 1:]   \n",
    "                shifted_logits = outputs.logits[:, :-1, :]\n",
    "                loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "                completion_log_prob = -loss_fct(shifted_logits.view(-1, shifted_logits.size(-1)), labels.view(-1)).detach().cpu().numpy()\n",
    "                completion_ppl = np.exp(completion_log_prob/len(labels))\n",
    "                # print(sent_ppl)\n",
    "                # print(loss)\n",
    "\n",
    "                # Approach \n",
    "                # log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "                # token_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "                # completion_log_prob = token_log_probs.sum()\n",
    "                # print(completion_log_prob)\n",
    "\n",
    "                # normalize the embeds\n",
    "                if config.normalize_embeds:\n",
    "                    norm = np.linalg.norm(last_token_embeds)\n",
    "                    last_token_embeds /= norm\n",
    "                    # print(np.linalg.norm(last_token_embeds))\n",
    "\n",
    "                completions_embeds[b_idx] = last_token_embeds\n",
    "                completions_log_probs[b_idx] = completion_log_prob\n",
    "                completions_ppl[b_idx] = completion_ppl\n",
    "\n",
    "        # print(completions_embeds)\n",
    "        # print(completions_log_probs)\n",
    "        # print(completions_ppl)\n",
    "\n",
    "        # get completion's embeddings\n",
    "\n",
    "        V = config.lam*np.eye(2048)\n",
    "        K = int(config.n / config.beam_width)\n",
    "        if len(active_beams) <= K:\n",
    "            continue \n",
    "            \n",
    "        selected_idxes = _select_diverse(\n",
    "            completions_embeds, completions_log_probs, completions_ppl, K, V)\n",
    "        # print(len(completions_embeds))\n",
    "        # print(selected_idxes)\n",
    "\n",
    "        for idx, beam in enumerate(active_beams):\n",
    "            if idx not in selected_idxes:\n",
    "                beam.pruned = True\n",
    "\n",
    "    if len(completed_beams) != config.n:\n",
    "        # If we don't have enough completed_beams, duplicate until we reach config.n\n",
    "        repeats = (config.n // len(completed_beams)) + 1\n",
    "        # print(\n",
    "        #     f\"Extending completed_beams with {repeats} repetitions to reach size {config.n}\"\n",
    "        # )\n",
    "        extended_completed_beams = [\n",
    "            copy.deepcopy(b) for b in (completed_beams * repeats)[: config.n]\n",
    "        ]\n",
    "        completed_beams = extended_completed_beams\n",
    "\n",
    "    return completed_beams\n",
    "    \n",
    "\n",
    "# general params\n",
    "config = Config()\n",
    "config.n = 4\n",
    "config.beam_width = 2\n",
    "config.lookahead = 0\n",
    "config.num_iterations = 2\n",
    "\n",
    "# diverse_select params\n",
    "config.lam = 10\n",
    "config.normalize_embeds = True\n",
    "\n",
    "level = '4'\n",
    "num_questions = len(data_by_levels[level])\n",
    "num_questions = 2\n",
    "num_trials = 1\n",
    "print(f\"num_questions = {num_questions}\")\n",
    "print(f\"num_trials = {num_trials}\")\n",
    "\n",
    "# get batch of questions\n",
    "batch_of_questions = [data_by_levels[level][q_idx]['problem'] for q_idx in range(num_questions)]\n",
    "\n",
    "beam_results = _select_diverse_search(batch_of_questions, config, llm_vllm, llm_tf, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef302-7ecf-4777-91f0-853f25d3a8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
